{"cells":[{"cell_type":"markdown","metadata":{"id":"e460cbb5"},"source":["# About this notebook\n","- tokenizer(anchor[SEP]target | CPC)\n","- Deberta-v3-large starter code\n","- pip wheels is [here](https://www.kaggle.com/code/yasufuminakama/pppm-pip-wheels)\n","- Inference notebook is [here](https://www.kaggle.com/code/yasufuminakama/pppm-deberta-v3-large-baseline-inference)\n","\n","If this notebook is helpful, feel free to upvote :)"]},{"cell_type":"markdown","metadata":{"id":"xONchFYMvMMf"},"source":["# Directory settings"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2390,"status":"ok","timestamp":1655711142357,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"fa3b873b","outputId":"d30f3541-28aa-4bb7-ee3b-6aafdd340142"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.7.13 (default, Apr 24 2022, 01:04:09) \n","[GCC 7.5.0]\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/colab_notebooks/kaggle/us-patent-phrase-to-phrase-matching/notebooks\n"]}],"source":["# ====================================================\n","# Directory settings\n","# ====================================================\n","comp_name = 'us-patent-phrase-to-phrase-matching'\n","nb_name = 'nb005t-deberta-v3-large'\n","\n","import sys\n","print(sys.version)\n","if \"google.colab\" in sys.modules:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    base = f\"/content/drive/MyDrive/colab_notebooks/kaggle/{comp_name}/notebooks\"\n","    %cd {base}\n","\n","\n","import os\n","INPUT_DIR = f'../input/{comp_name}/'\n","if 'kaggle_web_client' in sys.modules:\n","    OUTPUT_DIR = './'\n","else:\n","    OUTPUT_DIR = f'../input/{nb_name}/'\n","    if not os.path.exists(OUTPUT_DIR):\n","        os.makedirs(OUTPUT_DIR)"]},{"cell_type":"markdown","metadata":{"id":"1d0c4430"},"source":["# CFG"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655711142359,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"48dd82bb"},"outputs":[],"source":["# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    debug=False\n","    wandb=True\n","    wandbproject=comp_name\n","    wandbgroup=nb_name\n","    wandbname='exp003.002.001'\n","    _wandb_kernel='riow1983'\n","    apex=True\n","    print_freq=100\n","    num_workers=8\n","    model=\"microsoft/deberta-v3-large\"\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=4\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size=16\n","    fc_dropout=0.2\n","    target_size=1\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    #### AWP\n","    adv_lr=1e-6\n","    adv_eps=1e-3\n","    #### AWPAWP\n","    n_fold=10\n","    trn_fold=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","    train=True\n","    \n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]\n","    CFG.wandb = False"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":21953,"status":"ok","timestamp":1655711164301,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"b88c983e","colab":{"base_uri":"https://localhost:8080/","height":965},"outputId":"45b860f8-0083-49f5-ed4f-6d3aa204a4bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 15.3 MB/s \n","\u001b[?25hCollecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 82.9 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 55.9 MB/s \n","\u001b[?25hCollecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Building wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f914824d8db80075f053f55b7f49ce308c24c4f8944ed929eea978973aa56698\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriow1983\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.12.18"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>../input/nb005t-deberta-v3-large/wandb/run-20220620_074556-7ufuddux</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/riow1983/us-patent-phrase-to-phrase-matching/runs/7ufuddux\" target=\"_blank\">exp003.002.001</a></strong> to <a href=\"https://wandb.ai/riow1983/us-patent-phrase-to-phrase-matching\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["wandb run id: 7ufuddux\n"]}],"source":["# ====================================================\n","# wandb\n","# ====================================================\n","if CFG.wandb:\n","    if 'google.colab' in sys.modules:\n","        !pip install wandb\n","    import wandb\n","\n","    try:\n","        if 'kaggle_web_client' in sys.modules:\n","            from kaggle_secrets import UserSecretsClient\n","            user_secrets = UserSecretsClient()\n","            secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n","        else:\n","            import json\n","            f = open(\"../../wandb.json\", \"r\")\n","            json_data = json.load(f)\n","            secret_value_0 = json_data[\"wandb_api\"]\n","        wandb.login(key=secret_value_0)\n","        anony = None\n","    except:\n","        anony = \"must\"\n","        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n","\n","\n","    def class2dict(f):\n","        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n","    \n","    run = wandb.init(\n","        dir=OUTPUT_DIR,\n","        project=CFG.wandbproject,\n","        group=CFG.wandbgroup,\n","        name=CFG.wandbname, \n","        config=class2dict(CFG),\n","        job_type=\"train\",\n","        anonymous=anony)\n","    print(f\"wandb run id: {run.id}\")"]},{"cell_type":"markdown","metadata":{"id":"f2ed8ef2"},"source":["# Library"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19005,"status":"ok","timestamp":1655711183290,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"35916341","outputId":"7131bc14-dd4a-42e7-874c-4cacb3a59d62"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.__version__: 1.11.0+cu113\n","tokenizers.__version__: 0.12.1\n","transformers.__version__: 4.18.0\n","env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["# ====================================================\n","# Library\n","# ====================================================\n","import os\n","import gc\n","import re\n","import ast\n","import sys\n","import copy\n","import json\n","import time\n","import math\n","import shutil\n","import string\n","import pickle\n","import random\n","import joblib\n","import itertools\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n","\n","\n","# # PyTorchのバージョンを1.10.1に下げる (Google Colabなのでpipでやる)\n","# os.system('pip uninstall -y torch torchvision torchaudio')\n","# os.system('pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html')\n","\n","\n","import torch\n","print(f\"torch.__version__: {torch.__version__}\")\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","os.system('pip uninstall -y transformers')\n","os.system('pip uninstall -y tokenizers')\n","os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels transformers')\n","os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels tokenizers')\n","# os.system('python -m pip install transformers')\n","# os.system('python -m pip install tokenizers')\n","os.system('pip install sentencepiece')\n","import tokenizers\n","import transformers\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","%env TOKENIZERS_PARALLELISM=true\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"fd586614"},"source":["# Utils"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1655711183292,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"d5c0ccc6"},"outputs":[],"source":["# ====================================================\n","# Utils\n","# ====================================================\n","def get_score(y_true, y_pred):\n","    score = sp.stats.pearsonr(y_true, y_pred)[0]\n","    return score\n","\n","\n","def get_logger(filename=OUTPUT_DIR+'train'):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=f\"{filename}.log\")\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = get_logger()\n","\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=42)"]},{"cell_type":"markdown","metadata":{"id":"cb3d8e1e"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":638},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1655711183294,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"bef012d3","outputId":"b166e12c-d3d4-4248-9a66-9ab24e989d98"},"outputs":[{"output_type":"stream","name":"stdout","text":["train.shape: (36473, 5)\n","test.shape: (36, 4)\n","submission.shape: (36, 2)\n"]},{"output_type":"display_data","data":{"text/plain":["                 id     anchor                  target context  score\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"],"text/html":["\n","  <div id=\"df-b3c59d80-b4d0-4223-b911-682e4cf29845\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3c59d80-b4d0-4223-b911-682e4cf29845')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b3c59d80-b4d0-4223-b911-682e4cf29845 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b3c59d80-b4d0-4223-b911-682e4cf29845');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                 id              anchor                         target context\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n","2  36baf228038e314b      lower trunnion                 lower locating     B60\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04"],"text/html":["\n","  <div id=\"df-638c8ec6-02fd-4fa6-a96e-4e86c056fcfb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-638c8ec6-02fd-4fa6-a96e-4e86c056fcfb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-638c8ec6-02fd-4fa6-a96e-4e86c056fcfb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-638c8ec6-02fd-4fa6-a96e-4e86c056fcfb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                 id  score\n","0  4112d61851461f60      0\n","1  09e418c93a776564      0\n","2  36baf228038e314b      0\n","3  1f37ead645e7f0c8      0\n","4  71a5b6ad068d531f      0"],"text/html":["\n","  <div id=\"df-912c8b1e-9d41-4de9-a3f2-3869a499b22d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-912c8b1e-9d41-4de9-a3f2-3869a499b22d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-912c8b1e-9d41-4de9-a3f2-3869a499b22d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-912c8b1e-9d41-4de9-a3f2-3869a499b22d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["# ====================================================\n","# Data Loading\n","# ====================================================\n","# #### AUG\n","# # train = pd.read_csv(INPUT_DIR+'train.csv')\n","# train = pd.read_csv('../input/kagglenb006-back-translate-aug-data/train.csv')\n","# #### AUGAUG\n","train = pd.read_csv(INPUT_DIR+'train.csv')\n","test = pd.read_csv(INPUT_DIR+'test.csv')\n","submission = pd.read_csv(INPUT_DIR+'sample_submission.csv')\n","print(f\"train.shape: {train.shape}\")\n","print(f\"test.shape: {test.shape}\")\n","print(f\"submission.shape: {submission.shape}\")\n","display(train.head())\n","display(test.head())\n","display(submission.head())"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":613},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1655711183296,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"UCsnldv5vMMq","outputId":"a49b84a8-2058-4c92-cb72-b4bacae3f910"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                 id     anchor                  target context  score                                       context_text\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE..."],"text/html":["\n","  <div id=\"df-ac517f98-3928-4de7-aef5-dd27176b21dc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac517f98-3928-4de7-aef5-dd27176b21dc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ac517f98-3928-4de7-aef5-dd27176b21dc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ac517f98-3928-4de7-aef5-dd27176b21dc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                 id              anchor                         target context                                       context_text\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE"],"text/html":["\n","  <div id=\"df-2b22886c-24aa-4d70-b600-a6ab5f0c2025\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>context_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","      <td>PHYSICS. OPTICS</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b22886c-24aa-4d70-b600-a6ab5f0c2025')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2b22886c-24aa-4d70-b600-a6ab5f0c2025 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2b22886c-24aa-4d70-b600-a6ab5f0c2025');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["# ====================================================\n","# CPC Data\n","# ====================================================\n","def get_cpc_texts():\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir('../input/cpc-data/CPCSchemeXML202105'):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","\n","    # for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","    #     with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n","    #         s = f.read()\n","    #     pattern = f'{cpc}\\t\\t.+'\n","    #     result = re.findall(pattern, s)\n","    #     cpc_result = result[0].lstrip(pattern)\n","    #     for context in [c for c in contexts if c[0] == cpc]:\n","    #         pattern = f'{context}\\t\\t.+'\n","    #         result = re.findall(pattern, s)\n","    #         results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n","    \n","    # Credits to https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/discussion/324928\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(f'../input/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        pattern = \"^\"+pattern[:-2]\n","        cpc_result = re.sub(pattern, \"\", result[0])\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            pattern = \"^\"+pattern[:-2]\n","            results[context] = cpc_result + \". \" + re.sub(pattern, \"\", result[0])\n","\n","    \n","    return results\n","\n","\n","cpc_texts = get_cpc_texts()\n","torch.save(cpc_texts, OUTPUT_DIR+\"cpc_texts.pth\")\n","train['context_text'] = train['context'].map(cpc_texts)\n","test['context_text'] = test['context'].map(cpc_texts)\n","display(train.head())\n","display(test.head())"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1655711183301,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"PJvUJQujvMMr","outputId":"18a6b088-1985-4c21-ee43-da79211bcaa4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["                 id     anchor                  target context  score                                       context_text                                  text                                              text2\n","0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  abatement[SEP]abatement of pollution  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...          abatement[SEP]act of abating  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","2  36d72442aefd8232  abatement         active catalyst     A47   0.25  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...         abatement[SEP]active catalyst  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...     abatement[SEP]eliminating process  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...\n","4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...           abatement[SEP]forest region  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE..."],"text/html":["\n","  <div id=\"df-abf283b3-2e76-49aa-9b0e-554b65765c4c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>score</th>\n","      <th>context_text</th>\n","      <th>text</th>\n","      <th>text2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>37d61fd2272659b1</td>\n","      <td>abatement</td>\n","      <td>abatement of pollution</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]abatement of pollution</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7b9652b17b68b7a4</td>\n","      <td>abatement</td>\n","      <td>act of abating</td>\n","      <td>A47</td>\n","      <td>0.75</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]act of abating</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36d72442aefd8232</td>\n","      <td>abatement</td>\n","      <td>active catalyst</td>\n","      <td>A47</td>\n","      <td>0.25</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]active catalyst</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5296b0c19e1ce60e</td>\n","      <td>abatement</td>\n","      <td>eliminating process</td>\n","      <td>A47</td>\n","      <td>0.50</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]eliminating process</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>54c1e3b9184cb5b6</td>\n","      <td>abatement</td>\n","      <td>forest region</td>\n","      <td>A47</td>\n","      <td>0.00</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","      <td>abatement[SEP]forest region</td>\n","      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abf283b3-2e76-49aa-9b0e-554b65765c4c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-abf283b3-2e76-49aa-9b0e-554b65765c4c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-abf283b3-2e76-49aa-9b0e-554b65765c4c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                 id              anchor                         target context                                       context_text                                              text                                              text2\n","0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02                                    PHYSICS. OPTICS        opc drum[SEP]inorganic photoconductor drum                                    PHYSICS. OPTICS\n","1  09e418c93a776564     adjust gas flow              altering gas flow     F23  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...             adjust gas flow[SEP]altering gas flow  MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...\n","2  36baf228038e314b      lower trunnion                 lower locating     B60  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...                 lower trunnion[SEP]lower locating  PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...\n","3  1f37ead645e7f0c8       cap component                  upper portion     D06  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...                   cap component[SEP]upper portion  TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...\n","4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE  neural stimulation[SEP]artificial neural network      ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE"],"text/html":["\n","  <div id=\"df-d7b31b5f-528b-4cc2-8367-c199dfde460a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>anchor</th>\n","      <th>target</th>\n","      <th>context</th>\n","      <th>context_text</th>\n","      <th>text</th>\n","      <th>text2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4112d61851461f60</td>\n","      <td>opc drum</td>\n","      <td>inorganic photoconductor drum</td>\n","      <td>G02</td>\n","      <td>PHYSICS. OPTICS</td>\n","      <td>opc drum[SEP]inorganic photoconductor drum</td>\n","      <td>PHYSICS. OPTICS</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>09e418c93a776564</td>\n","      <td>adjust gas flow</td>\n","      <td>altering gas flow</td>\n","      <td>F23</td>\n","      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n","      <td>adjust gas flow[SEP]altering gas flow</td>\n","      <td>MECHANICAL ENGINEERING; LIGHTING; HEATING; WEA...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>36baf228038e314b</td>\n","      <td>lower trunnion</td>\n","      <td>lower locating</td>\n","      <td>B60</td>\n","      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n","      <td>lower trunnion[SEP]lower locating</td>\n","      <td>PERFORMING OPERATIONS; TRANSPORTING. VEHICLES ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1f37ead645e7f0c8</td>\n","      <td>cap component</td>\n","      <td>upper portion</td>\n","      <td>D06</td>\n","      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n","      <td>cap component[SEP]upper portion</td>\n","      <td>TEXTILES; PAPER. TREATMENT OF TEXTILES OR THE ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71a5b6ad068d531f</td>\n","      <td>neural stimulation</td>\n","      <td>artificial neural network</td>\n","      <td>H04</td>\n","      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n","      <td>neural stimulation[SEP]artificial neural network</td>\n","      <td>ELECTRICITY. ELECTRIC COMMUNICATION TECHNIQUE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7b31b5f-528b-4cc2-8367-c199dfde460a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d7b31b5f-528b-4cc2-8367-c199dfde460a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d7b31b5f-528b-4cc2-8367-c199dfde460a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["# train['text'] = train['anchor'] + '[SEP]' + train['target'] + '[SEP]'  + train['context_text']\n","# test['text'] = test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n","\n","train['text'] = train['anchor'] + '[SEP]' + train['target']\n","test['text'] = test['anchor'] + '[SEP]' + test['target']\n","\n","train['text2'] = train['context_text']\n","test['text2'] = test['context_text']\n","\n","\n","display(train.head())\n","display(test.head())"]},{"cell_type":"markdown","metadata":{"id":"zuhGVmnivMMs"},"source":["# EDA"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1655711183324,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"CdwSw4u5vMMs","outputId":"2b38d7ed-ff5a-4f85-a3bd-49afad30900e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f8f617b3550>"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT6klEQVR4nO3cf6zd9X3f8eerdkkIJJiE7iqyWe0pbjYHVo1eAVWk7iauwJAKI5VGIFpM5tVSS7KsRWvMqokpCRJRS1lg+VFveDYRjaGsm61CSy3CFdpUE6BkmB+l3AEBeySksXHnkB919t4f53PbU9fm3nvOvef4+jwf0tX9fj/fz/f7/bzPOfbrfn+cb6oKSdJo+5FhD0CSNHyGgSTJMJAkGQaSJAwDSRKwdNgD6NVZZ51VK1eu7Gnd73znO5x22mnzO6ATnDWPhlGredTqhf5rfvzxx/+yqn7s6PZFGwYrV67kscce62ndyclJJiYm5ndAJzhrHg2jVvOo1Qv915zk68dq9zSRJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJJYxN9Alk5UKzffN7R9b1s3Wo9m0PzxyECSNHMYJNma5LUkT3W1/VaSP0/yZJL/lmRZ17IbkkwleS7JxV3t61rbVJLNXe2rkjzS2u9Ocsp8FihJmtlsjgy2AeuOatsNnFNV/xT4C+AGgCRrgCuB97V1Pp9kSZIlwOeAS4A1wFWtL8BngFur6j3AQWBjXxVJkuZsxjCoqoeBA0e1/UlVHWmze4AVbXo9sKOqvl9VLwJTwPntZ6qqXqiqHwA7gPVJAnwQuLetvx24vM+aJElzNB8XkP8FcHebXk4nHKbta20ArxzVfgHwLuD1rmDp7v/3JNkEbAIYGxtjcnKypwEfPny453UXK2senOvPPTJzpwUyau/zqNULC1dzX2GQ5DeBI8Bd8zOcN1dVW4AtAOPj49XrM719BvpoGFbN1w75bqJRep/9XM+fnsMgybXAzwFrq6pa837g7K5uK1obx2n/NrAsydJ2dNDdX5I0ID3dWppkHfAbwGVV9UbXol3AlUnekmQVsBr4KvAosLrdOXQKnYvMu1qIPARc0dbfAOzsrRRJUq9mc2vpl4E/Bd6bZF+SjcB/BN4O7E7ytSRfBKiqp4F7gGeAPwauq6oftr/6Pwo8ADwL3NP6AnwC+PUkU3SuIdwxrxVKkmY042miqrrqGM3H/Q+7qm4CbjpG+/3A/cdof4HO3UaSpCHxG8iSJMNAkuSD6kbG3v2HhnLL40s3f2jg+5Q0dx4ZSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkphFGCTZmuS1JE91tb0zye4kz7ffZ7b2JLktyVSSJ5Oc17XOhtb/+SQbutp/Ksnets5tSTLfRUqS3txsjgy2AeuOatsMPFhVq4EH2zzAJcDq9rMJ+AJ0wgO4EbgAOB+4cTpAWp9f7lrv6H1JkhbYjGFQVQ8DB45qXg9sb9Pbgcu72u+sjj3AsiTvBi4GdlfVgao6COwG1rVl76iqPVVVwJ1d25IkDcjSHtcbq6pX2/Q3gLE2vRx4pavfvtb2Zu37jtF+TEk20TniYGxsjMnJyZ4Gf/jw4Z7XXazGToXrzz0y8P0O83Ue1vs8jNd52qh9tketXli4mnsNg79RVZWk5mMws9jXFmALwPj4eE1MTPS0ncnJSXpdd7G6/a6d3LK377d7zl66emLg+5w2rPf52s33DXyf07atO22kPtuj+G95oWru9W6ib7ZTPLTfr7X2/cDZXf1WtLY3a19xjHZJ0gD1Gga7gOk7gjYAO7var2l3FV0IHGqnkx4ALkpyZrtwfBHwQFv2V0kubHcRXdO1LUnSgMx43iDJl4EJ4Kwk++jcFXQzcE+SjcDXgQ+37vcDlwJTwBvARwCq6kCSTwGPtn6frKrpi9K/SueOpVOBP2o/kqQBmjEMquqq4yxae4y+BVx3nO1sBbYeo/0x4JyZxiFJWjh+A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJ9BkGSX4tydNJnkry5SRvTbIqySNJppLcneSU1vctbX6qLV/ZtZ0bWvtzSS7uryRJ0lz1HAZJlgP/ChivqnOAJcCVwGeAW6vqPcBBYGNbZSNwsLXf2vqRZE1b733AOuDzSZb0Oi5J0tz1e5poKXBqkqXA24BXgQ8C97bl24HL2/T6Nk9bvjZJWvuOqvp+Vb0ITAHn9zkuSdIcLO11xaran+S3gZeB7wJ/AjwOvF5VR1q3fcDyNr0ceKWteyTJIeBdrX1P16a71/k7kmwCNgGMjY0xOTnZ09gPHz7c87qL1dipcP25R2buOM+G+ToP630exus8bdQ+26NWLyxczT2HQZIz6fxVvwp4Hfh9Oqd5FkxVbQG2AIyPj9fExERP25mcnKTXdRer2+/ayS17e367e/bS1RMD3+e0Yb3P126+b+D7nLZt3Wkj9dkexX/LC1VzP6eJfhZ4saq+VVV/DfwB8H5gWTttBLAC2N+m9wNnA7TlZwDf7m4/xjqSpAHoJwxeBi5M8rZ27n8t8AzwEHBF67MB2Nmmd7V52vKvVFW19ivb3UargNXAV/sYlyRpjvq5ZvBIknuBPwOOAE/QOYVzH7Ajyadb2x1tlTuALyWZAg7QuYOIqno6yT10guQIcF1V/bDXcUmS5q6vk8hVdSNw41HNL3CMu4Gq6nvALxxnOzcBN/UzFklS7/wGsiTJMJAkGQaSJPq8ZrBY7d1/aCj3gr9084cGvk9Jmg2PDCRJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0GQZJliW5N8mfJ3k2yU8neWeS3Umeb7/PbH2T5LYkU0meTHJe13Y2tP7PJ9nQb1GSpLnp98jgs8AfV9U/Bn4SeBbYDDxYVauBB9s8wCXA6vazCfgCQJJ3AjcCFwDnAzdOB4gkaTB6DoMkZwA/A9wBUFU/qKrXgfXA9tZtO3B5m14P3Fkde4BlSd4NXAzsrqoDVXUQ2A2s63VckqS5W9rHuquAbwH/JclPAo8DHwfGqurV1ucbwFibXg680rX+vtZ2vPa/J8kmOkcVjI2NMTk52dPAx06F68890tO6/eh1vPNhFGs+fPjwUPY/jNd52rBqHpZRqxcWruZ+wmApcB7wsap6JMln+dtTQgBUVSWpfgZ41Pa2AFsAxsfHa2Jioqft3H7XTm7Z20/pvXnp6omB73PaKNY8OTlJr5+Rfly7+b6B73PatnWnDaXmYRnWezxMC1VzP9cM9gH7quqRNn8vnXD4Zjv9Q/v9Wlu+Hzi7a/0Vre147ZKkAek5DKrqG8ArSd7bmtYCzwC7gOk7gjYAO9v0LuCadlfRhcChdjrpAeCiJGe2C8cXtTZJ0oD0e97gY8BdSU4BXgA+Qidg7kmyEfg68OHW937gUmAKeKP1paoOJPkU8Gjr98mqOtDnuCRJc9BXGFTV14DxYyxae4y+BVx3nO1sBbb2MxZJUu/8BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCQxD2GQZEmSJ5L8YZtfleSRJFNJ7k5ySmt/S5ufastXdm3jhtb+XJKL+x2TJGlu5uPI4OPAs13znwFurar3AAeBja19I3Cwtd/a+pFkDXAl8D5gHfD5JEvmYVySpFnqKwySrAA+BPznNh/gg8C9rct24PI2vb7N05avbf3XAzuq6vtV9SIwBZzfz7gkSXOztM/1/wPwG8Db2/y7gNer6kib3wcsb9PLgVcAqupIkkOt/3JgT9c2u9f5O5JsAjYBjI2NMTk52dOgx06F6889MnPHedbreOfDKNZ8+PDhoex/GK/ztGHVvHf/oYHvE2DVGUuG+hkbhoV6j3sOgyQ/B7xWVY8nmZi/IR1fVW0BtgCMj4/XxERvu739rp3csrffHJy7l66eGPg+p41izZOTk/T6GenHtZvvG/g+p21bd9pI1TyseodpoT7X/fzv8H7gsiSXAm8F3gF8FliWZGk7OlgB7G/99wNnA/uSLAXOAL7d1T6tex1J0gD0fM2gqm6oqhVVtZLOBeCvVNXVwEPAFa3bBmBnm97V5mnLv1JV1dqvbHcbrQJWA1/tdVySpLlbiPMGnwB2JPk08ARwR2u/A/hSkingAJ0AoaqeTnIP8AxwBLiuqn64AOOSJB3HvIRBVU0Ck236BY5xN1BVfQ/4heOsfxNw03yMRZI0d34DWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0EQZJzk7yUJJnkjyd5OOt/Z1Jdid5vv0+s7UnyW1JppI8meS8rm1taP2fT7Kh/7IkSXPRz5HBEeD6qloDXAhcl2QNsBl4sKpWAw+2eYBLgNXtZxPwBeiEB3AjcAFwPnDjdIBIkgaj5zCoqler6s/a9P8FngWWA+uB7a3bduDyNr0euLM69gDLkrwbuBjYXVUHquogsBtY1+u4JElzl6rqfyPJSuBh4Bzg5apa1toDHKyqZUn+ELi5qv5HW/Yg8AlgAnhrVX26tf874LtV9dvH2M8mOkcVjI2N/dSOHTt6Gu9rBw7xze/2tGpfzl1+xuB32oxizYcPH+b0008f+H737j808H1OW3XGkpGqeVj1DlO/n+sPfOADj1fV+NHtS/saFZDkdOC/Av+6qv6q8/9/R1VVkv7T5m+3twXYAjA+Pl4TExM9bef2u3Zyy96+S5+zl66eGPg+p41izZOTk/T6GenHtZvvG/g+p21bd9pI1TyseodpoT7Xfd1NlORH6QTBXVX1B635m+30D+33a619P3B21+orWtvx2iVJA9LP3UQB7gCerarf6Vq0C5i+I2gDsLOr/Zp2V9GFwKGqehV4ALgoyZntwvFFrU2SNCD9nDd4P/BLwN4kX2tt/xa4GbgnyUbg68CH27L7gUuBKeAN4CMAVXUgyaeAR1u/T1bVgT7GJUmao57DoF0IznEWrz1G/wKuO862tgJbex2LJKk/fgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkpiHB9VJ0ihaOcSH8y0EjwwkSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJIkTKAySrEvyXJKpJJuHPR5JGiUnRBgkWQJ8DrgEWANclWTNcEclSaPjhAgD4HxgqqpeqKofADuA9UMekySNjFTVsMdAkiuAdVX1L9v8LwEXVNVHj+q3CdjUZt8LPNfjLs8C/rLHdRcrax4No1bzqNUL/df841X1Y0c3Lu1jgwNXVVuALf1uJ8ljVTU+D0NaNKx5NIxazaNWLyxczSfKaaL9wNld8ytamyRpAE6UMHgUWJ1kVZJTgCuBXUMekySNjBPiNFFVHUnyUeABYAmwtaqeXsBd9n2qaRGy5tEwajWPWr2wQDWfEBeQJUnDdaKcJpIkDZFhIEk6ucNgpkdcJHlLkrvb8keSrBz8KOfPLOr99STPJHkyyYNJfnwY45xPs32MSZKfT1JJFv1tiLOpOcmH23v9dJLfG/QY59ssPtv/MMlDSZ5on+9LhzHO+ZJka5LXkjx1nOVJclt7PZ5Mcl7fO62qk/KHzoXo/w38I+AU4H8Ba47q86vAF9v0lcDdwx73Atf7AeBtbfpXFnO9s6259Xs78DCwBxgf9rgH8D6vBp4Azmzz/2DY4x5AzVuAX2nTa4CXhj3uPmv+GeA84KnjLL8U+CMgwIXAI/3u82Q+MpjNIy7WA9vb9L3A2iQZ4Bjn04z1VtVDVfVGm91D5/sci9lsH2PyKeAzwPcGObgFMpuafxn4XFUdBKiq1wY8xvk2m5oLeEebPgP4PwMc37yrqoeBA2/SZT1wZ3XsAZYleXc/+zyZw2A58ErX/L7Wdsw+VXUEOAS8ayCjm3+zqbfbRjp/WSxmM9bcDp/Prqr7BjmwBTSb9/kngJ9I8j+T7EmybmCjWxizqfnfA7+YZB9wP/CxwQxtaOb6731GJ8T3DDRYSX4RGAf++bDHspCS/AjwO8C1Qx7KoC2lc6pogs7R38NJzq2q14c6qoV1FbCtqm5J8tPAl5KcU1X/b9gDWyxO5iOD2Tzi4m/6JFlK5/Dy2wMZ3fyb1SM9kvws8JvAZVX1/QGNbaHMVPPbgXOAySQv0Tm3umuRX0Sezfu8D9hVVX9dVS8Cf0EnHBar2dS8EbgHoKr+FHgrnQe6nazm/RE+J3MYzOYRF7uADW36CuAr1a7OLEIz1pvknwG/SycIFvt5ZJih5qo6VFVnVdXKqlpJ5zrJZVX12HCGOy9m87n+73SOCkhyFp3TRi8McpDzbDY1vwysBUjyT+iEwbcGOsrB2gVc0+4quhA4VFWv9rPBk/Y0UR3nERdJPgk8VlW7gDvoHE5O0blYc+XwRtyfWdb7W8DpwO+36+QvV9VlQxt0n2ZZ80llljU/AFyU5Bngh8C/qarFesQ725qvB/5Tkl+jczH52kX8hx1Jvkwn0M9q10FuBH4UoKq+SOe6yKXAFPAG8JG+97mIXy9J0jw5mU8TSZJmyTCQJBkGkiTDQJKEYSBJwjCQJGEYSJKA/w/+hJpxtNMEiwAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}],"source":["train['score'].hist()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1655711183325,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"GJBRJnjevMMs","outputId":"5e37343a-e01e-47c7-99f6-0269de787bc4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["B    8019\n","H    6195\n","G    6013\n","C    5288\n","A    4094\n","F    4054\n","E    1531\n","D    1279\n","Name: context, dtype: int64"]},"metadata":{}}],"source":["display(train['context'].apply(lambda x: x[0]).value_counts())"]},{"cell_type":"markdown","metadata":{"id":"62MFTSvavMMt"},"source":["- Y is not in training data, but may be in test data?"]},{"cell_type":"markdown","metadata":{"id":"9e05b6c4"},"source":["# CV split"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2139,"status":"ok","timestamp":1655711185146,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"3ba287c4","outputId":"ff217ee7-ea82-403c-f480-0de730bbcf2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["660 73\n","660 73\n","659 74\n","659 74\n","660 73\n","659 74\n","660 73\n","660 73\n","660 73\n","660 73\n","0    3954\n","9    3947\n","4    3902\n","6    3647\n","1    3634\n","3    3604\n","2    3524\n","5    3517\n","8    3487\n","7    3257\n","Name: fold, dtype: int64\n"]}],"source":["# ====================================================\n","# CV split\n","# ====================================================\n","# train['score_map'] = train['score'].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n","# Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n","# for n, (train_index, val_index) in enumerate(Fold.split(train, train['score_map'])):\n","#     train.loc[val_index, 'fold'] = int(n)\n","# train['fold'] = train['fold'].astype(int)\n","# display(train.groupby('fold').size())\n","\n","\n","# #### AUG\n","# aug = train[train['is_aug']==1].reset_index(drop=True)\n","# train = train[train['is_aug']==0].reset_index(drop=True)\n","# #### AUGAUG\n","\n","\n","# Credits to https://www.kaggle.com/code/hannes82/pppm-deberta-v3-large-closing-the-cv-lb-gap/notebook#CV-split\n","#credits to: https://www.kaggle.com/code/abhishek/creating-folds-properly-hopefully-p\n","\n","!pip install -q iterative-stratification\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","dfx = pd.get_dummies(train, columns=[\"score\"]).groupby([\"anchor\"], as_index=False).sum()\n","cols = [c for c in dfx.columns if c.startswith(\"score_\") or c == \"anchor\"]\n","dfx = dfx[cols]\n","\n","mskf = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=42)\n","labels = [c for c in dfx.columns if c != \"anchor\"]\n","dfx_labels = dfx[labels]\n","dfx[\"fold\"] = -1\n","\n","for fold, (trn_, val_) in enumerate(mskf.split(dfx, dfx_labels)):\n","    print(len(trn_), len(val_))\n","    dfx.loc[val_, \"fold\"] = fold\n","\n","train = train.merge(dfx[[\"anchor\", \"fold\"]], on=\"anchor\", how=\"left\")\n","del dfx\n","\n","\n","\n","# #### AUG\n","# res = []\n","# for fold in range(CFG.n_fold):\n","#     val_ids = train.loc[train['fold']==fold, 'id'].values\n","#     to_add_aug = aug[~aug['id'].isin(val_ids)].reset_index(drop=True)\n","#     to_add_aug['fold'] = fold+10\n","#     res.append(to_add_aug)\n","\n","# to_add_aug = pd.concat(res, axis=0, ignore_index=True)\n","# train = pd.concat([train, to_add_aug], axis=0, ignore_index=True)\n","# del aug, to_add_aug, res, val_ids\n","# #### AUGAUG\n","\n","print(train.fold.value_counts())"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1655711185147,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"4c3ce877"},"outputs":[],"source":["if CFG.debug:\n","    display(train.groupby('fold').size())\n","    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n","    display(train.groupby('fold').size())"]},{"cell_type":"markdown","metadata":{"id":"918a28aa"},"source":["# tokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3144,"status":"ok","timestamp":1655711188273,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"H4pgQRxAvMMv","outputId":"6575aedb-e95f-4628-83bc-23c44f4fa4b4"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# ====================================================\n","# tokenizer\n","# ====================================================\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n","CFG.tokenizer = tokenizer"]},{"cell_type":"markdown","metadata":{"id":"14da40cf"},"source":["# Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["3ac66b6c07744a33bff4d56865805e89","4c1639fe3dbb455aa5414d6b65a54e92","866d48fa5f504e0f8ac59a3c168d5973","be9cf9130db24e1fb7fa76740f0bccac","bee0b391360d42909042aec83fe3fe92","c41bc33e0e234f7dbdca4c6dca44bdb6","b03579da87b94698a417602ed4432c80","91954344fa794c38811d8bd3b8535f4a","e94793aa60df41d7a4130c55eae0d5b4","8653837c3b9e4435ab4332bf9d0f9093","739979a0b03f49bc9f93c59dceb5545d","039f80a40aa44932a4c6e4b727ffa1a3","f284cdbdd63243c4b4dac15f105a0130","b119ed613c8b45a4bb3984cb22898794","20dfec03c6e44a259baef51623a90bf4","c3dc992e38a7400580f88c701695ffa7","9a1a97345a73499388e45b1f6334b14f","fdddd16566c442b2b9f056517edaae18","dc000b714db643399b5d1e5332ea6a87","4ad792bc199c4be68cdd897b06cab243","e78b209fe3584824a8f841d0ca511e8a","7504df3a6f5542f9a251d12b71c55390","47bc78679d234a7bbc4d5cfc0647938e","e1b290e61709441c8990cc4abb91ac70","c284caafc5484f9fbdfd2b362c307571","c6cfae6a927541529bd0bfa911de2174","0f1c6e2cbe9d41aaaf64f3ecd682e964","87d44827c36243d88b03704779b2df84","559b152dafb049ce88c566eedfa69f89","9d815330b1d840628ab6b13ce41dbfc5","457496984fe643b8986d6fb2277ff292","81f89a025fd64a6398050b5b5f2cd2bb","75cb13840a764c06951251837acbcead"]},"executionInfo":{"elapsed":7332,"status":"ok","timestamp":1655711195593,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"c00327b0","outputId":"819f764f-bd39-4446-aa38-e72b80a3bbd9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/136 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac66b6c07744a33bff4d56865805e89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/36473 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"039f80a40aa44932a4c6e4b727ffa1a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/36473 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47bc78679d234a7bbc4d5cfc0647938e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["max_len: 133\n"]}],"source":["# ====================================================\n","# Define max_len\n","# ====================================================\n","lengths_dict = {}\n","\n","lengths = []\n","tk0 = tqdm(cpc_texts.values(), total=len(cpc_texts))\n","for text in tk0:\n","    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","    lengths.append(length)\n","lengths_dict['context_text'] = lengths\n","\n","for text_col in ['anchor', 'target']:\n","    lengths = []\n","    tk0 = tqdm(train[text_col].fillna(\"\").values, total=len(train))\n","    for text in tk0:\n","        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n","        lengths.append(length)\n","    lengths_dict[text_col] = lengths\n","    \n","CFG.max_len = max(lengths_dict['anchor']) + max(lengths_dict['target'])\\\n","                + max(lengths_dict['context_text']) + 4 # CLS + SEP + SEP + SEP\n","# CFG.max_len = max(max(lengths_dict['anchor'])+max(lengths_dict['target'])+3, max(lengths_dict['context_text'])+2)\n","LOGGER.info(f\"max_len: {CFG.max_len}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1655711196022,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"9f791a19"},"outputs":[],"source":["# ====================================================\n","# Dataset\n","# ====================================================\n","def prepare_input(cfg, text, text2):\n","    inputs = cfg.tokenizer(text, text2,\n","                           add_special_tokens=True,\n","                           max_length=cfg.max_len,\n","                           padding=\"max_length\",\n","                           return_offsets_mapping=False)\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype=torch.long)\n","    return inputs\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['text'].values\n","        self.texts2 = df['text2'].values\n","        self.labels = df['score'].values\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item], self.texts2[item])\n","        #inputs2 = prepare_input(self.cfg, self.texts2[item])\n","        label = torch.tensor(self.labels[item], dtype=torch.float)\n","        #return inputs, inputs2, label\n","        return inputs, label"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1655711196023,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"a200bd5b","outputId":"c092bd2a-f596-4ef6-e81a-966961f25e5e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ntrain_dataset = TrainDataset(CFG, train)\\ninputs, label = train_dataset[0]\\nprint(inputs)\\nprint(label)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["\"\"\"\n","train_dataset = TrainDataset(CFG, train)\n","inputs, label = train_dataset[0]\n","print(inputs)\n","print(label)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"e04d6363"},"source":["# Model"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1655711196037,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"4c5bab44"},"outputs":[],"source":["# ====================================================\n","# Model\n","# ====================================================\n","class CustomModel(nn.Module):\n","    def __init__(self, cfg, config_path=None, pretrained=False):\n","        super().__init__()\n","        self.cfg = cfg\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n","        else:\n","            self.config = torch.load(config_path)\n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n","        else:\n","            self.model = AutoModel.from_config(self.config)\n","        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n","        self.fc = nn.Linear(self.config.hidden_size, self.cfg.target_size)\n","        self._init_weights(self.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self._init_weights(self.attention)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","        last_hidden_states = outputs[0]\n","        \n","        # outputs2 = self.model(**inputs2)\n","        # last_hidden_states2 = outputs2[0]\n","        \n","        # feature = torch.mean(last_hidden_states, 1)\n","        weights = self.attention(last_hidden_states)\n","        feature = torch.sum(weights * last_hidden_states, dim=1)\n","        #feature2 = torch.mean(last_hidden_states2, dim=1)\n","        #feature += feature2\n","        return feature\n","\n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        output = self.fc(self.fc_dropout(feature))\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"deee9675"},"source":["# Helpler functions"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1655711196042,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"c8263b0c"},"outputs":[],"source":["# ====================================================\n","# Helper functions\n","# ====================================================\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n","\n","#### AWP\n","#def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, scaler, score, awp):\n","#### AWPAWP\n","    model.train()\n","    # AWP\n","    #scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    # AWPAWP\n","    losses = AverageMeter()\n","    start = end = time.time()\n","    global_step = 0\n","\n","    for step, (inputs, labels) in enumerate(train_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        # for k, v in inputs2.items():\n","        #     inputs2[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled=CFG.apex):\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","\n","        #### AWP\n","        if score > 0.75:\n","            awp.attack_backward(inputs['input_ids'], labels, inputs['attention_mask'], step) \n","        #### AWPAWP\n","\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f}  '\n","                  'LR: {lr:.8f}  '\n","                  .format(epoch+1, step, len(train_loader), \n","                          remain=timeSince(start, float(step+1)/len(train_loader)),\n","                          loss=losses,\n","                          grad_norm=grad_norm,\n","                          lr=scheduler.get_lr()[0]))\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] loss\": losses.val,\n","                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n","    return losses.avg\n","\n","\n","def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        # for k, v in inputs2.items():\n","        #     inputs2[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss=losses,\n","                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n","    predictions = np.concatenate(preds)\n","    predictions = np.concatenate(predictions)\n","    return losses.avg, predictions\n","\n","\n","def inference_fn(test_loader, model, device):\n","    preds = []\n","    model.eval()\n","    model.to(device)\n","    tk0 = tqdm(test_loader, total=len(test_loader))\n","    for inputs in tk0:\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        # for k, v in inputs[1].items():\n","        #     inputs[1][k] = v.to(device)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","        preds.append(y_preds.sigmoid().to('cpu').numpy())\n","    predictions = np.concatenate(preds)\n","    return predictions"]},{"cell_type":"code","source":["#### AWP\n","class AWP:\n","    def __init__(\n","        self,\n","        model,\n","        optimizer,\n","        adv_param=\"weight\",\n","        adv_lr=1,\n","        adv_eps=0.2,\n","        start_epoch=0,\n","        adv_step=1,\n","        scaler=None\n","    ):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.scaler = scaler\n","\n","    def attack_backward(self, x, y, attention_mask,epoch):\n","        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n","            return None\n","\n","        self._save() \n","        for i in range(self.adv_step):\n","            self._attack_step() \n","            with torch.cuda.amp.autocast():\n","                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n","                adv_loss = adv_loss.mean()\n","            self.optimizer.zero_grad()\n","            self.scaler.scale(adv_loss).backward()\n","            \n","        self._restore()\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self,):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}\n","\n","#### AWPAWP"],"metadata":{"id":"CI1gkNrPRgan","executionInfo":{"status":"ok","timestamp":1655711196044,"user_tz":-540,"elapsed":44,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1655711196051,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"},"user_tz":-540},"id":"bed940e1"},"outputs":[],"source":["# ====================================================\n","# train loop\n","# ====================================================\n","def train_loop(folds, fold):\n","    \n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","\n","    # ====================================================\n","    # loader\n","    # ====================================================\n","    \n","    # #### AUG\n","    # train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","    # train_folds_aug = train_folds[train_folds['fold']==fold+10].reset_index(drop=True)\n","    # train_folds_base = train_folds[train_folds['fold']<10].reset_index(drop=True)\n","    # train_folds = pd.concat([train_folds_base, train_folds_aug], axis=0, ignore_index=True)\n","    # del train_folds_aug, train_folds_base\n","    # #### AUGAUG\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n","\n","    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n","    valid_labels = valid_folds['score'].values\n","    \n","    train_dataset = TrainDataset(CFG, train_folds)\n","    valid_dataset = TrainDataset(CFG, valid_folds)\n","\n","    train_loader = DataLoader(train_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=True,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size=CFG.batch_size,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","\n","    # ====================================================\n","    # model & optimizer\n","    # ====================================================\n","    model = CustomModel(CFG, config_path=None, pretrained=True)\n","    torch.save(model.config, OUTPUT_DIR+'config.pth')\n","    model.to(device)\n","    \n","    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        optimizer_parameters = [\n","            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': weight_decay},\n","            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n","             'lr': encoder_lr, 'weight_decay': 0.0},\n","            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n","             'lr': decoder_lr, 'weight_decay': 0.0}\n","        ]\n","        return optimizer_parameters\n","\n","    optimizer_parameters = get_optimizer_params(model,\n","                                                encoder_lr=CFG.encoder_lr, \n","                                                decoder_lr=CFG.decoder_lr,\n","                                                weight_decay=CFG.weight_decay)\n","    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n","    \n","    # ====================================================\n","    # scheduler\n","    # ====================================================\n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","\n","    # ====================================================\n","    # loop\n","    # ====================================================\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n","    \n","    best_score = 0.\n","    #### AWP\n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n","    awp = AWP(model,\n","              optimizer,\n","              adv_lr=CFG.adv_lr,\n","              adv_eps=CFG.adv_eps,\n","              start_epoch=num_train_steps/CFG.epochs,\n","              scaler=scaler)\n","    score = 0.\n","    #### AWPAWP\n","    for epoch in range(CFG.epochs):\n","\n","        start_time = time.time()\n","\n","        # train\n","        #### AWP\n","        #avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, scaler, score, awp)\n","        #### AWPAWP\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n","        \n","        # scoring\n","        score = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","\n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n","        if CFG.wandb:\n","            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n","                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n","                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n","                       f\"[fold{fold}] score\": score})\n","        \n","        if best_score < score:\n","            best_score = score\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","\n","    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n","                             map_location=torch.device('cpu'))['predictions']\n","    valid_folds['pred'] = predictions\n","\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return valid_folds"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b7488cbea96045c28d4d68d581aee6b5","94ce92fd1f6a4f16a9b64423ecc17d6e","1f10820a76d04171b965c6d1c53d4d05","c2ef0454e21146dab8befdd64a2caf82","0ca49ba0ae114c50897adab014d039e0","5eb5e8a75e3044d785fc27ee4c1399f1","de96819e92bd4fbe83e83e5a753afd8b","7aa7dbbadb2a45a6b913651a22864353"]},"id":"6cc76b1e","outputId":"6f8677dd-2268-4c09-a550-4ae20d16a5a1","executionInfo":{"status":"ok","timestamp":1655735326052,"user_tz":-540,"elapsed":12512865,"user":{"displayName":"Ryosuke Horiuchi","userId":"18359745667022839599"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 0 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2032] Elapsed 0m 0s (remain 26m 25s) Loss: 0.6698(0.6698) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2032] Elapsed 0m 27s (remain 8m 51s) Loss: 0.6552(0.6320) Grad: 41921.7188  LR: 0.00001999  \n","Epoch: [1][200/2032] Elapsed 0m 54s (remain 8m 20s) Loss: 0.5296(0.6066) Grad: 45010.0078  LR: 0.00001997  \n","Epoch: [1][300/2032] Elapsed 1m 22s (remain 7m 51s) Loss: 0.4997(0.5936) Grad: 13322.3369  LR: 0.00001993  \n","Epoch: [1][400/2032] Elapsed 1m 48s (remain 7m 22s) Loss: 0.5065(0.5855) Grad: 26405.1055  LR: 0.00001988  \n","Epoch: [1][500/2032] Elapsed 2m 15s (remain 6m 55s) Loss: 0.5063(0.5795) Grad: 26119.7734  LR: 0.00001981  \n","Epoch: [1][600/2032] Elapsed 2m 42s (remain 6m 27s) Loss: 0.6406(0.5758) Grad: 11106.2959  LR: 0.00001973  \n","Epoch: [1][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.5610(0.5728) Grad: 21092.0781  LR: 0.00001964  \n","Epoch: [1][800/2032] Elapsed 3m 37s (remain 5m 33s) Loss: 0.5775(0.5688) Grad: 23264.3828  LR: 0.00001952  \n","Epoch: [1][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.6571(0.5677) Grad: 66112.8047  LR: 0.00001940  \n","Epoch: [1][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.5294(0.5654) Grad: 26276.8145  LR: 0.00001926  \n","Epoch: [1][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.5002(0.5630) Grad: 15811.3623  LR: 0.00001911  \n","Epoch: [1][1200/2032] Elapsed 5m 25s (remain 3m 45s) Loss: 0.5142(0.5621) Grad: 19305.4785  LR: 0.00001894  \n","Epoch: [1][1300/2032] Elapsed 5m 52s (remain 3m 18s) Loss: 0.4875(0.5610) Grad: 16167.9570  LR: 0.00001876  \n","Epoch: [1][1400/2032] Elapsed 6m 19s (remain 2m 50s) Loss: 0.5910(0.5594) Grad: 31507.9492  LR: 0.00001857  \n","Epoch: [1][1500/2032] Elapsed 6m 46s (remain 2m 23s) Loss: 0.5852(0.5578) Grad: 61277.2461  LR: 0.00001836  \n","Epoch: [1][1600/2032] Elapsed 7m 13s (remain 1m 56s) Loss: 0.4838(0.5561) Grad: 22654.6504  LR: 0.00001815  \n","Epoch: [1][1700/2032] Elapsed 7m 40s (remain 1m 29s) Loss: 0.5426(0.5557) Grad: 17695.7734  LR: 0.00001792  \n","Epoch: [1][1800/2032] Elapsed 8m 7s (remain 1m 2s) Loss: 0.5093(0.5555) Grad: 15486.0342  LR: 0.00001767  \n","Epoch: [1][1900/2032] Elapsed 8m 34s (remain 0m 35s) Loss: 0.5465(0.5545) Grad: 14376.0850  LR: 0.00001742  \n","Epoch: [1][2000/2032] Elapsed 9m 1s (remain 0m 8s) Loss: 0.6409(0.5540) Grad: 29913.7695  LR: 0.00001716  \n","Epoch: [1][2031/2032] Elapsed 9m 9s (remain 0m 0s) Loss: 0.5980(0.5537) Grad: 43791.5977  LR: 0.00001707  \n","EVAL: [0/248] Elapsed 0m 0s (remain 2m 27s) Loss: 0.4817(0.4817) \n","EVAL: [100/248] Elapsed 0m 16s (remain 0m 24s) Loss: 0.4175(0.5513) \n","EVAL: [200/248] Elapsed 0m 33s (remain 0m 7s) Loss: 0.4256(0.5668) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5537  avg_val_loss: 0.5614  time: 591s\n","Epoch 1 - Score: 0.7967\n","Epoch 1 - Save Best Score: 0.7967 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [247/248] Elapsed 0m 40s (remain 0m 0s) Loss: 0.6381(0.5614) \n","Epoch: [2][0/2032] Elapsed 0m 0s (remain 25m 28s) Loss: 0.4998(0.4998) Grad: 17176.9199  LR: 0.00001707  \n","Epoch: [2][100/2032] Elapsed 0m 28s (remain 8m 57s) Loss: 0.5659(0.5321) Grad: 48832.8789  LR: 0.00001679  \n","Epoch: [2][200/2032] Elapsed 0m 55s (remain 8m 26s) Loss: 0.4927(0.5212) Grad: 37958.6367  LR: 0.00001650  \n","Epoch: [2][300/2032] Elapsed 1m 22s (remain 7m 55s) Loss: 0.5458(0.5233) Grad: 34402.4141  LR: 0.00001620  \n","Epoch: [2][400/2032] Elapsed 1m 49s (remain 7m 25s) Loss: 0.4616(0.5212) Grad: 45942.8398  LR: 0.00001590  \n","Epoch: [2][500/2032] Elapsed 2m 16s (remain 6m 57s) Loss: 0.5652(0.5173) Grad: 65514.1758  LR: 0.00001558  \n","Epoch: [2][600/2032] Elapsed 2m 43s (remain 6m 29s) Loss: 0.4689(0.5168) Grad: 52502.7422  LR: 0.00001525  \n","Epoch: [2][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.3894(0.5173) Grad: 63074.6914  LR: 0.00001492  \n","Epoch: [2][800/2032] Elapsed 3m 37s (remain 5m 34s) Loss: 0.4254(0.5169) Grad: 64150.4062  LR: 0.00001458  \n","Epoch: [2][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.5779(0.5166) Grad: 33194.7227  LR: 0.00001423  \n","Epoch: [2][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.6318(0.5178) Grad: 21854.9219  LR: 0.00001388  \n","Epoch: [2][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.4620(0.5182) Grad: 41301.8164  LR: 0.00001352  \n","Epoch: [2][1200/2032] Elapsed 5m 25s (remain 3m 45s) Loss: 0.5557(0.5177) Grad: 11827.0156  LR: 0.00001316  \n","Epoch: [2][1300/2032] Elapsed 5m 52s (remain 3m 18s) Loss: 0.6080(0.5183) Grad: 98955.4531  LR: 0.00001279  \n","Epoch: [2][1400/2032] Elapsed 6m 19s (remain 2m 51s) Loss: 0.5644(0.5185) Grad: 43296.1797  LR: 0.00001242  \n","Epoch: [2][1500/2032] Elapsed 6m 46s (remain 2m 23s) Loss: 0.4827(0.5181) Grad: 13927.5342  LR: 0.00001204  \n","Epoch: [2][1600/2032] Elapsed 7m 13s (remain 1m 56s) Loss: 0.5611(0.5189) Grad: 27872.6113  LR: 0.00001166  \n","Epoch: [2][1700/2032] Elapsed 7m 41s (remain 1m 29s) Loss: 0.5944(0.5186) Grad: 39230.0625  LR: 0.00001128  \n","Epoch: [2][1800/2032] Elapsed 8m 8s (remain 1m 2s) Loss: 0.5858(0.5185) Grad: 64841.0234  LR: 0.00001089  \n","Epoch: [2][1900/2032] Elapsed 8m 35s (remain 0m 35s) Loss: 0.4954(0.5182) Grad: 9952.8008  LR: 0.00001051  \n","Epoch: [2][2000/2032] Elapsed 9m 2s (remain 0m 8s) Loss: 0.6228(0.5178) Grad: 56684.0391  LR: 0.00001012  \n","Epoch: [2][2031/2032] Elapsed 9m 10s (remain 0m 0s) Loss: 0.5930(0.5176) Grad: 25619.3457  LR: 0.00001000  \n","EVAL: [0/248] Elapsed 0m 0s (remain 2m 25s) Loss: 0.4770(0.4770) \n","EVAL: [100/248] Elapsed 0m 16s (remain 0m 24s) Loss: 0.4545(0.5495) \n","EVAL: [200/248] Elapsed 0m 33s (remain 0m 7s) Loss: 0.4574(0.5665) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5176  avg_val_loss: 0.5587  time: 591s\n","Epoch 2 - Score: 0.8036\n","Epoch 2 - Save Best Score: 0.8036 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [247/248] Elapsed 0m 40s (remain 0m 0s) Loss: 0.6329(0.5587) \n","Epoch: [3][0/2032] Elapsed 0m 0s (remain 25m 42s) Loss: 0.4988(0.4988) Grad: 11779.0078  LR: 0.00001000  \n","Epoch: [3][100/2032] Elapsed 0m 28s (remain 8m 59s) Loss: 0.5524(0.5023) Grad: 771276.0625  LR: 0.00000961  \n","Epoch: [3][200/2032] Elapsed 0m 55s (remain 8m 28s) Loss: 0.5290(0.5032) Grad: 16192.3643  LR: 0.00000923  \n","Epoch: [3][300/2032] Elapsed 1m 22s (remain 7m 56s) Loss: 0.4994(0.5030) Grad: 10724.7676  LR: 0.00000884  \n","Epoch: [3][400/2032] Elapsed 1m 49s (remain 7m 26s) Loss: 0.4381(0.4992) Grad: 19537.0312  LR: 0.00000846  \n","Epoch: [3][500/2032] Elapsed 2m 16s (remain 6m 57s) Loss: 0.4677(0.5022) Grad: 15027.7207  LR: 0.00000808  \n","Epoch: [3][600/2032] Elapsed 2m 43s (remain 6m 29s) Loss: 0.5272(0.5043) Grad: 29611.5508  LR: 0.00000770  \n","Epoch: [3][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.4368(0.5052) Grad: 11801.2188  LR: 0.00000733  \n","Epoch: [3][800/2032] Elapsed 3m 37s (remain 5m 34s) Loss: 0.4809(0.5047) Grad: 7776.3413  LR: 0.00000696  \n","Epoch: [3][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.4273(0.5048) Grad: 9599.2617  LR: 0.00000659  \n","Epoch: [3][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.4508(0.5044) Grad: 8278.1494  LR: 0.00000623  \n","Epoch: [3][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.5359(0.5039) Grad: 14604.7139  LR: 0.00000587  \n","Epoch: [3][1200/2032] Elapsed 5m 24s (remain 3m 44s) Loss: 0.5218(0.5041) Grad: 28727.0938  LR: 0.00000553  \n","Epoch: [3][1300/2032] Elapsed 5m 51s (remain 3m 17s) Loss: 0.6359(0.5038) Grad: 30678.7852  LR: 0.00000518  \n","Epoch: [3][1400/2032] Elapsed 6m 18s (remain 2m 50s) Loss: 0.3960(0.5039) Grad: 34095.1250  LR: 0.00000485  \n","Epoch: [3][1500/2032] Elapsed 6m 45s (remain 2m 23s) Loss: 0.5361(0.5043) Grad: 13632.5859  LR: 0.00000452  \n","Epoch: [3][1600/2032] Elapsed 7m 12s (remain 1m 56s) Loss: 0.5366(0.5047) Grad: 15225.7471  LR: 0.00000420  \n","Epoch: [3][1700/2032] Elapsed 7m 39s (remain 1m 29s) Loss: 0.5997(0.5047) Grad: 39197.4727  LR: 0.00000389  \n","Epoch: [3][1800/2032] Elapsed 8m 6s (remain 1m 2s) Loss: 0.5125(0.5047) Grad: 33721.1523  LR: 0.00000359  \n","Epoch: [3][1900/2032] Elapsed 8m 33s (remain 0m 35s) Loss: 0.5663(0.5040) Grad: 14389.1084  LR: 0.00000330  \n","Epoch: [3][2000/2032] Elapsed 9m 0s (remain 0m 8s) Loss: 0.4172(0.5038) Grad: 24511.7207  LR: 0.00000302  \n","Epoch: [3][2031/2032] Elapsed 9m 8s (remain 0m 0s) Loss: 0.3835(0.5036) Grad: 56403.0898  LR: 0.00000293  \n","EVAL: [0/248] Elapsed 0m 0s (remain 2m 28s) Loss: 0.4752(0.4752) \n","EVAL: [100/248] Elapsed 0m 16s (remain 0m 24s) Loss: 0.4145(0.5654) \n","EVAL: [200/248] Elapsed 0m 33s (remain 0m 7s) Loss: 0.4319(0.5841) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5036  avg_val_loss: 0.5753  time: 590s\n","Epoch 3 - Score: 0.8105\n","Epoch 3 - Save Best Score: 0.8105 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [247/248] Elapsed 0m 40s (remain 0m 0s) Loss: 0.6304(0.5753) \n","Epoch: [4][0/2032] Elapsed 0m 0s (remain 25m 11s) Loss: 0.6195(0.6195) Grad: 159894.7969  LR: 0.00000293  \n","Epoch: [4][100/2032] Elapsed 0m 28s (remain 8m 57s) Loss: 0.4221(0.4889) Grad: 133283.2344  LR: 0.00000266  \n","Epoch: [4][200/2032] Elapsed 0m 55s (remain 8m 25s) Loss: 0.4198(0.4912) Grad: 11909.8066  LR: 0.00000240  \n","Epoch: [4][300/2032] Elapsed 1m 22s (remain 7m 53s) Loss: 0.4914(0.4921) Grad: 12812.2344  LR: 0.00000216  \n","Epoch: [4][400/2032] Elapsed 1m 49s (remain 7m 24s) Loss: 0.5010(0.4949) Grad: 18269.4629  LR: 0.00000192  \n","Epoch: [4][500/2032] Elapsed 2m 16s (remain 6m 57s) Loss: 0.6684(0.4958) Grad: 17849.8418  LR: 0.00000170  \n","Epoch: [4][600/2032] Elapsed 2m 43s (remain 6m 29s) Loss: 0.6269(0.4966) Grad: 15439.1885  LR: 0.00000149  \n","Epoch: [4][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.4990(0.4964) Grad: 22674.6641  LR: 0.00000130  \n","Epoch: [4][800/2032] Elapsed 3m 37s (remain 5m 34s) Loss: 0.5957(0.4955) Grad: 58500.2773  LR: 0.00000111  \n","Epoch: [4][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.5453(0.4958) Grad: 44895.2383  LR: 0.00000094  \n","Epoch: [4][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.5674(0.4959) Grad: 26897.5840  LR: 0.00000078  \n","Epoch: [4][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.4901(0.4957) Grad: 51248.9023  LR: 0.00000064  \n","Epoch: [4][1200/2032] Elapsed 5m 25s (remain 3m 44s) Loss: 0.4546(0.4946) Grad: 80521.1094  LR: 0.00000051  \n","Epoch: [4][1300/2032] Elapsed 5m 51s (remain 3m 17s) Loss: 0.4889(0.4944) Grad: 225270.4844  LR: 0.00000040  \n","Epoch: [4][1400/2032] Elapsed 6m 18s (remain 2m 50s) Loss: 0.4487(0.4948) Grad: 19118.2266  LR: 0.00000030  \n","Epoch: [4][1500/2032] Elapsed 6m 45s (remain 2m 23s) Loss: 0.4623(0.4942) Grad: 62607.6992  LR: 0.00000021  \n","Epoch: [4][1600/2032] Elapsed 7m 12s (remain 1m 56s) Loss: 0.4820(0.4943) Grad: 38916.9883  LR: 0.00000014  \n","Epoch: [4][1700/2032] Elapsed 7m 40s (remain 1m 29s) Loss: 0.4865(0.4936) Grad: 79111.9062  LR: 0.00000008  \n","Epoch: [4][1800/2032] Elapsed 8m 7s (remain 1m 2s) Loss: 0.4987(0.4938) Grad: 49460.3555  LR: 0.00000004  \n","Epoch: [4][1900/2032] Elapsed 8m 34s (remain 0m 35s) Loss: 0.4846(0.4937) Grad: 48944.2188  LR: 0.00000001  \n","Epoch: [4][2000/2032] Elapsed 9m 1s (remain 0m 8s) Loss: 0.3913(0.4942) Grad: 108797.0391  LR: 0.00000000  \n","Epoch: [4][2031/2032] Elapsed 9m 9s (remain 0m 0s) Loss: 0.4479(0.4940) Grad: 36585.0859  LR: 0.00000000  \n","EVAL: [0/248] Elapsed 0m 0s (remain 2m 25s) Loss: 0.4744(0.4744) \n","EVAL: [100/248] Elapsed 0m 16s (remain 0m 24s) Loss: 0.4124(0.5652) \n","EVAL: [200/248] Elapsed 0m 33s (remain 0m 7s) Loss: 0.4368(0.5857) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4940  avg_val_loss: 0.5768  time: 590s\n","Epoch 4 - Score: 0.8140\n","Epoch 4 - Save Best Score: 0.8140 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [247/248] Elapsed 0m 40s (remain 0m 0s) Loss: 0.6285(0.5768) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 0 result ==========\n","Score: 0.8140\n","========== fold: 1 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2052] Elapsed 0m 0s (remain 25m 29s) Loss: 0.7794(0.7794) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2052] Elapsed 0m 28s (remain 9m 4s) Loss: 0.5983(0.6555) Grad: 23219.5586  LR: 0.00001999  \n","Epoch: [1][200/2052] Elapsed 0m 55s (remain 8m 29s) Loss: 0.5799(0.6257) Grad: 38772.6445  LR: 0.00001997  \n","Epoch: [1][300/2052] Elapsed 1m 22s (remain 7m 59s) Loss: 0.5372(0.6095) Grad: 16629.3008  LR: 0.00001993  \n","Epoch: [1][400/2052] Elapsed 1m 49s (remain 7m 30s) Loss: 0.6308(0.6006) Grad: 31863.6094  LR: 0.00001988  \n","Epoch: [1][500/2052] Elapsed 2m 16s (remain 7m 2s) Loss: 0.4337(0.5920) Grad: 30538.3652  LR: 0.00001982  \n","Epoch: [1][600/2052] Elapsed 2m 43s (remain 6m 34s) Loss: 0.5073(0.5839) Grad: 47717.8711  LR: 0.00001974  \n","Epoch: [1][700/2052] Elapsed 3m 10s (remain 6m 7s) Loss: 0.4236(0.5783) Grad: 31151.3984  LR: 0.00001964  \n","Epoch: [1][800/2052] Elapsed 3m 37s (remain 5m 40s) Loss: 0.6395(0.5739) Grad: 22035.9961  LR: 0.00001953  \n","Epoch: [1][900/2052] Elapsed 4m 4s (remain 5m 12s) Loss: 0.5126(0.5725) Grad: 22595.4785  LR: 0.00001941  \n","Epoch: [1][1000/2052] Elapsed 4m 31s (remain 4m 45s) Loss: 0.5372(0.5697) Grad: 29278.6738  LR: 0.00001928  \n","Epoch: [1][1100/2052] Elapsed 4m 59s (remain 4m 18s) Loss: 0.5258(0.5678) Grad: 13418.7227  LR: 0.00001913  \n","Epoch: [1][1200/2052] Elapsed 5m 26s (remain 3m 51s) Loss: 0.4186(0.5654) Grad: 16487.5156  LR: 0.00001896  \n","Epoch: [1][1300/2052] Elapsed 5m 53s (remain 3m 24s) Loss: 0.3913(0.5627) Grad: 24749.2910  LR: 0.00001879  \n","Epoch: [1][1400/2052] Elapsed 6m 20s (remain 2m 56s) Loss: 0.5204(0.5613) Grad: 54795.9141  LR: 0.00001860  \n","Epoch: [1][1500/2052] Elapsed 6m 47s (remain 2m 29s) Loss: 0.4006(0.5606) Grad: 13750.0186  LR: 0.00001839  \n","Epoch: [1][1600/2052] Elapsed 7m 14s (remain 2m 2s) Loss: 0.5021(0.5597) Grad: 14213.0059  LR: 0.00001818  \n","Epoch: [1][1700/2052] Elapsed 7m 42s (remain 1m 35s) Loss: 0.5396(0.5584) Grad: 20265.5078  LR: 0.00001795  \n","Epoch: [1][1800/2052] Elapsed 8m 9s (remain 1m 8s) Loss: 0.5859(0.5578) Grad: 15890.1719  LR: 0.00001772  \n","Epoch: [1][1900/2052] Elapsed 8m 36s (remain 0m 41s) Loss: 0.5541(0.5576) Grad: 11792.6758  LR: 0.00001747  \n","Epoch: [1][2000/2052] Elapsed 9m 3s (remain 0m 13s) Loss: 0.6083(0.5562) Grad: 23298.8262  LR: 0.00001721  \n","Epoch: [1][2051/2052] Elapsed 9m 17s (remain 0m 0s) Loss: 0.5065(0.5559) Grad: 129955.0703  LR: 0.00001707  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 25s) Loss: 0.6084(0.6084) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4132(0.5342) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.6033(0.5418) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5559  avg_val_loss: 0.5416  time: 595s\n","Epoch 1 - Score: 0.8336\n","Epoch 1 - Save Best Score: 0.8336 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.3153(0.5416) \n","Epoch: [2][0/2052] Elapsed 0m 0s (remain 26m 43s) Loss: 0.4519(0.4519) Grad: 35173.4297  LR: 0.00001707  \n","Epoch: [2][100/2052] Elapsed 0m 28s (remain 9m 3s) Loss: 0.5289(0.5214) Grad: 55778.9961  LR: 0.00001679  \n","Epoch: [2][200/2052] Elapsed 0m 55s (remain 8m 28s) Loss: 0.3983(0.5136) Grad: 20476.2246  LR: 0.00001651  \n","Epoch: [2][300/2052] Elapsed 1m 22s (remain 7m 59s) Loss: 0.3739(0.5165) Grad: 40530.5078  LR: 0.00001621  \n","Epoch: [2][400/2052] Elapsed 1m 49s (remain 7m 29s) Loss: 0.5199(0.5153) Grad: 44588.2344  LR: 0.00001591  \n","Epoch: [2][500/2052] Elapsed 2m 16s (remain 7m 1s) Loss: 0.4252(0.5172) Grad: 39176.4492  LR: 0.00001559  \n","Epoch: [2][600/2052] Elapsed 2m 42s (remain 6m 33s) Loss: 0.5536(0.5189) Grad: 21588.0391  LR: 0.00001527  \n","Epoch: [2][700/2052] Elapsed 3m 9s (remain 6m 6s) Loss: 0.4647(0.5201) Grad: 42983.4258  LR: 0.00001494  \n","Epoch: [2][800/2052] Elapsed 3m 36s (remain 5m 38s) Loss: 0.5168(0.5182) Grad: 98028.9141  LR: 0.00001461  \n","Epoch: [2][900/2052] Elapsed 4m 3s (remain 5m 11s) Loss: 0.5603(0.5184) Grad: 63697.2227  LR: 0.00001427  \n","Epoch: [2][1000/2052] Elapsed 4m 30s (remain 4m 44s) Loss: 0.5735(0.5177) Grad: 21805.3477  LR: 0.00001392  \n","Epoch: [2][1100/2052] Elapsed 4m 57s (remain 4m 16s) Loss: 0.5749(0.5181) Grad: 65671.8906  LR: 0.00001356  \n","Epoch: [2][1200/2052] Elapsed 5m 24s (remain 3m 49s) Loss: 0.6185(0.5182) Grad: 28630.9238  LR: 0.00001320  \n","Epoch: [2][1300/2052] Elapsed 5m 51s (remain 3m 22s) Loss: 0.5918(0.5188) Grad: 15557.9248  LR: 0.00001284  \n","Epoch: [2][1400/2052] Elapsed 6m 18s (remain 2m 55s) Loss: 0.5458(0.5177) Grad: 41680.3398  LR: 0.00001247  \n","Epoch: [2][1500/2052] Elapsed 6m 45s (remain 2m 28s) Loss: 0.5137(0.5184) Grad: 21312.8047  LR: 0.00001209  \n","Epoch: [2][1600/2052] Elapsed 7m 12s (remain 2m 1s) Loss: 0.5976(0.5189) Grad: 20569.2305  LR: 0.00001172  \n","Epoch: [2][1700/2052] Elapsed 7m 39s (remain 1m 34s) Loss: 0.4173(0.5193) Grad: 14542.0723  LR: 0.00001134  \n","Epoch: [2][1800/2052] Elapsed 8m 6s (remain 1m 7s) Loss: 0.5518(0.5192) Grad: 138932.1562  LR: 0.00001096  \n","Epoch: [2][1900/2052] Elapsed 8m 33s (remain 0m 40s) Loss: 0.4935(0.5195) Grad: 26388.2285  LR: 0.00001058  \n","Epoch: [2][2000/2052] Elapsed 9m 0s (remain 0m 13s) Loss: 0.5144(0.5196) Grad: 33022.2070  LR: 0.00001020  \n","Epoch: [2][2051/2052] Elapsed 9m 14s (remain 0m 0s) Loss: 0.5251(0.5191) Grad: 8307.7832  LR: 0.00001000  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 24s) Loss: 0.6256(0.6256) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4367(0.5349) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5994(0.5488) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5191  avg_val_loss: 0.5470  time: 592s\n","Epoch 2 - Score: 0.8349\n","Epoch 2 - Save Best Score: 0.8349 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.2433(0.5470) \n","Epoch: [3][0/2052] Elapsed 0m 0s (remain 27m 25s) Loss: 0.4697(0.4697) Grad: 9092.6455  LR: 0.00001000  \n","Epoch: [3][100/2052] Elapsed 0m 28s (remain 9m 4s) Loss: 0.4884(0.5015) Grad: 21798.3516  LR: 0.00000962  \n","Epoch: [3][200/2052] Elapsed 0m 55s (remain 8m 31s) Loss: 0.5255(0.5011) Grad: 10322.8174  LR: 0.00000923  \n","Epoch: [3][300/2052] Elapsed 1m 22s (remain 8m 0s) Loss: 0.4201(0.5042) Grad: 5890.2671  LR: 0.00000885  \n","Epoch: [3][400/2052] Elapsed 1m 49s (remain 7m 30s) Loss: 0.4377(0.5037) Grad: 10809.5488  LR: 0.00000847  \n","Epoch: [3][500/2052] Elapsed 2m 16s (remain 7m 2s) Loss: 0.5140(0.5010) Grad: 21074.5078  LR: 0.00000810  \n","Epoch: [3][600/2052] Elapsed 2m 43s (remain 6m 35s) Loss: 0.6424(0.5018) Grad: 27764.5312  LR: 0.00000772  \n","Epoch: [3][700/2052] Elapsed 3m 11s (remain 6m 8s) Loss: 0.4134(0.5036) Grad: 29632.2012  LR: 0.00000735  \n","Epoch: [3][800/2052] Elapsed 3m 37s (remain 5m 40s) Loss: 0.4562(0.5038) Grad: 18514.8340  LR: 0.00000698  \n","Epoch: [3][900/2052] Elapsed 4m 4s (remain 5m 12s) Loss: 0.4299(0.5032) Grad: 45605.3477  LR: 0.00000662  \n","Epoch: [3][1000/2052] Elapsed 4m 32s (remain 4m 45s) Loss: 0.4191(0.5019) Grad: 32101.5723  LR: 0.00000626  \n","Epoch: [3][1100/2052] Elapsed 4m 58s (remain 4m 18s) Loss: 0.5668(0.5016) Grad: 21468.7266  LR: 0.00000591  \n","Epoch: [3][1200/2052] Elapsed 5m 26s (remain 3m 51s) Loss: 0.4813(0.5019) Grad: 11771.0830  LR: 0.00000557  \n","Epoch: [3][1300/2052] Elapsed 5m 53s (remain 3m 23s) Loss: 0.4727(0.5030) Grad: 45796.8320  LR: 0.00000523  \n","Epoch: [3][1400/2052] Elapsed 6m 20s (remain 2m 56s) Loss: 0.4977(0.5031) Grad: 48804.1055  LR: 0.00000489  \n","Epoch: [3][1500/2052] Elapsed 6m 47s (remain 2m 29s) Loss: 0.5950(0.5025) Grad: 24207.0410  LR: 0.00000457  \n","Epoch: [3][1600/2052] Elapsed 7m 14s (remain 2m 2s) Loss: 0.6018(0.5024) Grad: 29631.5215  LR: 0.00000425  \n","Epoch: [3][1700/2052] Elapsed 7m 41s (remain 1m 35s) Loss: 0.5447(0.5024) Grad: 32327.6562  LR: 0.00000394  \n","Epoch: [3][1800/2052] Elapsed 8m 8s (remain 1m 8s) Loss: 0.4714(0.5025) Grad: 12946.2744  LR: 0.00000364  \n","Epoch: [3][1900/2052] Elapsed 8m 35s (remain 0m 40s) Loss: 0.4463(0.5028) Grad: 24825.7969  LR: 0.00000335  \n","Epoch: [3][2000/2052] Elapsed 9m 2s (remain 0m 13s) Loss: 0.6405(0.5034) Grad: 71417.7266  LR: 0.00000307  \n","Epoch: [3][2051/2052] Elapsed 9m 16s (remain 0m 0s) Loss: 0.5228(0.5034) Grad: 18300.8438  LR: 0.00000293  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 23s) Loss: 0.8004(0.8004) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4059(0.5559) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5975(0.5633) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5034  avg_val_loss: 0.5610  time: 594s\n","Epoch 3 - Score: 0.8393\n","Epoch 3 - Save Best Score: 0.8393 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.2461(0.5610) \n","Epoch: [4][0/2052] Elapsed 0m 0s (remain 27m 32s) Loss: 0.4626(0.4626) Grad: 74916.9062  LR: 0.00000293  \n","Epoch: [4][100/2052] Elapsed 0m 28s (remain 9m 5s) Loss: 0.5304(0.4967) Grad: 16361.3584  LR: 0.00000266  \n","Epoch: [4][200/2052] Elapsed 0m 55s (remain 8m 33s) Loss: 0.4446(0.4926) Grad: 16503.3613  LR: 0.00000241  \n","Epoch: [4][300/2052] Elapsed 1m 22s (remain 8m 1s) Loss: 0.4580(0.4885) Grad: 13098.6699  LR: 0.00000216  \n","Epoch: [4][400/2052] Elapsed 1m 49s (remain 7m 32s) Loss: 0.5255(0.4932) Grad: 24623.7383  LR: 0.00000193  \n","Epoch: [4][500/2052] Elapsed 2m 16s (remain 7m 3s) Loss: 0.4778(0.4934) Grad: 35508.3594  LR: 0.00000171  \n","Epoch: [4][600/2052] Elapsed 2m 43s (remain 6m 35s) Loss: 0.4277(0.4925) Grad: 9183.6787  LR: 0.00000150  \n","Epoch: [4][700/2052] Elapsed 3m 10s (remain 6m 7s) Loss: 0.4629(0.4953) Grad: 160918.9062  LR: 0.00000131  \n","Epoch: [4][800/2052] Elapsed 3m 37s (remain 5m 40s) Loss: 0.5499(0.4954) Grad: 49459.8164  LR: 0.00000113  \n","Epoch: [4][900/2052] Elapsed 4m 4s (remain 5m 12s) Loss: 0.4458(0.4959) Grad: 36905.1914  LR: 0.00000096  \n","Epoch: [4][1000/2052] Elapsed 4m 31s (remain 4m 45s) Loss: 0.4711(0.4959) Grad: 26949.3809  LR: 0.00000080  \n","Epoch: [4][1100/2052] Elapsed 4m 59s (remain 4m 18s) Loss: 0.3485(0.4956) Grad: 28045.5078  LR: 0.00000066  \n","Epoch: [4][1200/2052] Elapsed 5m 26s (remain 3m 51s) Loss: 0.4308(0.4959) Grad: 16597.9512  LR: 0.00000053  \n","Epoch: [4][1300/2052] Elapsed 5m 53s (remain 3m 23s) Loss: 0.4777(0.4944) Grad: 36660.6875  LR: 0.00000041  \n","Epoch: [4][1400/2052] Elapsed 6m 20s (remain 2m 56s) Loss: 0.4925(0.4939) Grad: 36607.0820  LR: 0.00000031  \n","Epoch: [4][1500/2052] Elapsed 6m 47s (remain 2m 29s) Loss: 0.6118(0.4945) Grad: 27367.8633  LR: 0.00000022  \n","Epoch: [4][1600/2052] Elapsed 7m 14s (remain 2m 2s) Loss: 0.5189(0.4949) Grad: 25850.5645  LR: 0.00000015  \n","Epoch: [4][1700/2052] Elapsed 7m 41s (remain 1m 35s) Loss: 0.4717(0.4951) Grad: 32674.1621  LR: 0.00000009  \n","Epoch: [4][1800/2052] Elapsed 8m 8s (remain 1m 8s) Loss: 0.4370(0.4948) Grad: 10671.3262  LR: 0.00000005  \n","Epoch: [4][1900/2052] Elapsed 8m 35s (remain 0m 40s) Loss: 0.4083(0.4948) Grad: 30748.0527  LR: 0.00000002  \n","Epoch: [4][2000/2052] Elapsed 9m 2s (remain 0m 13s) Loss: 0.5107(0.4942) Grad: 20310.6973  LR: 0.00000000  \n","Epoch: [4][2051/2052] Elapsed 9m 16s (remain 0m 0s) Loss: 0.5905(0.4947) Grad: 18084.0664  LR: 0.00000000  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 31s) Loss: 0.8053(0.8053) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4065(0.5595) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5981(0.5683) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4947  avg_val_loss: 0.5658  time: 594s\n","Epoch 4 - Score: 0.8377\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.2403(0.5658) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 1 result ==========\n","Score: 0.8393\n","========== fold: 2 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2059] Elapsed 0m 0s (remain 25m 56s) Loss: 0.7206(0.7206) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2059] Elapsed 0m 27s (remain 9m 1s) Loss: 0.6378(0.6532) Grad: 60292.4727  LR: 0.00001999  \n","Epoch: [1][200/2059] Elapsed 0m 55s (remain 8m 29s) Loss: 0.5352(0.6225) Grad: 93923.5625  LR: 0.00001997  \n","Epoch: [1][300/2059] Elapsed 1m 22s (remain 8m 1s) Loss: 0.6173(0.6065) Grad: 92125.7188  LR: 0.00001993  \n","Epoch: [1][400/2059] Elapsed 1m 49s (remain 7m 32s) Loss: 0.6055(0.6007) Grad: 83238.1328  LR: 0.00001988  \n","Epoch: [1][500/2059] Elapsed 2m 16s (remain 7m 4s) Loss: 0.6142(0.5945) Grad: 14060.3955  LR: 0.00001982  \n","Epoch: [1][600/2059] Elapsed 2m 43s (remain 6m 37s) Loss: 0.5923(0.5917) Grad: 24818.1055  LR: 0.00001974  \n","Epoch: [1][700/2059] Elapsed 3m 10s (remain 6m 9s) Loss: 0.4924(0.5865) Grad: 17175.9121  LR: 0.00001964  \n","Epoch: [1][800/2059] Elapsed 3m 38s (remain 5m 42s) Loss: 0.5910(0.5832) Grad: 11544.8506  LR: 0.00001954  \n","Epoch: [1][900/2059] Elapsed 4m 5s (remain 5m 15s) Loss: 0.5879(0.5798) Grad: 11737.2988  LR: 0.00001942  \n","Epoch: [1][1000/2059] Elapsed 4m 32s (remain 4m 47s) Loss: 0.6330(0.5770) Grad: 6474.8633  LR: 0.00001928  \n","Epoch: [1][1100/2059] Elapsed 4m 59s (remain 4m 20s) Loss: 0.5798(0.5756) Grad: 11047.8682  LR: 0.00001913  \n","Epoch: [1][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.5558(0.5739) Grad: 11147.6006  LR: 0.00001897  \n","Epoch: [1][1300/2059] Elapsed 5m 53s (remain 3m 26s) Loss: 0.4675(0.5724) Grad: 8852.4414  LR: 0.00001879  \n","Epoch: [1][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.6075(0.5708) Grad: 3609.8005  LR: 0.00001861  \n","Epoch: [1][1500/2059] Elapsed 6m 48s (remain 2m 31s) Loss: 0.5989(0.5695) Grad: 6117.3521  LR: 0.00001841  \n","Epoch: [1][1600/2059] Elapsed 7m 15s (remain 2m 4s) Loss: 0.4424(0.5692) Grad: 6836.9961  LR: 0.00001819  \n","Epoch: [1][1700/2059] Elapsed 7m 42s (remain 1m 37s) Loss: 0.5331(0.5702) Grad: 6922.7158  LR: 0.00001797  \n","Epoch: [1][1800/2059] Elapsed 8m 9s (remain 1m 10s) Loss: 0.5641(0.5694) Grad: 5940.2007  LR: 0.00001773  \n","Epoch: [1][1900/2059] Elapsed 8m 36s (remain 0m 42s) Loss: 0.5475(0.5682) Grad: 3362.7891  LR: 0.00001748  \n","Epoch: [1][2000/2059] Elapsed 9m 3s (remain 0m 15s) Loss: 0.5164(0.5673) Grad: 2385.0781  LR: 0.00001723  \n","Epoch: [1][2058/2059] Elapsed 9m 18s (remain 0m 0s) Loss: 0.4892(0.5667) Grad: 2083.6597  LR: 0.00001707  \n","EVAL: [0/221] Elapsed 0m 0s (remain 2m 18s) Loss: 0.4202(0.4202) \n","EVAL: [100/221] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5833(0.5542) \n","EVAL: [200/221] Elapsed 0m 33s (remain 0m 3s) Loss: 0.5816(0.5575) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5667  avg_val_loss: 0.5602  time: 595s\n","Epoch 1 - Score: 0.7869\n","Epoch 1 - Save Best Score: 0.7869 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [220/221] Elapsed 0m 36s (remain 0m 0s) Loss: 0.3688(0.5602) \n","Epoch: [2][0/2059] Elapsed 0m 0s (remain 28m 24s) Loss: 0.5831(0.5831) Grad: 2747.3318  LR: 0.00001707  \n","Epoch: [2][100/2059] Elapsed 0m 28s (remain 9m 8s) Loss: 0.5408(0.5335) Grad: 1571.5675  LR: 0.00001679  \n","Epoch: [2][200/2059] Elapsed 0m 55s (remain 8m 33s) Loss: 0.5494(0.5366) Grad: 2499.7922  LR: 0.00001651  \n","Epoch: [2][300/2059] Elapsed 1m 22s (remain 8m 4s) Loss: 0.6122(0.5366) Grad: 5344.7563  LR: 0.00001622  \n","Epoch: [2][400/2059] Elapsed 1m 50s (remain 7m 35s) Loss: 0.4354(0.5352) Grad: 1250.2910  LR: 0.00001591  \n","Epoch: [2][500/2059] Elapsed 2m 17s (remain 7m 6s) Loss: 0.4844(0.5316) Grad: 3123.3423  LR: 0.00001560  \n","Epoch: [2][600/2059] Elapsed 2m 44s (remain 6m 38s) Loss: 0.5673(0.5348) Grad: 1113.2417  LR: 0.00001528  \n","Epoch: [2][700/2059] Elapsed 3m 11s (remain 6m 10s) Loss: 0.5443(0.5351) Grad: 2835.5977  LR: 0.00001495  \n","Epoch: [2][800/2059] Elapsed 3m 38s (remain 5m 42s) Loss: 0.6819(0.5343) Grad: 2266.1057  LR: 0.00001462  \n","Epoch: [2][900/2059] Elapsed 4m 5s (remain 5m 15s) Loss: 0.5034(0.5334) Grad: 2966.6680  LR: 0.00001428  \n","Epoch: [2][1000/2059] Elapsed 4m 32s (remain 4m 47s) Loss: 0.5621(0.5329) Grad: 1448.9708  LR: 0.00001393  \n","Epoch: [2][1100/2059] Elapsed 4m 59s (remain 4m 20s) Loss: 0.4421(0.5323) Grad: 1055.7081  LR: 0.00001357  \n","Epoch: [2][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.5900(0.5309) Grad: 4417.3315  LR: 0.00001322  \n","Epoch: [2][1300/2059] Elapsed 5m 53s (remain 3m 25s) Loss: 0.3112(0.5304) Grad: 2593.3840  LR: 0.00001285  \n","Epoch: [2][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.5998(0.5302) Grad: 2022.1245  LR: 0.00001249  \n","Epoch: [2][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.6188(0.5292) Grad: 4524.4399  LR: 0.00001211  \n","Epoch: [2][1600/2059] Elapsed 7m 14s (remain 2m 4s) Loss: 0.4321(0.5285) Grad: 1091.8188  LR: 0.00001174  \n","Epoch: [2][1700/2059] Elapsed 7m 41s (remain 1m 37s) Loss: 0.5168(0.5279) Grad: 1880.4803  LR: 0.00001136  \n","Epoch: [2][1800/2059] Elapsed 8m 8s (remain 1m 10s) Loss: 0.3800(0.5275) Grad: 3989.4326  LR: 0.00001098  \n","Epoch: [2][1900/2059] Elapsed 8m 35s (remain 0m 42s) Loss: 0.5780(0.5268) Grad: 2828.7302  LR: 0.00001060  \n","Epoch: [2][2000/2059] Elapsed 9m 2s (remain 0m 15s) Loss: 0.3870(0.5265) Grad: 6301.5024  LR: 0.00001022  \n","Epoch: [2][2058/2059] Elapsed 9m 18s (remain 0m 0s) Loss: 0.5840(0.5266) Grad: 4470.2036  LR: 0.00001000  \n","EVAL: [0/221] Elapsed 0m 0s (remain 2m 23s) Loss: 0.3999(0.3999) \n","EVAL: [100/221] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5632(0.5430) \n","EVAL: [200/221] Elapsed 0m 33s (remain 0m 3s) Loss: 0.5681(0.5476) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5266  avg_val_loss: 0.5500  time: 595s\n","Epoch 2 - Score: 0.8236\n","Epoch 2 - Save Best Score: 0.8236 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [220/221] Elapsed 0m 36s (remain 0m 0s) Loss: 0.3504(0.5500) \n","Epoch: [3][0/2059] Elapsed 0m 0s (remain 28m 14s) Loss: 0.4816(0.4816) Grad: 4229.1157  LR: 0.00001000  \n","Epoch: [3][100/2059] Elapsed 0m 28s (remain 9m 5s) Loss: 0.5064(0.5128) Grad: 2982.9998  LR: 0.00000962  \n","Epoch: [3][200/2059] Elapsed 0m 55s (remain 8m 33s) Loss: 0.4778(0.5082) Grad: 1783.5828  LR: 0.00000924  \n","Epoch: [3][300/2059] Elapsed 1m 22s (remain 8m 2s) Loss: 0.5400(0.5093) Grad: 2547.6067  LR: 0.00000886  \n","Epoch: [3][400/2059] Elapsed 1m 49s (remain 7m 33s) Loss: 0.4853(0.5083) Grad: 2169.4526  LR: 0.00000848  \n","Epoch: [3][500/2059] Elapsed 2m 16s (remain 7m 5s) Loss: 0.5259(0.5050) Grad: 12099.8711  LR: 0.00000810  \n","Epoch: [3][600/2059] Elapsed 2m 43s (remain 6m 37s) Loss: 0.4897(0.5038) Grad: 3439.7898  LR: 0.00000773  \n","Epoch: [3][700/2059] Elapsed 3m 11s (remain 6m 10s) Loss: 0.5887(0.5040) Grad: 10928.5332  LR: 0.00000736  \n","Epoch: [3][800/2059] Elapsed 3m 38s (remain 5m 42s) Loss: 0.5916(0.5030) Grad: 16970.7559  LR: 0.00000699  \n","Epoch: [3][900/2059] Elapsed 4m 5s (remain 5m 15s) Loss: 0.4735(0.5025) Grad: 1994.7291  LR: 0.00000663  \n","Epoch: [3][1000/2059] Elapsed 4m 32s (remain 4m 47s) Loss: 0.5729(0.5024) Grad: 4143.5127  LR: 0.00000628  \n","Epoch: [3][1100/2059] Elapsed 4m 59s (remain 4m 20s) Loss: 0.5631(0.5032) Grad: 1586.8578  LR: 0.00000592  \n","Epoch: [3][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.4305(0.5035) Grad: 2774.6724  LR: 0.00000558  \n","Epoch: [3][1300/2059] Elapsed 5m 53s (remain 3m 26s) Loss: 0.4632(0.5035) Grad: 6306.1914  LR: 0.00000524  \n","Epoch: [3][1400/2059] Elapsed 6m 21s (remain 2m 58s) Loss: 0.5491(0.5041) Grad: 3802.0791  LR: 0.00000491  \n","Epoch: [3][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.5104(0.5044) Grad: 2064.2410  LR: 0.00000458  \n","Epoch: [3][1600/2059] Elapsed 7m 14s (remain 2m 4s) Loss: 0.5271(0.5045) Grad: 6131.6763  LR: 0.00000427  \n","Epoch: [3][1700/2059] Elapsed 7m 42s (remain 1m 37s) Loss: 0.5229(0.5042) Grad: 8889.6357  LR: 0.00000396  \n","Epoch: [3][1800/2059] Elapsed 8m 9s (remain 1m 10s) Loss: 0.4831(0.5037) Grad: 4940.2129  LR: 0.00000366  \n","Epoch: [3][1900/2059] Elapsed 8m 36s (remain 0m 42s) Loss: 0.4580(0.5033) Grad: 13197.9893  LR: 0.00000337  \n","Epoch: [3][2000/2059] Elapsed 9m 3s (remain 0m 15s) Loss: 0.4079(0.5031) Grad: 6859.9937  LR: 0.00000309  \n","Epoch: [3][2058/2059] Elapsed 9m 19s (remain 0m 0s) Loss: 0.4835(0.5031) Grad: 3624.3293  LR: 0.00000293  \n","EVAL: [0/221] Elapsed 0m 0s (remain 2m 20s) Loss: 0.4040(0.4040) \n","EVAL: [100/221] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5673(0.5457) \n","EVAL: [200/221] Elapsed 0m 33s (remain 0m 3s) Loss: 0.5622(0.5513) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5031  avg_val_loss: 0.5553  time: 596s\n","Epoch 3 - Score: 0.8287\n","Epoch 3 - Save Best Score: 0.8287 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [220/221] Elapsed 0m 36s (remain 0m 0s) Loss: 0.3452(0.5553) \n","Epoch: [4][0/2059] Elapsed 0m 0s (remain 28m 21s) Loss: 0.5726(0.5726) Grad: 4231.6260  LR: 0.00000293  \n","Epoch: [4][100/2059] Elapsed 0m 28s (remain 9m 9s) Loss: 0.5191(0.4977) Grad: 9399.5830  LR: 0.00000266  \n","Epoch: [4][200/2059] Elapsed 0m 55s (remain 8m 34s) Loss: 0.5034(0.4980) Grad: 10486.5322  LR: 0.00000241  \n","Epoch: [4][300/2059] Elapsed 1m 22s (remain 8m 3s) Loss: 0.4161(0.4957) Grad: 4795.1021  LR: 0.00000217  \n","Epoch: [4][400/2059] Elapsed 1m 49s (remain 7m 34s) Loss: 0.4897(0.4977) Grad: 3135.6633  LR: 0.00000194  \n","Epoch: [4][500/2059] Elapsed 2m 16s (remain 7m 5s) Loss: 0.5801(0.4959) Grad: 12985.7891  LR: 0.00000172  \n","Epoch: [4][600/2059] Elapsed 2m 43s (remain 6m 37s) Loss: 0.5444(0.4955) Grad: 3689.7490  LR: 0.00000151  \n","Epoch: [4][700/2059] Elapsed 3m 11s (remain 6m 10s) Loss: 0.4662(0.4949) Grad: 4596.5752  LR: 0.00000131  \n","Epoch: [4][800/2059] Elapsed 3m 38s (remain 5m 42s) Loss: 0.6026(0.4962) Grad: 4283.0557  LR: 0.00000113  \n","Epoch: [4][900/2059] Elapsed 4m 5s (remain 5m 15s) Loss: 0.4863(0.4962) Grad: 1834.6104  LR: 0.00000096  \n","Epoch: [4][1000/2059] Elapsed 4m 32s (remain 4m 47s) Loss: 0.4712(0.4961) Grad: 2702.6680  LR: 0.00000080  \n","Epoch: [4][1100/2059] Elapsed 4m 59s (remain 4m 20s) Loss: 0.3785(0.4958) Grad: 10311.8809  LR: 0.00000066  \n","Epoch: [4][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.5376(0.4956) Grad: 9731.8125  LR: 0.00000053  \n","Epoch: [4][1300/2059] Elapsed 5m 53s (remain 3m 26s) Loss: 0.5566(0.4958) Grad: 2444.7278  LR: 0.00000042  \n","Epoch: [4][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.5764(0.4966) Grad: 2835.1812  LR: 0.00000031  \n","Epoch: [4][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.4067(0.4962) Grad: 3264.3330  LR: 0.00000023  \n","Epoch: [4][1600/2059] Elapsed 7m 15s (remain 2m 4s) Loss: 0.5842(0.4963) Grad: 7381.1455  LR: 0.00000015  \n","Epoch: [4][1700/2059] Elapsed 7m 42s (remain 1m 37s) Loss: 0.6096(0.4958) Grad: 7733.4727  LR: 0.00000009  \n","Epoch: [4][1800/2059] Elapsed 8m 9s (remain 1m 10s) Loss: 0.5418(0.4953) Grad: 7462.8374  LR: 0.00000005  \n","Epoch: [4][1900/2059] Elapsed 8m 36s (remain 0m 42s) Loss: 0.5400(0.4949) Grad: 54565.3516  LR: 0.00000002  \n","Epoch: [4][2000/2059] Elapsed 9m 3s (remain 0m 15s) Loss: 0.4452(0.4948) Grad: 16179.1865  LR: 0.00000000  \n","Epoch: [4][2058/2059] Elapsed 9m 19s (remain 0m 0s) Loss: 0.5185(0.4946) Grad: 6557.1807  LR: 0.00000000  \n","EVAL: [0/221] Elapsed 0m 0s (remain 2m 24s) Loss: 0.4041(0.4041) \n","EVAL: [100/221] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5687(0.5493) \n","EVAL: [200/221] Elapsed 0m 33s (remain 0m 3s) Loss: 0.5626(0.5569) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4946  avg_val_loss: 0.5612  time: 596s\n","Epoch 4 - Score: 0.8278\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [220/221] Elapsed 0m 36s (remain 0m 0s) Loss: 0.3427(0.5612) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 2 result ==========\n","Score: 0.8287\n","========== fold: 3 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2054] Elapsed 0m 0s (remain 26m 14s) Loss: 0.7883(0.7883) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2054] Elapsed 0m 27s (remain 8m 58s) Loss: 0.6801(0.6487) Grad: 34460.2578  LR: 0.00001999  \n","Epoch: [1][200/2054] Elapsed 0m 55s (remain 8m 27s) Loss: 0.6260(0.6186) Grad: 85682.0078  LR: 0.00001997  \n","Epoch: [1][300/2054] Elapsed 1m 22s (remain 7m 58s) Loss: 0.5677(0.6022) Grad: 33566.2656  LR: 0.00001993  \n","Epoch: [1][400/2054] Elapsed 1m 49s (remain 7m 30s) Loss: 0.5713(0.5943) Grad: 38258.8008  LR: 0.00001988  \n","Epoch: [1][500/2054] Elapsed 2m 16s (remain 7m 3s) Loss: 0.5826(0.5856) Grad: 88474.3203  LR: 0.00001982  \n","Epoch: [1][600/2054] Elapsed 2m 43s (remain 6m 35s) Loss: 0.4249(0.5801) Grad: 43529.6484  LR: 0.00001974  \n","Epoch: [1][700/2054] Elapsed 3m 10s (remain 6m 8s) Loss: 0.5201(0.5764) Grad: 31838.4355  LR: 0.00001964  \n","Epoch: [1][800/2054] Elapsed 3m 37s (remain 5m 40s) Loss: 0.5850(0.5733) Grad: 120763.5625  LR: 0.00001953  \n","Epoch: [1][900/2054] Elapsed 4m 4s (remain 5m 13s) Loss: 0.5071(0.5710) Grad: 13917.1172  LR: 0.00001941  \n","Epoch: [1][1000/2054] Elapsed 4m 32s (remain 4m 46s) Loss: 0.5761(0.5692) Grad: 19691.5527  LR: 0.00001928  \n","Epoch: [1][1100/2054] Elapsed 4m 59s (remain 4m 19s) Loss: 0.4532(0.5662) Grad: 21873.4785  LR: 0.00001913  \n","Epoch: [1][1200/2054] Elapsed 5m 26s (remain 3m 52s) Loss: 0.5006(0.5650) Grad: 56587.8867  LR: 0.00001896  \n","Epoch: [1][1300/2054] Elapsed 5m 53s (remain 3m 24s) Loss: 0.5468(0.5635) Grad: 23731.2949  LR: 0.00001879  \n","Epoch: [1][1400/2054] Elapsed 6m 21s (remain 2m 57s) Loss: 0.4996(0.5626) Grad: 14465.9766  LR: 0.00001860  \n","Epoch: [1][1500/2054] Elapsed 6m 48s (remain 2m 30s) Loss: 0.4748(0.5618) Grad: 20680.1035  LR: 0.00001840  \n","Epoch: [1][1600/2054] Elapsed 7m 15s (remain 2m 3s) Loss: 0.4847(0.5602) Grad: 18175.8242  LR: 0.00001818  \n","Epoch: [1][1700/2054] Elapsed 7m 42s (remain 1m 35s) Loss: 0.5845(0.5587) Grad: 22915.9648  LR: 0.00001796  \n","Epoch: [1][1800/2054] Elapsed 8m 9s (remain 1m 8s) Loss: 0.3983(0.5581) Grad: 18805.5449  LR: 0.00001772  \n","Epoch: [1][1900/2054] Elapsed 8m 36s (remain 0m 41s) Loss: 0.5206(0.5575) Grad: 46928.1289  LR: 0.00001747  \n","Epoch: [1][2000/2054] Elapsed 9m 3s (remain 0m 14s) Loss: 0.7019(0.5569) Grad: 70696.4688  LR: 0.00001721  \n","Epoch: [1][2053/2054] Elapsed 9m 17s (remain 0m 0s) Loss: 0.5430(0.5566) Grad: 82147.3594  LR: 0.00001707  \n","EVAL: [0/226] Elapsed 0m 0s (remain 2m 24s) Loss: 0.6071(0.6071) \n","EVAL: [100/226] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5571(0.5646) \n","EVAL: [200/226] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5556(0.5550) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5566  avg_val_loss: 0.5535  time: 595s\n","Epoch 1 - Score: 0.8173\n","Epoch 1 - Save Best Score: 0.8173 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [225/226] Elapsed 0m 37s (remain 0m 0s) Loss: 0.8297(0.5535) \n","Epoch: [2][0/2054] Elapsed 0m 0s (remain 27m 25s) Loss: 0.6001(0.6001) Grad: 17602.7441  LR: 0.00001707  \n","Epoch: [2][100/2054] Elapsed 0m 28s (remain 9m 8s) Loss: 0.5795(0.5131) Grad: 79802.8125  LR: 0.00001679  \n","Epoch: [2][200/2054] Elapsed 0m 55s (remain 8m 34s) Loss: 0.4318(0.5151) Grad: 34411.3750  LR: 0.00001651  \n","Epoch: [2][300/2054] Elapsed 1m 23s (remain 8m 4s) Loss: 0.5772(0.5118) Grad: 23729.1270  LR: 0.00001621  \n","Epoch: [2][400/2054] Elapsed 1m 50s (remain 7m 34s) Loss: 0.5914(0.5129) Grad: 35345.5273  LR: 0.00001591  \n","Epoch: [2][500/2054] Elapsed 2m 17s (remain 7m 5s) Loss: 0.5015(0.5138) Grad: 33959.0859  LR: 0.00001560  \n","Epoch: [2][600/2054] Elapsed 2m 44s (remain 6m 37s) Loss: 0.4454(0.5145) Grad: 53845.9727  LR: 0.00001528  \n","Epoch: [2][700/2054] Elapsed 3m 11s (remain 6m 9s) Loss: 0.4234(0.5160) Grad: 25386.0645  LR: 0.00001495  \n","Epoch: [2][800/2054] Elapsed 3m 38s (remain 5m 41s) Loss: 0.5889(0.5161) Grad: 33662.0156  LR: 0.00001461  \n","Epoch: [2][900/2054] Elapsed 4m 5s (remain 5m 14s) Loss: 0.5549(0.5157) Grad: 16843.9688  LR: 0.00001427  \n","Epoch: [2][1000/2054] Elapsed 4m 32s (remain 4m 46s) Loss: 0.5201(0.5150) Grad: 32028.4805  LR: 0.00001392  \n","Epoch: [2][1100/2054] Elapsed 4m 59s (remain 4m 19s) Loss: 0.5175(0.5163) Grad: 62961.4961  LR: 0.00001357  \n","Epoch: [2][1200/2054] Elapsed 5m 26s (remain 3m 52s) Loss: 0.4759(0.5160) Grad: 129016.2188  LR: 0.00001321  \n","Epoch: [2][1300/2054] Elapsed 5m 53s (remain 3m 24s) Loss: 0.6348(0.5161) Grad: 24141.5352  LR: 0.00001284  \n","Epoch: [2][1400/2054] Elapsed 6m 20s (remain 2m 57s) Loss: 0.3649(0.5162) Grad: 24106.1680  LR: 0.00001247  \n","Epoch: [2][1500/2054] Elapsed 6m 47s (remain 2m 30s) Loss: 0.5160(0.5159) Grad: 35868.7422  LR: 0.00001210  \n","Epoch: [2][1600/2054] Elapsed 7m 15s (remain 2m 3s) Loss: 0.5983(0.5162) Grad: 120185.9844  LR: 0.00001173  \n","Epoch: [2][1700/2054] Elapsed 7m 42s (remain 1m 35s) Loss: 0.4740(0.5169) Grad: 27200.0918  LR: 0.00001135  \n","Epoch: [2][1800/2054] Elapsed 8m 9s (remain 1m 8s) Loss: 0.4291(0.5169) Grad: 35903.6289  LR: 0.00001097  \n","Epoch: [2][1900/2054] Elapsed 8m 36s (remain 0m 41s) Loss: 0.4678(0.5170) Grad: 60667.4883  LR: 0.00001059  \n","Epoch: [2][2000/2054] Elapsed 9m 3s (remain 0m 14s) Loss: 0.5243(0.5176) Grad: 42800.3164  LR: 0.00001020  \n","Epoch: [2][2053/2054] Elapsed 9m 17s (remain 0m 0s) Loss: 0.4032(0.5175) Grad: 78241.4219  LR: 0.00001000  \n","EVAL: [0/226] Elapsed 0m 0s (remain 2m 27s) Loss: 0.6036(0.6036) \n","EVAL: [100/226] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5513(0.5654) \n","EVAL: [200/226] Elapsed 0m 33s (remain 0m 4s) Loss: 0.6046(0.5530) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5175  avg_val_loss: 0.5513  time: 595s\n","Epoch 2 - Score: 0.8225\n","Epoch 2 - Save Best Score: 0.8225 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [225/226] Elapsed 0m 37s (remain 0m 0s) Loss: 0.9233(0.5513) \n","Epoch: [3][0/2054] Elapsed 0m 0s (remain 27m 54s) Loss: 0.5869(0.5869) Grad: 63838.2734  LR: 0.00001000  \n","Epoch: [3][100/2054] Elapsed 0m 28s (remain 9m 5s) Loss: 0.5796(0.4939) Grad: 40123.8125  LR: 0.00000962  \n","Epoch: [3][200/2054] Elapsed 0m 55s (remain 8m 33s) Loss: 0.5266(0.5044) Grad: 62565.5117  LR: 0.00000923  \n","Epoch: [3][300/2054] Elapsed 1m 22s (remain 8m 1s) Loss: 0.4592(0.5018) Grad: 69178.8672  LR: 0.00000885  \n","Epoch: [3][400/2054] Elapsed 1m 49s (remain 7m 32s) Loss: 0.4414(0.5006) Grad: 74067.0625  LR: 0.00000847  \n","Epoch: [3][500/2054] Elapsed 2m 17s (remain 7m 4s) Loss: 0.4995(0.5017) Grad: 244286.0938  LR: 0.00000810  \n","Epoch: [3][600/2054] Elapsed 2m 44s (remain 6m 37s) Loss: 0.4880(0.5019) Grad: 48335.2031  LR: 0.00000772  \n","Epoch: [3][700/2054] Elapsed 3m 11s (remain 6m 9s) Loss: 0.5614(0.5031) Grad: 42982.0820  LR: 0.00000735  \n","Epoch: [3][800/2054] Elapsed 3m 38s (remain 5m 42s) Loss: 0.4009(0.5022) Grad: 30682.7578  LR: 0.00000699  \n","Epoch: [3][900/2054] Elapsed 4m 5s (remain 5m 14s) Loss: 0.4816(0.5028) Grad: 45498.3828  LR: 0.00000662  \n","Epoch: [3][1000/2054] Elapsed 4m 32s (remain 4m 46s) Loss: 0.5295(0.5031) Grad: 8359.8936  LR: 0.00000627  \n","Epoch: [3][1100/2054] Elapsed 4m 59s (remain 4m 19s) Loss: 0.5501(0.5033) Grad: 9904.9570  LR: 0.00000592  \n","Epoch: [3][1200/2054] Elapsed 5m 27s (remain 3m 52s) Loss: 0.5936(0.5031) Grad: 11591.8291  LR: 0.00000557  \n","Epoch: [3][1300/2054] Elapsed 5m 54s (remain 3m 25s) Loss: 0.5664(0.5036) Grad: 34832.1914  LR: 0.00000523  \n","Epoch: [3][1400/2054] Elapsed 6m 21s (remain 2m 57s) Loss: 0.6029(0.5038) Grad: 14889.7568  LR: 0.00000490  \n","Epoch: [3][1500/2054] Elapsed 6m 48s (remain 2m 30s) Loss: 0.3849(0.5032) Grad: 11603.2227  LR: 0.00000457  \n","Epoch: [3][1600/2054] Elapsed 7m 15s (remain 2m 3s) Loss: 0.5045(0.5032) Grad: 8583.0049  LR: 0.00000426  \n","Epoch: [3][1700/2054] Elapsed 7m 42s (remain 1m 35s) Loss: 0.4468(0.5032) Grad: 14220.8662  LR: 0.00000395  \n","Epoch: [3][1800/2054] Elapsed 8m 9s (remain 1m 8s) Loss: 0.6239(0.5032) Grad: 13068.2705  LR: 0.00000365  \n","Epoch: [3][1900/2054] Elapsed 8m 36s (remain 0m 41s) Loss: 0.4130(0.5034) Grad: 6012.5747  LR: 0.00000336  \n","Epoch: [3][2000/2054] Elapsed 9m 4s (remain 0m 14s) Loss: 0.5675(0.5031) Grad: 14006.2471  LR: 0.00000308  \n","Epoch: [3][2053/2054] Elapsed 9m 18s (remain 0m 0s) Loss: 0.4083(0.5032) Grad: 13911.3516  LR: 0.00000293  \n","EVAL: [0/226] Elapsed 0m 0s (remain 2m 28s) Loss: 0.5940(0.5940) \n","EVAL: [100/226] Elapsed 0m 16s (remain 0m 20s) Loss: 0.5443(0.5836) \n","EVAL: [200/226] Elapsed 0m 33s (remain 0m 4s) Loss: 0.6107(0.5756) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5032  avg_val_loss: 0.5742  time: 596s\n","Epoch 3 - Score: 0.8205\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [225/226] Elapsed 0m 37s (remain 0m 0s) Loss: 1.2339(0.5742) \n","Epoch: [4][0/2054] Elapsed 0m 0s (remain 27m 6s) Loss: 0.5703(0.5703) Grad: 11371.4082  LR: 0.00000293  \n","Epoch: [4][100/2054] Elapsed 0m 28s (remain 9m 2s) Loss: 0.4504(0.4943) Grad: 39552.9766  LR: 0.00000266  \n","Epoch: [4][200/2054] Elapsed 0m 55s (remain 8m 29s) Loss: 0.5934(0.5008) Grad: 10516.0381  LR: 0.00000241  \n","Epoch: [4][300/2054] Elapsed 1m 22s (remain 8m 0s) Loss: 0.4448(0.4984) Grad: 7195.9634  LR: 0.00000217  \n","Epoch: [4][400/2054] Elapsed 1m 49s (remain 7m 31s) Loss: 0.5098(0.5006) Grad: 11016.5615  LR: 0.00000193  \n","Epoch: [4][500/2054] Elapsed 2m 16s (remain 7m 4s) Loss: 0.4177(0.5028) Grad: 11349.5459  LR: 0.00000171  \n","Epoch: [4][600/2054] Elapsed 2m 43s (remain 6m 36s) Loss: 0.4971(0.5001) Grad: 8810.7764  LR: 0.00000151  \n","Epoch: [4][700/2054] Elapsed 3m 11s (remain 6m 9s) Loss: 0.4880(0.4994) Grad: 8316.3330  LR: 0.00000131  \n","Epoch: [4][800/2054] Elapsed 3m 38s (remain 5m 41s) Loss: 0.4639(0.4977) Grad: 26025.0469  LR: 0.00000113  \n","Epoch: [4][900/2054] Elapsed 4m 5s (remain 5m 14s) Loss: 0.3787(0.4982) Grad: 15299.2578  LR: 0.00000096  \n","Epoch: [4][1000/2054] Elapsed 4m 32s (remain 4m 47s) Loss: 0.5848(0.4990) Grad: 24145.6973  LR: 0.00000080  \n","Epoch: [4][1100/2054] Elapsed 5m 0s (remain 4m 20s) Loss: 0.5126(0.4990) Grad: 16853.9512  LR: 0.00000066  \n","Epoch: [4][1200/2054] Elapsed 5m 27s (remain 3m 52s) Loss: 0.4175(0.4991) Grad: 20638.6094  LR: 0.00000053  \n","Epoch: [4][1300/2054] Elapsed 5m 54s (remain 3m 25s) Loss: 0.5534(0.4991) Grad: 106350.5625  LR: 0.00000041  \n","Epoch: [4][1400/2054] Elapsed 6m 21s (remain 2m 58s) Loss: 0.4963(0.4979) Grad: 59148.3008  LR: 0.00000031  \n","Epoch: [4][1500/2054] Elapsed 6m 49s (remain 2m 30s) Loss: 0.4771(0.4975) Grad: 29016.8008  LR: 0.00000022  \n","Epoch: [4][1600/2054] Elapsed 7m 16s (remain 2m 3s) Loss: 0.6445(0.4965) Grad: 15388.2598  LR: 0.00000015  \n","Epoch: [4][1700/2054] Elapsed 7m 43s (remain 1m 36s) Loss: 0.7129(0.4963) Grad: 111483.6719  LR: 0.00000009  \n","Epoch: [4][1800/2054] Elapsed 8m 10s (remain 1m 8s) Loss: 0.5198(0.4965) Grad: 17583.3691  LR: 0.00000005  \n","Epoch: [4][1900/2054] Elapsed 8m 38s (remain 0m 41s) Loss: 0.5562(0.4957) Grad: 26456.7188  LR: 0.00000002  \n","Epoch: [4][2000/2054] Elapsed 9m 5s (remain 0m 14s) Loss: 0.4932(0.4957) Grad: 11030.7451  LR: 0.00000000  \n","Epoch: [4][2053/2054] Elapsed 9m 19s (remain 0m 0s) Loss: 0.5197(0.4956) Grad: 35583.6797  LR: 0.00000000  \n","EVAL: [0/226] Elapsed 0m 0s (remain 2m 29s) Loss: 0.5962(0.5962) \n","EVAL: [100/226] Elapsed 0m 16s (remain 0m 21s) Loss: 0.5477(0.5880) \n","EVAL: [200/226] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5543(0.5796) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4 - avg_train_loss: 0.4956  avg_val_loss: 0.5782  time: 597s\n","Epoch 4 - Score: 0.8187\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [225/226] Elapsed 0m 37s (remain 0m 0s) Loss: 1.2411(0.5782) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["========== fold: 3 result ==========\n","Score: 0.8225\n","========== fold: 4 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: [1][0/2035] Elapsed 0m 0s (remain 25m 25s) Loss: 0.7740(0.7740) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2035] Elapsed 0m 28s (remain 8m 58s) Loss: 0.5925(0.6337) Grad: 60422.3516  LR: 0.00001999  \n","Epoch: [1][200/2035] Elapsed 0m 55s (remain 8m 23s) Loss: 0.6537(0.6151) Grad: 93121.2188  LR: 0.00001997  \n","Epoch: [1][300/2035] Elapsed 1m 22s (remain 7m 53s) Loss: 0.4811(0.5965) Grad: 111837.4688  LR: 0.00001993  \n","Epoch: [1][400/2035] Elapsed 1m 49s (remain 7m 25s) Loss: 0.6018(0.5881) Grad: 46347.1211  LR: 0.00001988  \n","Epoch: [1][500/2035] Elapsed 2m 16s (remain 6m 58s) Loss: 0.5194(0.5836) Grad: 28266.4922  LR: 0.00001981  \n","Epoch: [1][600/2035] Elapsed 2m 43s (remain 6m 30s) Loss: 0.6972(0.5833) Grad: 103470.5156  LR: 0.00001973  \n","Epoch: [1][700/2035] Elapsed 3m 10s (remain 6m 2s) Loss: 0.4987(0.5796) Grad: 16002.2227  LR: 0.00001964  \n","Epoch: [1][800/2035] Elapsed 3m 37s (remain 5m 35s) Loss: 0.5929(0.5765) Grad: 20681.0488  LR: 0.00001953  \n","Epoch: [1][900/2035] Elapsed 4m 4s (remain 5m 8s) Loss: 0.5990(0.5747) Grad: 29132.5371  LR: 0.00001940  \n","Epoch: [1][1000/2035] Elapsed 4m 31s (remain 4m 40s) Loss: 0.5108(0.5728) Grad: 19306.9199  LR: 0.00001926  \n","Epoch: [1][1100/2035] Elapsed 4m 59s (remain 4m 13s) Loss: 0.5864(0.5711) Grad: 66003.1719  LR: 0.00001911  \n","Epoch: [1][1200/2035] Elapsed 5m 26s (remain 3m 46s) Loss: 0.4792(0.5689) Grad: 14947.6514  LR: 0.00001895  \n","Epoch: [1][1300/2035] Elapsed 5m 53s (remain 3m 19s) Loss: 0.5402(0.5662) Grad: 42806.1016  LR: 0.00001877  \n","Epoch: [1][1400/2035] Elapsed 6m 20s (remain 2m 52s) Loss: 0.6113(0.5648) Grad: 23728.9141  LR: 0.00001857  \n","Epoch: [1][1500/2035] Elapsed 6m 47s (remain 2m 25s) Loss: 0.5174(0.5637) Grad: 35603.8828  LR: 0.00001837  \n","Epoch: [1][1600/2035] Elapsed 7m 14s (remain 1m 57s) Loss: 0.6421(0.5628) Grad: 30643.5742  LR: 0.00001815  \n","Epoch: [1][1700/2035] Elapsed 7m 42s (remain 1m 30s) Loss: 0.3403(0.5617) Grad: 13425.9521  LR: 0.00001792  \n","Epoch: [1][1800/2035] Elapsed 8m 9s (remain 1m 3s) Loss: 0.6576(0.5606) Grad: 56436.4102  LR: 0.00001768  \n","Epoch: [1][1900/2035] Elapsed 8m 36s (remain 0m 36s) Loss: 0.3972(0.5592) Grad: 87524.2578  LR: 0.00001743  \n","Epoch: [1][2000/2035] Elapsed 9m 3s (remain 0m 9s) Loss: 0.5137(0.5582) Grad: 58479.0508  LR: 0.00001716  \n","Epoch: [1][2034/2035] Elapsed 9m 12s (remain 0m 0s) Loss: 0.5022(0.5580) Grad: 23822.5820  LR: 0.00001707  \n","EVAL: [0/244] Elapsed 0m 0s (remain 2m 32s) Loss: 0.6646(0.6646) \n","EVAL: [100/244] Elapsed 0m 16s (remain 0m 23s) Loss: 0.5842(0.5447) \n","EVAL: [200/244] Elapsed 0m 33s (remain 0m 7s) Loss: 0.5220(0.5415) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1 - avg_train_loss: 0.5580  avg_val_loss: 0.5442  time: 593s\n","Epoch 1 - Score: 0.8202\n","Epoch 1 - Save Best Score: 0.8202 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [243/244] Elapsed 0m 40s (remain 0m 0s) Loss: 0.5070(0.5442) \n","Epoch: [2][0/2035] Elapsed 0m 0s (remain 29m 11s) Loss: 0.5844(0.5844) Grad: 14885.9463  LR: 0.00001707  \n","Epoch: [2][100/2035] Elapsed 0m 28s (remain 9m 8s) Loss: 0.5628(0.5340) Grad: 37825.8398  LR: 0.00001679  \n","Epoch: [2][200/2035] Elapsed 0m 56s (remain 8m 32s) Loss: 0.5175(0.5316) Grad: 53598.0000  LR: 0.00001650  \n","Epoch: [2][300/2035] Elapsed 1m 23s (remain 7m 59s) Loss: 0.4927(0.5288) Grad: 12282.5430  LR: 0.00001621  \n","Epoch: [2][400/2035] Elapsed 1m 50s (remain 7m 31s) Loss: 0.5160(0.5284) Grad: 5395.0015  LR: 0.00001590  \n","Epoch: [2][500/2035] Elapsed 2m 18s (remain 7m 2s) Loss: 0.5660(0.5273) Grad: 27018.5117  LR: 0.00001558  \n","Epoch: [2][600/2035] Elapsed 2m 45s (remain 6m 34s) Loss: 0.4160(0.5251) Grad: 16607.6934  LR: 0.00001526  \n","Epoch: [2][700/2035] Elapsed 3m 12s (remain 6m 5s) Loss: 0.4712(0.5247) Grad: 58101.6133  LR: 0.00001493  \n","Epoch: [2][800/2035] Elapsed 3m 39s (remain 5m 37s) Loss: 0.5911(0.5238) Grad: 229029.6406  LR: 0.00001459  \n","Epoch: [2][900/2035] Elapsed 4m 6s (remain 5m 9s) Loss: 0.5287(0.5223) Grad: 30193.1484  LR: 0.00001424  \n","Epoch: [2][1000/2035] Elapsed 4m 33s (remain 4m 42s) Loss: 0.5650(0.5219) Grad: 90715.4531  LR: 0.00001389  \n","Epoch: [2][1100/2035] Elapsed 5m 0s (remain 4m 14s) Loss: 0.5949(0.5216) Grad: 48852.2930  LR: 0.00001353  \n","Epoch: [2][1200/2035] Elapsed 5m 27s (remain 3m 47s) Loss: 0.4967(0.5213) Grad: 54428.0312  LR: 0.00001317  \n","Epoch: [2][1300/2035] Elapsed 5m 54s (remain 3m 19s) Loss: 0.6044(0.5215) Grad: 131657.1562  LR: 0.00001280  \n","Epoch: [2][1400/2035] Elapsed 6m 21s (remain 2m 52s) Loss: 0.5887(0.5209) Grad: 70265.1875  LR: 0.00001243  \n","Epoch: [2][1500/2035] Elapsed 6m 48s (remain 2m 25s) Loss: 0.5197(0.5203) Grad: 38989.3008  LR: 0.00001205  \n","Epoch: [2][1600/2035] Elapsed 7m 15s (remain 1m 58s) Loss: 0.4366(0.5197) Grad: 11594.6826  LR: 0.00001167  \n","Epoch: [2][1700/2035] Elapsed 7m 42s (remain 1m 30s) Loss: 0.4415(0.5188) Grad: 11715.6729  LR: 0.00001129  \n","Epoch: [2][1800/2035] Elapsed 8m 9s (remain 1m 3s) Loss: 0.5177(0.5195) Grad: 10242.6973  LR: 0.00001091  \n","Epoch: [2][1900/2035] Elapsed 8m 36s (remain 0m 36s) Loss: 0.5191(0.5202) Grad: 32135.2148  LR: 0.00001052  \n","Epoch: [2][2000/2035] Elapsed 9m 4s (remain 0m 9s) Loss: 0.5795(0.5203) Grad: 18527.5098  LR: 0.00001014  \n","Epoch: [2][2034/2035] Elapsed 9m 13s (remain 0m 0s) Loss: 0.5295(0.5204) Grad: 53747.7734  LR: 0.00001000  \n","EVAL: [0/244] Elapsed 0m 0s (remain 2m 36s) Loss: 0.7237(0.7237) \n","EVAL: [100/244] Elapsed 0m 16s (remain 0m 23s) Loss: 0.5816(0.5565) \n","EVAL: [200/244] Elapsed 0m 33s (remain 0m 7s) Loss: 0.5106(0.5492) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2 - avg_train_loss: 0.5204  avg_val_loss: 0.5523  time: 594s\n","Epoch 2 - Score: 0.8239\n","Epoch 2 - Save Best Score: 0.8239 Model\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["EVAL: [243/244] Elapsed 0m 40s (remain 0m 0s) Loss: 0.4909(0.5523) \n","Epoch: [3][0/2035] Elapsed 0m 0s (remain 27m 54s) Loss: 0.5407(0.5407) Grad: 13762.4160  LR: 0.00001000  \n","Epoch: [3][100/2035] Elapsed 0m 28s (remain 9m 1s) Loss: 0.6524(0.4996) Grad: 5988.6187  LR: 0.00000961  \n","Epoch: [3][200/2035] Elapsed 0m 55s (remain 8m 29s) Loss: 0.5516(0.5076) Grad: 21893.4219  LR: 0.00000923  \n","Epoch: [3][300/2035] Elapsed 1m 22s (remain 7m 58s) Loss: 0.6681(0.5062) Grad: 12247.6211  LR: 0.00000885  \n","Epoch: [3][400/2035] Elapsed 1m 50s (remain 7m 28s) Loss: 0.4696(0.5040) Grad: 18792.2715  LR: 0.00000846  \n","Epoch: [3][500/2035] Elapsed 2m 17s (remain 6m 59s) Loss: 0.3550(0.5043) Grad: 4873.8667  LR: 0.00000808  \n","Epoch: [3][600/2035] Elapsed 2m 44s (remain 6m 31s) Loss: 0.4789(0.5056) Grad: 17063.8770  LR: 0.00000771  \n","Epoch: [3][700/2035] Elapsed 3m 11s (remain 6m 4s) Loss: 0.5920(0.5063) Grad: 24876.3730  LR: 0.00000733  \n","Epoch: [3][800/2035] Elapsed 3m 38s (remain 5m 37s) Loss: 0.4945(0.5072) Grad: 7556.3135  LR: 0.00000696  \n","Epoch: [3][900/2035] Elapsed 4m 5s (remain 5m 9s) Loss: 0.3649(0.5060) Grad: 33733.6406  LR: 0.00000660  \n","Epoch: [3][1000/2035] Elapsed 4m 33s (remain 4m 42s) Loss: 0.3827(0.5070) Grad: 22297.8418  LR: 0.00000624  \n","Epoch: [3][1100/2035] Elapsed 5m 0s (remain 4m 14s) Loss: 0.6268(0.5085) Grad: 11722.6279  LR: 0.00000588  \n","Epoch: [3][1200/2035] Elapsed 5m 27s (remain 3m 47s) Loss: 0.5481(0.5085) Grad: 20071.0859  LR: 0.00000553  \n","Epoch: [3][1300/2035] Elapsed 5m 54s (remain 3m 19s) Loss: 0.4417(0.5071) Grad: 8772.3789  LR: 0.00000519  \n","Epoch: [3][1400/2035] Elapsed 6m 21s (remain 2m 52s) Loss: 0.4420(0.5069) Grad: 5121.9561  LR: 0.00000486  \n","Epoch: [3][1500/2035] Elapsed 6m 48s (remain 2m 25s) Loss: 0.4658(0.5057) Grad: 17089.1309  LR: 0.00000453  \n","Epoch: [3][1600/2035] Elapsed 7m 15s (remain 1m 58s) Loss: 0.4893(0.5053) Grad: 7006.9053  LR: 0.00000421  \n","Epoch: [3][1700/2035] Elapsed 7m 42s (remain 1m 30s) Loss: 0.4512(0.5054) Grad: 20089.1504  LR: 0.00000390  \n","Epoch: [3][1800/2035] Elapsed 8m 9s (remain 1m 3s) Loss: 0.4855(0.5058) Grad: 9399.0752  LR: 0.00000360  \n","Epoch: [3][1900/2035] Elapsed 8m 36s (remain 0m 36s) Loss: 0.4488(0.5060) Grad: 48339.4609  LR: 0.00000331  \n","Epoch: [3][2000/2035] Elapsed 9m 4s (remain 0m 9s) Loss: 0.4534(0.5056) Grad: 14886.5391  LR: 0.00000303  \n","Epoch: [3][2034/2035] Elapsed 9m 13s (remain 0m 0s) Loss: 0.5624(0.5055) Grad: 9817.3037  LR: 0.00000293  \n","EVAL: [0/244] Elapsed 0m 0s (remain 2m 37s) Loss: 0.7193(0.7193) \n","EVAL: [100/244] Elapsed 0m 16s (remain 0m 23s) Loss: 0.5684(0.5622) \n","EVAL: [200/244] Elapsed 0m 33s (remain 0m 7s) Loss: 0.5042(0.5526) \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3 - avg_train_loss: 0.5055  avg_val_loss: 0.5557  time: 594s\n","Epoch 3 - Score: 0.8321\n","Epoch 3 - Save Best Score: 0.8321 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [243/244] Elapsed 0m 40s (remain 0m 0s) Loss: 0.5017(0.5557) \n","Epoch: [4][0/2035] Elapsed 0m 0s (remain 26m 41s) Loss: 0.5263(0.5263) Grad: 66473.7656  LR: 0.00000293  \n","Epoch: [4][100/2035] Elapsed 0m 28s (remain 8m 59s) Loss: 0.3398(0.4911) Grad: 17571.8867  LR: 0.00000266  \n","Epoch: [4][200/2035] Elapsed 0m 55s (remain 8m 25s) Loss: 0.4068(0.4872) Grad: 10978.5527  LR: 0.00000241  \n","Epoch: [4][300/2035] Elapsed 1m 22s (remain 7m 55s) Loss: 0.5890(0.4895) Grad: 16780.8359  LR: 0.00000216  \n","Epoch: [4][400/2035] Elapsed 1m 49s (remain 7m 26s) Loss: 0.5592(0.4884) Grad: 33384.8125  LR: 0.00000193  \n","Epoch: [4][500/2035] Elapsed 2m 16s (remain 6m 57s) Loss: 0.5177(0.4895) Grad: 53395.2109  LR: 0.00000171  \n","Epoch: [4][600/2035] Elapsed 2m 43s (remain 6m 30s) Loss: 0.5654(0.4905) Grad: 26987.1953  LR: 0.00000150  \n","Epoch: [4][700/2035] Elapsed 3m 10s (remain 6m 2s) Loss: 0.5943(0.4915) Grad: 19879.5098  LR: 0.00000130  \n","Epoch: [4][800/2035] Elapsed 3m 37s (remain 5m 35s) Loss: 0.4999(0.4910) Grad: 21907.4160  LR: 0.00000112  \n","Epoch: [4][900/2035] Elapsed 4m 4s (remain 5m 8s) Loss: 0.5378(0.4923) Grad: 29155.5703  LR: 0.00000095  \n","Epoch: [4][1000/2035] Elapsed 4m 31s (remain 4m 40s) Loss: 0.3311(0.4935) Grad: 44296.1094  LR: 0.00000079  \n","Epoch: [4][1100/2035] Elapsed 4m 59s (remain 4m 13s) Loss: 0.5480(0.4939) Grad: 11480.0049  LR: 0.00000065  \n","Epoch: [4][1200/2035] Elapsed 5m 26s (remain 3m 46s) Loss: 0.4903(0.4946) Grad: 21290.4297  LR: 0.00000052  \n","Epoch: [4][1300/2035] Elapsed 5m 53s (remain 3m 19s) Loss: 0.4158(0.4949) Grad: 11300.9131  LR: 0.00000040  \n","Epoch: [4][1400/2035] Elapsed 6m 20s (remain 2m 52s) Loss: 0.4892(0.4948) Grad: 15593.2354  LR: 0.00000030  \n","Epoch: [4][1500/2035] Elapsed 6m 47s (remain 2m 25s) Loss: 0.5158(0.4942) Grad: 17182.3184  LR: 0.00000021  \n","Epoch: [4][1600/2035] Elapsed 7m 14s (remain 1m 57s) Loss: 0.4758(0.4946) Grad: 21179.0801  LR: 0.00000014  \n","Epoch: [4][1700/2035] Elapsed 7m 41s (remain 1m 30s) Loss: 0.5092(0.4945) Grad: 13508.2676  LR: 0.00000008  \n","Epoch: [4][1800/2035] Elapsed 8m 8s (remain 1m 3s) Loss: 0.6036(0.4944) Grad: 28181.0645  LR: 0.00000004  \n","Epoch: [4][1900/2035] Elapsed 8m 35s (remain 0m 36s) Loss: 0.5850(0.4953) Grad: 17543.3477  LR: 0.00000001  \n","Epoch: [4][2000/2035] Elapsed 9m 2s (remain 0m 9s) Loss: 0.3718(0.4952) Grad: 45247.4531  LR: 0.00000000  \n","Epoch: [4][2034/2035] Elapsed 9m 12s (remain 0m 0s) Loss: 0.5542(0.4952) Grad: 15119.1641  LR: 0.00000000  \n","EVAL: [0/244] Elapsed 0m 0s (remain 2m 39s) Loss: 0.7372(0.7372) \n","EVAL: [100/244] Elapsed 0m 16s (remain 0m 23s) Loss: 0.5854(0.5737) \n","EVAL: [200/244] Elapsed 0m 33s (remain 0m 7s) Loss: 0.5133(0.5651) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4952  avg_val_loss: 0.5674  time: 593s\n","Epoch 4 - Score: 0.8292\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [243/244] Elapsed 0m 40s (remain 0m 0s) Loss: 0.4980(0.5674) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 4 result ==========\n","Score: 0.8321\n","========== fold: 5 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2059] Elapsed 0m 0s (remain 27m 53s) Loss: 0.6219(0.6219) Grad: 205890.7969  LR: 0.00002000  \n","Epoch: [1][100/2059] Elapsed 0m 27s (remain 9m 2s) Loss: 0.6380(0.6521) Grad: 18344.0723  LR: 0.00001999  \n","Epoch: [1][200/2059] Elapsed 0m 55s (remain 8m 29s) Loss: 0.6746(0.6270) Grad: 43759.3672  LR: 0.00001997  \n","Epoch: [1][300/2059] Elapsed 1m 22s (remain 7m 59s) Loss: 0.6076(0.6125) Grad: 46908.8008  LR: 0.00001993  \n","Epoch: [1][400/2059] Elapsed 1m 49s (remain 7m 31s) Loss: 0.6348(0.6001) Grad: 22513.3750  LR: 0.00001988  \n","Epoch: [1][500/2059] Elapsed 2m 16s (remain 7m 3s) Loss: 0.5851(0.5918) Grad: 25596.7090  LR: 0.00001982  \n","Epoch: [1][600/2059] Elapsed 2m 43s (remain 6m 36s) Loss: 0.4872(0.5841) Grad: 24501.7910  LR: 0.00001974  \n","Epoch: [1][700/2059] Elapsed 3m 10s (remain 6m 9s) Loss: 0.5703(0.5804) Grad: 25357.9199  LR: 0.00001964  \n","Epoch: [1][800/2059] Elapsed 3m 37s (remain 5m 41s) Loss: 0.5116(0.5763) Grad: 62234.9805  LR: 0.00001954  \n","Epoch: [1][900/2059] Elapsed 4m 4s (remain 5m 14s) Loss: 0.4196(0.5735) Grad: 58275.8672  LR: 0.00001942  \n","Epoch: [1][1000/2059] Elapsed 4m 31s (remain 4m 47s) Loss: 0.5294(0.5701) Grad: 15856.0674  LR: 0.00001928  \n","Epoch: [1][1100/2059] Elapsed 4m 58s (remain 4m 19s) Loss: 0.4332(0.5689) Grad: 35971.2188  LR: 0.00001913  \n","Epoch: [1][1200/2059] Elapsed 5m 25s (remain 3m 52s) Loss: 0.5591(0.5673) Grad: 24187.3047  LR: 0.00001897  \n","Epoch: [1][1300/2059] Elapsed 5m 53s (remain 3m 25s) Loss: 0.6078(0.5655) Grad: 93136.5781  LR: 0.00001879  \n","Epoch: [1][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.5313(0.5640) Grad: 157713.0312  LR: 0.00001861  \n","Epoch: [1][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.6699(0.5622) Grad: 46594.6914  LR: 0.00001841  \n","Epoch: [1][1600/2059] Elapsed 7m 14s (remain 2m 4s) Loss: 0.6045(0.5615) Grad: 13813.3037  LR: 0.00001819  \n","Epoch: [1][1700/2059] Elapsed 7m 41s (remain 1m 37s) Loss: 0.4671(0.5603) Grad: 18740.2793  LR: 0.00001797  \n","Epoch: [1][1800/2059] Elapsed 8m 8s (remain 1m 10s) Loss: 0.4765(0.5599) Grad: 18674.4297  LR: 0.00001773  \n","Epoch: [1][1900/2059] Elapsed 8m 35s (remain 0m 42s) Loss: 0.5723(0.5592) Grad: 16232.5654  LR: 0.00001749  \n","Epoch: [1][2000/2059] Elapsed 9m 2s (remain 0m 15s) Loss: 0.5585(0.5582) Grad: 18747.9531  LR: 0.00001723  \n","Epoch: [1][2058/2059] Elapsed 9m 18s (remain 0m 0s) Loss: 0.4362(0.5581) Grad: 19393.0879  LR: 0.00001707  \n","EVAL: [0/220] Elapsed 0m 0s (remain 2m 18s) Loss: 0.5696(0.5696) \n","EVAL: [100/220] Elapsed 0m 16s (remain 0m 19s) Loss: 0.5860(0.5500) \n","EVAL: [200/220] Elapsed 0m 33s (remain 0m 3s) Loss: 0.3339(0.5473) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5581  avg_val_loss: 0.5459  time: 595s\n","Epoch 1 - Score: 0.8088\n","Epoch 1 - Save Best Score: 0.8088 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [219/220] Elapsed 0m 36s (remain 0m 0s) Loss: 0.4587(0.5459) \n","Epoch: [2][0/2059] Elapsed 0m 0s (remain 27m 1s) Loss: 0.5646(0.5646) Grad: 14856.2217  LR: 0.00001707  \n","Epoch: [2][100/2059] Elapsed 0m 28s (remain 9m 9s) Loss: 0.4986(0.5196) Grad: 76635.8906  LR: 0.00001680  \n","Epoch: [2][200/2059] Elapsed 0m 55s (remain 8m 33s) Loss: 0.5989(0.5196) Grad: 25715.5879  LR: 0.00001651  \n","Epoch: [2][300/2059] Elapsed 1m 22s (remain 8m 4s) Loss: 0.4949(0.5164) Grad: 32227.8965  LR: 0.00001622  \n","Epoch: [2][400/2059] Elapsed 1m 50s (remain 7m 34s) Loss: 0.6636(0.5152) Grad: 12351.7500  LR: 0.00001591  \n","Epoch: [2][500/2059] Elapsed 2m 17s (remain 7m 6s) Loss: 0.5787(0.5162) Grad: 38612.3867  LR: 0.00001560  \n","Epoch: [2][600/2059] Elapsed 2m 44s (remain 6m 38s) Loss: 0.5274(0.5161) Grad: 73034.0703  LR: 0.00001528  \n","Epoch: [2][700/2059] Elapsed 3m 11s (remain 6m 10s) Loss: 0.4717(0.5145) Grad: 31289.0527  LR: 0.00001495  \n","Epoch: [2][800/2059] Elapsed 3m 37s (remain 5m 42s) Loss: 0.4390(0.5152) Grad: 36316.0586  LR: 0.00001462  \n","Epoch: [2][900/2059] Elapsed 4m 4s (remain 5m 14s) Loss: 0.5091(0.5167) Grad: 24691.6387  LR: 0.00001428  \n","Epoch: [2][1000/2059] Elapsed 4m 32s (remain 4m 47s) Loss: 0.6286(0.5166) Grad: 32721.8633  LR: 0.00001393  \n","Epoch: [2][1100/2059] Elapsed 4m 59s (remain 4m 20s) Loss: 0.5677(0.5165) Grad: 15918.5908  LR: 0.00001358  \n","Epoch: [2][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.4875(0.5161) Grad: 55831.2422  LR: 0.00001322  \n","Epoch: [2][1300/2059] Elapsed 5m 53s (remain 3m 26s) Loss: 0.4544(0.5161) Grad: 59150.1055  LR: 0.00001286  \n","Epoch: [2][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.4775(0.5158) Grad: 25342.3730  LR: 0.00001249  \n","Epoch: [2][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.5114(0.5161) Grad: 103111.7578  LR: 0.00001212  \n","Epoch: [2][1600/2059] Elapsed 7m 15s (remain 2m 4s) Loss: 0.4950(0.5164) Grad: 50622.2227  LR: 0.00001174  \n","Epoch: [2][1700/2059] Elapsed 7m 41s (remain 1m 37s) Loss: 0.4560(0.5163) Grad: 42032.9297  LR: 0.00001137  \n","Epoch: [2][1800/2059] Elapsed 8m 9s (remain 1m 10s) Loss: 0.5183(0.5168) Grad: 24822.3926  LR: 0.00001099  \n","Epoch: [2][1900/2059] Elapsed 8m 36s (remain 0m 42s) Loss: 0.5338(0.5168) Grad: 33850.1680  LR: 0.00001061  \n","Epoch: [2][2000/2059] Elapsed 9m 3s (remain 0m 15s) Loss: 0.4005(0.5167) Grad: 22663.9961  LR: 0.00001023  \n","Epoch: [2][2058/2059] Elapsed 9m 18s (remain 0m 0s) Loss: 0.6676(0.5170) Grad: 101706.2812  LR: 0.00001001  \n","EVAL: [0/220] Elapsed 0m 0s (remain 2m 20s) Loss: 0.5736(0.5736) \n","EVAL: [100/220] Elapsed 0m 16s (remain 0m 19s) Loss: 0.6183(0.5487) \n","EVAL: [200/220] Elapsed 0m 33s (remain 0m 3s) Loss: 0.3312(0.5496) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.5170  avg_val_loss: 0.5484  time: 595s\n","Epoch 2 - Score: 0.8211\n","Epoch 2 - Save Best Score: 0.8211 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [219/220] Elapsed 0m 36s (remain 0m 0s) Loss: 0.4375(0.5484) \n","Epoch: [3][0/2059] Elapsed 0m 0s (remain 28m 24s) Loss: 0.4838(0.4838) Grad: 22284.4004  LR: 0.00001000  \n","Epoch: [3][100/2059] Elapsed 0m 28s (remain 9m 16s) Loss: 0.4197(0.5039) Grad: 36920.8594  LR: 0.00000962  \n","Epoch: [3][200/2059] Elapsed 0m 55s (remain 8m 37s) Loss: 0.3953(0.5029) Grad: 5758.4736  LR: 0.00000924  \n","Epoch: [3][300/2059] Elapsed 1m 22s (remain 8m 4s) Loss: 0.5356(0.5016) Grad: 16633.9277  LR: 0.00000886  \n","Epoch: [3][400/2059] Elapsed 1m 49s (remain 7m 34s) Loss: 0.5380(0.5017) Grad: 25410.4727  LR: 0.00000848  \n","Epoch: [3][500/2059] Elapsed 2m 16s (remain 7m 5s) Loss: 0.6196(0.5046) Grad: 15143.0557  LR: 0.00000811  \n","Epoch: [3][600/2059] Elapsed 2m 43s (remain 6m 37s) Loss: 0.5565(0.5043) Grad: 13005.9199  LR: 0.00000773  \n","Epoch: [3][700/2059] Elapsed 3m 10s (remain 6m 9s) Loss: 0.4906(0.5049) Grad: 120281.9922  LR: 0.00000736  \n","Epoch: [3][800/2059] Elapsed 3m 38s (remain 5m 42s) Loss: 0.5231(0.5043) Grad: 15214.1650  LR: 0.00000700  \n","Epoch: [3][900/2059] Elapsed 4m 4s (remain 5m 14s) Loss: 0.5670(0.5034) Grad: 12401.5244  LR: 0.00000664  \n","Epoch: [3][1000/2059] Elapsed 4m 31s (remain 4m 47s) Loss: 0.4886(0.5034) Grad: 26908.1504  LR: 0.00000628  \n","Epoch: [3][1100/2059] Elapsed 4m 58s (remain 4m 20s) Loss: 0.5486(0.5031) Grad: 70799.3750  LR: 0.00000593  \n","Epoch: [3][1200/2059] Elapsed 5m 25s (remain 3m 52s) Loss: 0.4926(0.5031) Grad: 10952.1074  LR: 0.00000558  \n","Epoch: [3][1300/2059] Elapsed 5m 52s (remain 3m 25s) Loss: 0.4848(0.5034) Grad: 21844.4082  LR: 0.00000525  \n","Epoch: [3][1400/2059] Elapsed 6m 19s (remain 2m 58s) Loss: 0.4389(0.5031) Grad: 19662.7832  LR: 0.00000491  \n","Epoch: [3][1500/2059] Elapsed 6m 46s (remain 2m 31s) Loss: 0.4678(0.5034) Grad: 48766.2031  LR: 0.00000459  \n","Epoch: [3][1600/2059] Elapsed 7m 13s (remain 2m 4s) Loss: 0.5297(0.5033) Grad: 57901.1562  LR: 0.00000427  \n","Epoch: [3][1700/2059] Elapsed 7m 40s (remain 1m 36s) Loss: 0.3700(0.5030) Grad: 19504.9707  LR: 0.00000396  \n","Epoch: [3][1800/2059] Elapsed 8m 7s (remain 1m 9s) Loss: 0.4972(0.5026) Grad: 16835.7656  LR: 0.00000366  \n","Epoch: [3][1900/2059] Elapsed 8m 34s (remain 0m 42s) Loss: 0.5135(0.5029) Grad: 25289.2480  LR: 0.00000337  \n","Epoch: [3][2000/2059] Elapsed 9m 2s (remain 0m 15s) Loss: 0.5152(0.5034) Grad: 20797.9180  LR: 0.00000309  \n","Epoch: [3][2058/2059] Elapsed 9m 17s (remain 0m 0s) Loss: 0.3251(0.5032) Grad: 33817.7852  LR: 0.00000294  \n","EVAL: [0/220] Elapsed 0m 0s (remain 2m 23s) Loss: 0.6643(0.6643) \n","EVAL: [100/220] Elapsed 0m 16s (remain 0m 19s) Loss: 0.6230(0.5680) \n","EVAL: [200/220] Elapsed 0m 33s (remain 0m 3s) Loss: 0.3299(0.5756) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.5032  avg_val_loss: 0.5764  time: 594s\n","Epoch 3 - Score: 0.8116\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [219/220] Elapsed 0m 36s (remain 0m 0s) Loss: 0.4048(0.5764) \n","Epoch: [4][0/2059] Elapsed 0m 0s (remain 27m 19s) Loss: 0.4883(0.4883) Grad: 40116.3203  LR: 0.00000293  \n","Epoch: [4][100/2059] Elapsed 0m 27s (remain 8m 58s) Loss: 0.4441(0.4966) Grad: 39416.6484  LR: 0.00000267  \n","Epoch: [4][200/2059] Elapsed 0m 54s (remain 8m 26s) Loss: 0.4498(0.4997) Grad: 21867.6797  LR: 0.00000241  \n","Epoch: [4][300/2059] Elapsed 1m 21s (remain 7m 58s) Loss: 0.3728(0.5012) Grad: 33794.4336  LR: 0.00000217  \n","Epoch: [4][400/2059] Elapsed 1m 49s (remain 7m 30s) Loss: 0.4267(0.5002) Grad: 35083.9492  LR: 0.00000194  \n","Epoch: [4][500/2059] Elapsed 2m 16s (remain 7m 3s) Loss: 0.5598(0.4997) Grad: 32756.1602  LR: 0.00000172  \n","Epoch: [4][600/2059] Elapsed 2m 43s (remain 6m 36s) Loss: 0.3888(0.4998) Grad: 34632.1484  LR: 0.00000151  \n","Epoch: [4][700/2059] Elapsed 3m 10s (remain 6m 9s) Loss: 0.5176(0.4992) Grad: 31909.5605  LR: 0.00000132  \n","Epoch: [4][800/2059] Elapsed 3m 37s (remain 5m 41s) Loss: 0.4409(0.4985) Grad: 23547.7383  LR: 0.00000113  \n","Epoch: [4][900/2059] Elapsed 4m 4s (remain 5m 14s) Loss: 0.4158(0.4987) Grad: 179255.8750  LR: 0.00000096  \n","Epoch: [4][1000/2059] Elapsed 4m 31s (remain 4m 47s) Loss: 0.6593(0.4991) Grad: 18905.3633  LR: 0.00000081  \n","Epoch: [4][1100/2059] Elapsed 4m 58s (remain 4m 20s) Loss: 0.4231(0.4988) Grad: 11345.8037  LR: 0.00000066  \n","Epoch: [4][1200/2059] Elapsed 5m 26s (remain 3m 53s) Loss: 0.5406(0.4988) Grad: 109707.4297  LR: 0.00000053  \n","Epoch: [4][1300/2059] Elapsed 5m 53s (remain 3m 25s) Loss: 0.5529(0.4975) Grad: 27810.3008  LR: 0.00000042  \n","Epoch: [4][1400/2059] Elapsed 6m 20s (remain 2m 58s) Loss: 0.5142(0.4958) Grad: 90093.0469  LR: 0.00000032  \n","Epoch: [4][1500/2059] Elapsed 6m 47s (remain 2m 31s) Loss: 0.5077(0.4954) Grad: 82223.2578  LR: 0.00000023  \n","Epoch: [4][1600/2059] Elapsed 7m 14s (remain 2m 4s) Loss: 0.4907(0.4958) Grad: 20854.2852  LR: 0.00000015  \n","Epoch: [4][1700/2059] Elapsed 7m 42s (remain 1m 37s) Loss: 0.4367(0.4952) Grad: 17528.1426  LR: 0.00000009  \n","Epoch: [4][1800/2059] Elapsed 8m 9s (remain 1m 10s) Loss: 0.4318(0.4949) Grad: 13794.2891  LR: 0.00000005  \n","Epoch: [4][1900/2059] Elapsed 8m 36s (remain 0m 42s) Loss: 0.5460(0.4952) Grad: 21583.1465  LR: 0.00000002  \n","Epoch: [4][2000/2059] Elapsed 9m 3s (remain 0m 15s) Loss: 0.5268(0.4957) Grad: 59894.7188  LR: 0.00000000  \n","Epoch: [4][2058/2059] Elapsed 9m 19s (remain 0m 0s) Loss: 0.6241(0.4957) Grad: 171239.1250  LR: 0.00000000  \n","EVAL: [0/220] Elapsed 0m 0s (remain 2m 21s) Loss: 0.6084(0.6084) \n","EVAL: [100/220] Elapsed 0m 16s (remain 0m 19s) Loss: 0.6194(0.5663) \n","EVAL: [200/220] Elapsed 0m 33s (remain 0m 3s) Loss: 0.3291(0.5745) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4957  avg_val_loss: 0.5750  time: 596s\n","Epoch 4 - Score: 0.8143\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [219/220] Elapsed 0m 36s (remain 0m 0s) Loss: 0.4065(0.5750) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 5 result ==========\n","Score: 0.8211\n","========== fold: 6 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2051] Elapsed 0m 0s (remain 27m 10s) Loss: 0.5656(0.5656) Grad: 129836.6562  LR: 0.00002000  \n","Epoch: [1][100/2051] Elapsed 0m 27s (remain 8m 57s) Loss: 0.5288(0.6318) Grad: 73143.1875  LR: 0.00001999  \n","Epoch: [1][200/2051] Elapsed 0m 54s (remain 8m 25s) Loss: 0.5203(0.6039) Grad: 54682.0625  LR: 0.00001997  \n","Epoch: [1][300/2051] Elapsed 1m 21s (remain 7m 56s) Loss: 0.4429(0.5922) Grad: 49416.2695  LR: 0.00001993  \n","Epoch: [1][400/2051] Elapsed 1m 48s (remain 7m 27s) Loss: 0.5960(0.5880) Grad: 61124.6641  LR: 0.00001988  \n","Epoch: [1][500/2051] Elapsed 2m 15s (remain 7m 0s) Loss: 0.5053(0.5838) Grad: 56818.8750  LR: 0.00001982  \n","Epoch: [1][600/2051] Elapsed 2m 42s (remain 6m 33s) Loss: 0.6579(0.5805) Grad: 198746.0625  LR: 0.00001974  \n","Epoch: [1][700/2051] Elapsed 3m 9s (remain 6m 5s) Loss: 0.5743(0.5776) Grad: 82772.8047  LR: 0.00001964  \n","Epoch: [1][800/2051] Elapsed 3m 36s (remain 5m 38s) Loss: 0.6027(0.5742) Grad: 100379.9375  LR: 0.00001953  \n","Epoch: [1][900/2051] Elapsed 4m 3s (remain 5m 11s) Loss: 0.6689(0.5714) Grad: 97150.2422  LR: 0.00001941  \n","Epoch: [1][1000/2051] Elapsed 4m 30s (remain 4m 43s) Loss: 0.5572(0.5693) Grad: 25637.7578  LR: 0.00001927  \n","Epoch: [1][1100/2051] Elapsed 4m 57s (remain 4m 16s) Loss: 0.5428(0.5668) Grad: 170270.7656  LR: 0.00001912  \n","Epoch: [1][1200/2051] Elapsed 5m 24s (remain 3m 50s) Loss: 0.4613(0.5648) Grad: 251916.4219  LR: 0.00001896  \n","Epoch: [1][1300/2051] Elapsed 5m 51s (remain 3m 22s) Loss: 0.5447(0.5633) Grad: 52632.2461  LR: 0.00001879  \n","Epoch: [1][1400/2051] Elapsed 6m 18s (remain 2m 55s) Loss: 0.5087(0.5610) Grad: 112019.5156  LR: 0.00001860  \n","Epoch: [1][1500/2051] Elapsed 6m 46s (remain 2m 28s) Loss: 0.6319(0.5602) Grad: 326794.9375  LR: 0.00001839  \n","Epoch: [1][1600/2051] Elapsed 7m 13s (remain 2m 1s) Loss: 0.4888(0.5589) Grad: 49532.7344  LR: 0.00001818  \n","Epoch: [1][1700/2051] Elapsed 7m 40s (remain 1m 34s) Loss: 0.4936(0.5579) Grad: 51230.8203  LR: 0.00001795  \n","Epoch: [1][1800/2051] Elapsed 8m 7s (remain 1m 7s) Loss: 0.5749(0.5566) Grad: 60432.6875  LR: 0.00001772  \n","Epoch: [1][1900/2051] Elapsed 8m 34s (remain 0m 40s) Loss: 0.4968(0.5558) Grad: 85685.5703  LR: 0.00001747  \n","Epoch: [1][2000/2051] Elapsed 9m 1s (remain 0m 13s) Loss: 0.4957(0.5541) Grad: 25268.7246  LR: 0.00001721  \n","Epoch: [1][2050/2051] Elapsed 9m 15s (remain 0m 0s) Loss: 0.5911(0.5540) Grad: 115021.3047  LR: 0.00001707  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 22s) Loss: 0.4743(0.4743) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4635(0.5459) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5930(0.5491) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5540  avg_val_loss: 0.5488  time: 593s\n","Epoch 1 - Score: 0.8282\n","Epoch 1 - Save Best Score: 0.8282 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.6397(0.5488) \n","Epoch: [2][0/2051] Elapsed 0m 0s (remain 26m 39s) Loss: 0.3627(0.3627) Grad: 48109.8555  LR: 0.00001707  \n","Epoch: [2][100/2051] Elapsed 0m 28s (remain 9m 4s) Loss: 0.6084(0.5260) Grad: 201125.2656  LR: 0.00001679  \n","Epoch: [2][200/2051] Elapsed 0m 55s (remain 8m 32s) Loss: 0.5721(0.5230) Grad: 37668.7578  LR: 0.00001651  \n","Epoch: [2][300/2051] Elapsed 1m 22s (remain 8m 1s) Loss: 0.4720(0.5229) Grad: 110556.7422  LR: 0.00001621  \n","Epoch: [2][400/2051] Elapsed 1m 50s (remain 7m 34s) Loss: 0.5543(0.5219) Grad: 39285.6172  LR: 0.00001591  \n","Epoch: [2][500/2051] Elapsed 2m 17s (remain 7m 5s) Loss: 0.4954(0.5192) Grad: 115656.1250  LR: 0.00001560  \n","Epoch: [2][600/2051] Elapsed 2m 44s (remain 6m 37s) Loss: 0.4140(0.5186) Grad: 97478.7422  LR: 0.00001527  \n","Epoch: [2][700/2051] Elapsed 3m 11s (remain 6m 9s) Loss: 0.4905(0.5190) Grad: 153625.8281  LR: 0.00001494  \n","Epoch: [2][800/2051] Elapsed 3m 38s (remain 5m 41s) Loss: 0.6122(0.5185) Grad: 69252.8359  LR: 0.00001461  \n","Epoch: [2][900/2051] Elapsed 4m 6s (remain 5m 14s) Loss: 0.4417(0.5182) Grad: 122651.1562  LR: 0.00001427  \n","Epoch: [2][1000/2051] Elapsed 4m 33s (remain 4m 47s) Loss: 0.5151(0.5184) Grad: 43802.4844  LR: 0.00001392  \n","Epoch: [2][1100/2051] Elapsed 5m 1s (remain 4m 20s) Loss: 0.3578(0.5178) Grad: 79490.1875  LR: 0.00001356  \n","Epoch: [2][1200/2051] Elapsed 5m 29s (remain 3m 52s) Loss: 0.3814(0.5186) Grad: 32549.4141  LR: 0.00001320  \n","Epoch: [2][1300/2051] Elapsed 5m 56s (remain 3m 25s) Loss: 0.6137(0.5183) Grad: 113074.6328  LR: 0.00001284  \n","Epoch: [2][1400/2051] Elapsed 6m 24s (remain 2m 58s) Loss: 0.5617(0.5180) Grad: 93577.0625  LR: 0.00001247  \n","Epoch: [2][1500/2051] Elapsed 6m 51s (remain 2m 30s) Loss: 0.6270(0.5184) Grad: 427244.6562  LR: 0.00001209  \n","Epoch: [2][1600/2051] Elapsed 7m 19s (remain 2m 3s) Loss: 0.5881(0.5183) Grad: 34725.3984  LR: 0.00001172  \n","Epoch: [2][1700/2051] Elapsed 7m 46s (remain 1m 36s) Loss: 0.5400(0.5177) Grad: 45560.9961  LR: 0.00001134  \n","Epoch: [2][1800/2051] Elapsed 8m 14s (remain 1m 8s) Loss: 0.3272(0.5172) Grad: 81556.3906  LR: 0.00001096  \n","Epoch: [2][1900/2051] Elapsed 8m 41s (remain 0m 41s) Loss: 0.4604(0.5173) Grad: 120035.4531  LR: 0.00001058  \n","Epoch: [2][2000/2051] Elapsed 9m 9s (remain 0m 13s) Loss: 0.5667(0.5175) Grad: 35068.9297  LR: 0.00001020  \n","Epoch: [2][2050/2051] Elapsed 9m 22s (remain 0m 0s) Loss: 0.4947(0.5176) Grad: 36762.6953  LR: 0.00001000  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 18s) Loss: 0.5219(0.5219) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4596(0.5477) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5991(0.5465) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.5176  avg_val_loss: 0.5460  time: 600s\n","Epoch 2 - Score: 0.8352\n","Epoch 2 - Save Best Score: 0.8352 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.6322(0.5460) \n","Epoch: [3][0/2051] Elapsed 0m 0s (remain 27m 17s) Loss: 0.5121(0.5121) Grad: 10065.9131  LR: 0.00001000  \n","Epoch: [3][100/2051] Elapsed 0m 28s (remain 9m 6s) Loss: 0.5743(0.4958) Grad: 29105.1230  LR: 0.00000962  \n","Epoch: [3][200/2051] Elapsed 0m 55s (remain 8m 30s) Loss: 0.4594(0.4950) Grad: 125075.1875  LR: 0.00000924  \n","Epoch: [3][300/2051] Elapsed 1m 22s (remain 7m 59s) Loss: 0.3824(0.4991) Grad: 19580.5898  LR: 0.00000885  \n","Epoch: [3][400/2051] Elapsed 1m 49s (remain 7m 31s) Loss: 0.6244(0.5027) Grad: 33390.0898  LR: 0.00000847  \n","Epoch: [3][500/2051] Elapsed 2m 16s (remain 7m 3s) Loss: 0.4463(0.5049) Grad: 19164.1836  LR: 0.00000810  \n","Epoch: [3][600/2051] Elapsed 2m 43s (remain 6m 35s) Loss: 0.5436(0.5055) Grad: 45111.0781  LR: 0.00000772  \n","Epoch: [3][700/2051] Elapsed 3m 11s (remain 6m 7s) Loss: 0.6487(0.5062) Grad: 145454.1719  LR: 0.00000735  \n","Epoch: [3][800/2051] Elapsed 3m 38s (remain 5m 40s) Loss: 0.5841(0.5075) Grad: 34941.1875  LR: 0.00000698  \n","Epoch: [3][900/2051] Elapsed 4m 5s (remain 5m 13s) Loss: 0.3732(0.5075) Grad: 24035.4707  LR: 0.00000662  \n","Epoch: [3][1000/2051] Elapsed 4m 32s (remain 4m 46s) Loss: 0.4938(0.5072) Grad: 49298.8086  LR: 0.00000626  \n","Epoch: [3][1100/2051] Elapsed 5m 0s (remain 4m 18s) Loss: 0.5789(0.5065) Grad: 147540.5312  LR: 0.00000591  \n","Epoch: [3][1200/2051] Elapsed 5m 27s (remain 3m 51s) Loss: 0.4580(0.5059) Grad: 18556.2578  LR: 0.00000557  \n","Epoch: [3][1300/2051] Elapsed 5m 54s (remain 3m 24s) Loss: 0.5960(0.5060) Grad: 99869.2812  LR: 0.00000523  \n","Epoch: [3][1400/2051] Elapsed 6m 21s (remain 2m 57s) Loss: 0.4280(0.5054) Grad: 216919.6719  LR: 0.00000489  \n","Epoch: [3][1500/2051] Elapsed 6m 48s (remain 2m 29s) Loss: 0.4719(0.5048) Grad: 155608.5156  LR: 0.00000457  \n","Epoch: [3][1600/2051] Elapsed 7m 15s (remain 2m 2s) Loss: 0.4670(0.5044) Grad: 90399.0078  LR: 0.00000425  \n","Epoch: [3][1700/2051] Elapsed 7m 42s (remain 1m 35s) Loss: 0.5844(0.5047) Grad: 27164.6094  LR: 0.00000394  \n","Epoch: [3][1800/2051] Elapsed 8m 9s (remain 1m 7s) Loss: 0.5969(0.5046) Grad: 238051.6875  LR: 0.00000364  \n","Epoch: [3][1900/2051] Elapsed 8m 36s (remain 0m 40s) Loss: 0.6817(0.5050) Grad: 63857.3047  LR: 0.00000335  \n","Epoch: [3][2000/2051] Elapsed 9m 3s (remain 0m 13s) Loss: 0.5564(0.5050) Grad: 35837.1953  LR: 0.00000307  \n","Epoch: [3][2050/2051] Elapsed 9m 17s (remain 0m 0s) Loss: 0.5113(0.5051) Grad: 92599.1562  LR: 0.00000293  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 20s) Loss: 0.4736(0.4736) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4510(0.5613) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.5990(0.5587) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.5051  avg_val_loss: 0.5574  time: 595s\n","Epoch 3 - Score: 0.8321\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.6278(0.5574) \n","Epoch: [4][0/2051] Elapsed 0m 0s (remain 26m 12s) Loss: 0.4504(0.4504) Grad: 41534.3594  LR: 0.00000293  \n","Epoch: [4][100/2051] Elapsed 0m 27s (remain 8m 57s) Loss: 0.4797(0.5016) Grad: 43290.8789  LR: 0.00000266  \n","Epoch: [4][200/2051] Elapsed 0m 54s (remain 8m 24s) Loss: 0.4499(0.5024) Grad: 34194.3477  LR: 0.00000241  \n","Epoch: [4][300/2051] Elapsed 1m 21s (remain 7m 55s) Loss: 0.4240(0.4990) Grad: 20025.8770  LR: 0.00000217  \n","Epoch: [4][400/2051] Elapsed 1m 48s (remain 7m 27s) Loss: 0.5673(0.4982) Grad: 31873.6465  LR: 0.00000193  \n","Epoch: [4][500/2051] Elapsed 2m 15s (remain 7m 0s) Loss: 0.5048(0.4977) Grad: 56370.0391  LR: 0.00000171  \n","Epoch: [4][600/2051] Elapsed 2m 42s (remain 6m 32s) Loss: 0.4735(0.4986) Grad: 55362.7070  LR: 0.00000151  \n","Epoch: [4][700/2051] Elapsed 3m 9s (remain 6m 5s) Loss: 0.6070(0.4972) Grad: 131493.7656  LR: 0.00000131  \n","Epoch: [4][800/2051] Elapsed 3m 36s (remain 5m 38s) Loss: 0.3920(0.4963) Grad: 74240.0625  LR: 0.00000113  \n","Epoch: [4][900/2051] Elapsed 4m 3s (remain 5m 11s) Loss: 0.5373(0.4977) Grad: 66010.8203  LR: 0.00000096  \n","Epoch: [4][1000/2051] Elapsed 4m 31s (remain 4m 44s) Loss: 0.4735(0.4972) Grad: 25232.2480  LR: 0.00000080  \n","Epoch: [4][1100/2051] Elapsed 4m 58s (remain 4m 17s) Loss: 0.5936(0.4969) Grad: 30139.0703  LR: 0.00000066  \n","Epoch: [4][1200/2051] Elapsed 5m 25s (remain 3m 50s) Loss: 0.4467(0.4962) Grad: 111701.6172  LR: 0.00000053  \n","Epoch: [4][1300/2051] Elapsed 5m 52s (remain 3m 23s) Loss: 0.4600(0.4959) Grad: 23156.0098  LR: 0.00000041  \n","Epoch: [4][1400/2051] Elapsed 6m 20s (remain 2m 56s) Loss: 0.5791(0.4961) Grad: 123513.1016  LR: 0.00000031  \n","Epoch: [4][1500/2051] Elapsed 6m 47s (remain 2m 29s) Loss: 0.4654(0.4969) Grad: 27680.8320  LR: 0.00000022  \n","Epoch: [4][1600/2051] Elapsed 7m 14s (remain 2m 2s) Loss: 0.4371(0.4965) Grad: 195908.9844  LR: 0.00000015  \n","Epoch: [4][1700/2051] Elapsed 7m 41s (remain 1m 34s) Loss: 0.5035(0.4966) Grad: 122208.3516  LR: 0.00000009  \n","Epoch: [4][1800/2051] Elapsed 8m 8s (remain 1m 7s) Loss: 0.4315(0.4967) Grad: 63076.4609  LR: 0.00000005  \n","Epoch: [4][1900/2051] Elapsed 8m 35s (remain 0m 40s) Loss: 0.5370(0.4964) Grad: 109717.2891  LR: 0.00000002  \n","Epoch: [4][2000/2051] Elapsed 9m 2s (remain 0m 13s) Loss: 0.6057(0.4971) Grad: 102492.3359  LR: 0.00000000  \n","Epoch: [4][2050/2051] Elapsed 9m 15s (remain 0m 0s) Loss: 0.5805(0.4970) Grad: 103671.7266  LR: 0.00000000  \n","EVAL: [0/228] Elapsed 0m 0s (remain 2m 16s) Loss: 0.4650(0.4650) \n","EVAL: [100/228] Elapsed 0m 16s (remain 0m 21s) Loss: 0.4523(0.5676) \n","EVAL: [200/228] Elapsed 0m 33s (remain 0m 4s) Loss: 0.6286(0.5655) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4970  avg_val_loss: 0.5642  time: 593s\n","Epoch 4 - Score: 0.8307\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [227/228] Elapsed 0m 37s (remain 0m 0s) Loss: 0.6306(0.5642) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 6 result ==========\n","Score: 0.8352\n","========== fold: 7 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2076] Elapsed 0m 0s (remain 27m 26s) Loss: 0.7682(0.7682) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2076] Elapsed 0m 27s (remain 9m 2s) Loss: 0.5998(0.6408) Grad: 29073.3359  LR: 0.00001999  \n","Epoch: [1][200/2076] Elapsed 0m 54s (remain 8m 30s) Loss: 0.5613(0.6122) Grad: 25863.1992  LR: 0.00001997  \n","Epoch: [1][300/2076] Elapsed 1m 21s (remain 8m 2s) Loss: 0.4900(0.5992) Grad: 16658.0312  LR: 0.00001994  \n","Epoch: [1][400/2076] Elapsed 1m 48s (remain 7m 35s) Loss: 0.5292(0.5890) Grad: 49902.8633  LR: 0.00001989  \n","Epoch: [1][500/2076] Elapsed 2m 15s (remain 7m 7s) Loss: 0.6313(0.5832) Grad: 28598.8594  LR: 0.00001982  \n","Epoch: [1][600/2076] Elapsed 2m 42s (remain 6m 39s) Loss: 0.4293(0.5792) Grad: 17731.2305  LR: 0.00001974  \n","Epoch: [1][700/2076] Elapsed 3m 9s (remain 6m 12s) Loss: 0.4822(0.5780) Grad: 22966.9590  LR: 0.00001965  \n","Epoch: [1][800/2076] Elapsed 3m 36s (remain 5m 45s) Loss: 0.5929(0.5757) Grad: 15097.1719  LR: 0.00001954  \n","Epoch: [1][900/2076] Elapsed 4m 3s (remain 5m 18s) Loss: 0.7015(0.5731) Grad: 58287.9141  LR: 0.00001942  \n","Epoch: [1][1000/2076] Elapsed 4m 31s (remain 4m 51s) Loss: 0.5365(0.5718) Grad: 4660.4414  LR: 0.00001929  \n","Epoch: [1][1100/2076] Elapsed 4m 58s (remain 4m 24s) Loss: 0.5906(0.5701) Grad: 14790.7051  LR: 0.00001914  \n","Epoch: [1][1200/2076] Elapsed 5m 25s (remain 3m 57s) Loss: 0.6042(0.5685) Grad: 23731.9961  LR: 0.00001899  \n","Epoch: [1][1300/2076] Elapsed 5m 52s (remain 3m 30s) Loss: 0.6075(0.5670) Grad: 12101.7021  LR: 0.00001881  \n","Epoch: [1][1400/2076] Elapsed 6m 20s (remain 3m 3s) Loss: 0.5069(0.5654) Grad: 13664.5225  LR: 0.00001863  \n","Epoch: [1][1500/2076] Elapsed 6m 47s (remain 2m 35s) Loss: 0.4083(0.5638) Grad: 15295.6416  LR: 0.00001843  \n","Epoch: [1][1600/2076] Elapsed 7m 14s (remain 2m 8s) Loss: 0.4629(0.5620) Grad: 14652.9004  LR: 0.00001822  \n","Epoch: [1][1700/2076] Elapsed 7m 41s (remain 1m 41s) Loss: 0.5088(0.5606) Grad: 9036.5234  LR: 0.00001800  \n","Epoch: [1][1800/2076] Elapsed 8m 8s (remain 1m 14s) Loss: 0.5498(0.5603) Grad: 7518.2759  LR: 0.00001777  \n","Epoch: [1][1900/2076] Elapsed 8m 35s (remain 0m 47s) Loss: 0.4935(0.5592) Grad: 9207.2695  LR: 0.00001752  \n","Epoch: [1][2000/2076] Elapsed 9m 2s (remain 0m 20s) Loss: 0.5070(0.5590) Grad: 8186.3662  LR: 0.00001727  \n","Epoch: [1][2075/2076] Elapsed 9m 23s (remain 0m 0s) Loss: 0.4513(0.5586) Grad: 9528.6084  LR: 0.00001707  \n","EVAL: [0/204] Elapsed 0m 0s (remain 2m 23s) Loss: 0.5415(0.5415) \n","EVAL: [100/204] Elapsed 0m 16s (remain 0m 17s) Loss: 0.4795(0.5404) \n","EVAL: [200/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.7015(0.5407) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5586  avg_val_loss: 0.5413  time: 597s\n","Epoch 1 - Score: 0.8199\n","Epoch 1 - Save Best Score: 0.8199 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [203/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.3711(0.5413) \n","Epoch: [2][0/2076] Elapsed 0m 0s (remain 28m 55s) Loss: 0.4193(0.4193) Grad: 5894.3540  LR: 0.00001707  \n","Epoch: [2][100/2076] Elapsed 0m 28s (remain 9m 11s) Loss: 0.4793(0.5167) Grad: 11596.6328  LR: 0.00001680  \n","Epoch: [2][200/2076] Elapsed 0m 55s (remain 8m 37s) Loss: 0.5226(0.5176) Grad: 9391.1592  LR: 0.00001651  \n","Epoch: [2][300/2076] Elapsed 1m 22s (remain 8m 6s) Loss: 0.4911(0.5163) Grad: 16734.5215  LR: 0.00001622  \n","Epoch: [2][400/2076] Elapsed 1m 49s (remain 7m 39s) Loss: 0.5922(0.5154) Grad: 9289.3242  LR: 0.00001592  \n","Epoch: [2][500/2076] Elapsed 2m 16s (remain 7m 10s) Loss: 0.5839(0.5161) Grad: 33668.7227  LR: 0.00001561  \n","Epoch: [2][600/2076] Elapsed 2m 43s (remain 6m 42s) Loss: 0.4068(0.5169) Grad: 17230.1328  LR: 0.00001530  \n","Epoch: [2][700/2076] Elapsed 3m 11s (remain 6m 14s) Loss: 0.7065(0.5177) Grad: 73678.6953  LR: 0.00001497  \n","Epoch: [2][800/2076] Elapsed 3m 38s (remain 5m 47s) Loss: 0.4812(0.5187) Grad: 7101.1021  LR: 0.00001464  \n","Epoch: [2][900/2076] Elapsed 4m 5s (remain 5m 19s) Loss: 0.4881(0.5192) Grad: 7450.4927  LR: 0.00001430  \n","Epoch: [2][1000/2076] Elapsed 4m 32s (remain 4m 52s) Loss: 0.5769(0.5193) Grad: 44855.0508  LR: 0.00001396  \n","Epoch: [2][1100/2076] Elapsed 4m 59s (remain 4m 24s) Loss: 0.7309(0.5201) Grad: 66649.5781  LR: 0.00001361  \n","Epoch: [2][1200/2076] Elapsed 5m 26s (remain 3m 57s) Loss: 0.4711(0.5192) Grad: 19951.8750  LR: 0.00001325  \n","Epoch: [2][1300/2076] Elapsed 5m 53s (remain 3m 30s) Loss: 0.5169(0.5196) Grad: 15475.5840  LR: 0.00001289  \n","Epoch: [2][1400/2076] Elapsed 6m 20s (remain 3m 3s) Loss: 0.7011(0.5198) Grad: 30640.9453  LR: 0.00001253  \n","Epoch: [2][1500/2076] Elapsed 6m 47s (remain 2m 35s) Loss: 0.4890(0.5190) Grad: 21799.6992  LR: 0.00001216  \n","Epoch: [2][1600/2076] Elapsed 7m 14s (remain 2m 8s) Loss: 0.5231(0.5189) Grad: 4078.3296  LR: 0.00001179  \n","Epoch: [2][1700/2076] Elapsed 7m 41s (remain 1m 41s) Loss: 0.5386(0.5187) Grad: 15485.5166  LR: 0.00001141  \n","Epoch: [2][1800/2076] Elapsed 8m 8s (remain 1m 14s) Loss: 0.5966(0.5192) Grad: 26659.5332  LR: 0.00001104  \n","Epoch: [2][1900/2076] Elapsed 8m 35s (remain 0m 47s) Loss: 0.4930(0.5193) Grad: 15012.5000  LR: 0.00001066  \n","Epoch: [2][2000/2076] Elapsed 9m 2s (remain 0m 20s) Loss: 0.4478(0.5192) Grad: 19699.5977  LR: 0.00001028  \n","Epoch: [2][2075/2076] Elapsed 9m 23s (remain 0m 0s) Loss: 0.4931(0.5192) Grad: 10029.5186  LR: 0.00001000  \n","EVAL: [0/204] Elapsed 0m 0s (remain 2m 21s) Loss: 0.5288(0.5288) \n","EVAL: [100/204] Elapsed 0m 16s (remain 0m 17s) Loss: 0.4767(0.5442) \n","EVAL: [200/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.6430(0.5433) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.5192  avg_val_loss: 0.5439  time: 597s\n","Epoch 2 - Score: 0.8307\n","Epoch 2 - Save Best Score: 0.8307 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [203/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.4477(0.5439) \n","Epoch: [3][0/2076] Elapsed 0m 0s (remain 30m 14s) Loss: 0.5293(0.5293) Grad: 19145.9980  LR: 0.00001000  \n","Epoch: [3][100/2076] Elapsed 0m 28s (remain 9m 11s) Loss: 0.3287(0.4993) Grad: 8176.9971  LR: 0.00000962  \n","Epoch: [3][200/2076] Elapsed 0m 55s (remain 8m 38s) Loss: 0.5356(0.5034) Grad: 7833.2959  LR: 0.00000924  \n","Epoch: [3][300/2076] Elapsed 1m 22s (remain 8m 7s) Loss: 0.3971(0.5031) Grad: 12460.1514  LR: 0.00000886  \n","Epoch: [3][400/2076] Elapsed 1m 49s (remain 7m 38s) Loss: 0.5204(0.5044) Grad: 45608.3867  LR: 0.00000849  \n","Epoch: [3][500/2076] Elapsed 2m 16s (remain 7m 9s) Loss: 0.5783(0.5046) Grad: 16954.0742  LR: 0.00000812  \n","Epoch: [3][600/2076] Elapsed 2m 43s (remain 6m 41s) Loss: 0.5330(0.5034) Grad: 74295.0078  LR: 0.00000775  \n","Epoch: [3][700/2076] Elapsed 3m 10s (remain 6m 14s) Loss: 0.4204(0.5032) Grad: 18810.2852  LR: 0.00000738  \n","Epoch: [3][800/2076] Elapsed 3m 37s (remain 5m 46s) Loss: 0.4720(0.5028) Grad: 29866.1934  LR: 0.00000702  \n","Epoch: [3][900/2076] Elapsed 4m 4s (remain 5m 19s) Loss: 0.5703(0.5036) Grad: 133330.1094  LR: 0.00000666  \n","Epoch: [3][1000/2076] Elapsed 4m 31s (remain 4m 51s) Loss: 0.4899(0.5036) Grad: 29950.4238  LR: 0.00000630  \n","Epoch: [3][1100/2076] Elapsed 4m 58s (remain 4m 24s) Loss: 0.5550(0.5030) Grad: 21987.6191  LR: 0.00000595  \n","Epoch: [3][1200/2076] Elapsed 5m 25s (remain 3m 57s) Loss: 0.5609(0.5025) Grad: 76240.6406  LR: 0.00000561  \n","Epoch: [3][1300/2076] Elapsed 5m 53s (remain 3m 30s) Loss: 0.4817(0.5026) Grad: 17770.2422  LR: 0.00000527  \n","Epoch: [3][1400/2076] Elapsed 6m 19s (remain 3m 3s) Loss: 0.6305(0.5019) Grad: 44111.3281  LR: 0.00000494  \n","Epoch: [3][1500/2076] Elapsed 6m 46s (remain 2m 35s) Loss: 0.4670(0.5023) Grad: 30707.9141  LR: 0.00000462  \n","Epoch: [3][1600/2076] Elapsed 7m 14s (remain 2m 8s) Loss: 0.4012(0.5022) Grad: 60755.9102  LR: 0.00000431  \n","Epoch: [3][1700/2076] Elapsed 7m 41s (remain 1m 41s) Loss: 0.4920(0.5021) Grad: 24573.1914  LR: 0.00000400  \n","Epoch: [3][1800/2076] Elapsed 8m 8s (remain 1m 14s) Loss: 0.5338(0.5024) Grad: 16621.6289  LR: 0.00000370  \n","Epoch: [3][1900/2076] Elapsed 8m 35s (remain 0m 47s) Loss: 0.5698(0.5020) Grad: 20950.4453  LR: 0.00000341  \n","Epoch: [3][2000/2076] Elapsed 9m 2s (remain 0m 20s) Loss: 0.5013(0.5017) Grad: 86913.2578  LR: 0.00000313  \n","Epoch: [3][2075/2076] Elapsed 9m 22s (remain 0m 0s) Loss: 0.4749(0.5021) Grad: 13323.3320  LR: 0.00000293  \n","EVAL: [0/204] Elapsed 0m 0s (remain 2m 25s) Loss: 0.5043(0.5043) \n","EVAL: [100/204] Elapsed 0m 17s (remain 0m 17s) Loss: 0.4816(0.5505) \n","EVAL: [200/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.5567(0.5485) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.5021  avg_val_loss: 0.5491  time: 597s\n","Epoch 3 - Score: 0.8317\n","Epoch 3 - Save Best Score: 0.8317 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [203/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.4400(0.5491) \n","Epoch: [4][0/2076] Elapsed 0m 0s (remain 30m 36s) Loss: 0.5356(0.5356) Grad: 17861.5645  LR: 0.00000293  \n","Epoch: [4][100/2076] Elapsed 0m 28s (remain 9m 17s) Loss: 0.4597(0.4946) Grad: 19574.7754  LR: 0.00000266  \n","Epoch: [4][200/2076] Elapsed 0m 56s (remain 8m 42s) Loss: 0.5720(0.4988) Grad: 25953.5605  LR: 0.00000241  \n","Epoch: [4][300/2076] Elapsed 1m 23s (remain 8m 10s) Loss: 0.5691(0.4985) Grad: 78008.4531  LR: 0.00000217  \n","Epoch: [4][400/2076] Elapsed 1m 50s (remain 7m 40s) Loss: 0.5267(0.4958) Grad: 50890.1680  LR: 0.00000194  \n","Epoch: [4][500/2076] Elapsed 2m 17s (remain 7m 11s) Loss: 0.4739(0.4944) Grad: 86690.0000  LR: 0.00000172  \n","Epoch: [4][600/2076] Elapsed 2m 44s (remain 6m 43s) Loss: 0.4807(0.4956) Grad: 50506.9297  LR: 0.00000152  \n","Epoch: [4][700/2076] Elapsed 3m 11s (remain 6m 15s) Loss: 0.4583(0.4952) Grad: 27736.8711  LR: 0.00000132  \n","Epoch: [4][800/2076] Elapsed 3m 38s (remain 5m 47s) Loss: 0.4895(0.4955) Grad: 27830.4414  LR: 0.00000114  \n","Epoch: [4][900/2076] Elapsed 4m 5s (remain 5m 19s) Loss: 0.5206(0.4952) Grad: 165535.7031  LR: 0.00000097  \n","Epoch: [4][1000/2076] Elapsed 4m 32s (remain 4m 52s) Loss: 0.5237(0.4947) Grad: 34436.9102  LR: 0.00000082  \n","Epoch: [4][1100/2076] Elapsed 4m 59s (remain 4m 24s) Loss: 0.4685(0.4952) Grad: 68745.7188  LR: 0.00000067  \n","Epoch: [4][1200/2076] Elapsed 5m 26s (remain 3m 57s) Loss: 0.5000(0.4953) Grad: 32635.5625  LR: 0.00000054  \n","Epoch: [4][1300/2076] Elapsed 5m 53s (remain 3m 30s) Loss: 0.6585(0.4953) Grad: 95518.1484  LR: 0.00000043  \n","Epoch: [4][1400/2076] Elapsed 6m 20s (remain 3m 3s) Loss: 0.4023(0.4951) Grad: 26634.6895  LR: 0.00000032  \n","Epoch: [4][1500/2076] Elapsed 6m 47s (remain 2m 36s) Loss: 0.5429(0.4953) Grad: 55749.2773  LR: 0.00000024  \n","Epoch: [4][1600/2076] Elapsed 7m 14s (remain 2m 8s) Loss: 0.5048(0.4950) Grad: 47886.6055  LR: 0.00000016  \n","Epoch: [4][1700/2076] Elapsed 7m 41s (remain 1m 41s) Loss: 0.5984(0.4951) Grad: 35399.2617  LR: 0.00000010  \n","Epoch: [4][1800/2076] Elapsed 8m 8s (remain 1m 14s) Loss: 0.3118(0.4947) Grad: 26518.0312  LR: 0.00000005  \n","Epoch: [4][1900/2076] Elapsed 8m 35s (remain 0m 47s) Loss: 0.5083(0.4949) Grad: 45778.1836  LR: 0.00000002  \n","Epoch: [4][2000/2076] Elapsed 9m 2s (remain 0m 20s) Loss: 0.5788(0.4951) Grad: 35058.0352  LR: 0.00000000  \n","Epoch: [4][2075/2076] Elapsed 9m 22s (remain 0m 0s) Loss: 0.4916(0.4954) Grad: 37669.9219  LR: 0.00000000  \n","EVAL: [0/204] Elapsed 0m 0s (remain 2m 22s) Loss: 0.5149(0.5149) \n","EVAL: [100/204] Elapsed 0m 16s (remain 0m 17s) Loss: 0.4846(0.5546) \n","EVAL: [200/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.6344(0.5556) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4954  avg_val_loss: 0.5561  time: 596s\n","Epoch 4 - Score: 0.8306\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [203/204] Elapsed 0m 33s (remain 0m 0s) Loss: 0.4098(0.5561) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 7 result ==========\n","Score: 0.8317\n","========== fold: 8 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2061] Elapsed 0m 0s (remain 31m 57s) Loss: 0.7149(0.7149) Grad: 127605.9219  LR: 0.00002000  \n","Epoch: [1][100/2061] Elapsed 0m 27s (remain 8m 59s) Loss: 0.6507(0.6469) Grad: 113185.9609  LR: 0.00001999  \n","Epoch: [1][200/2061] Elapsed 0m 54s (remain 8m 28s) Loss: 0.6862(0.6156) Grad: 131580.5156  LR: 0.00001997  \n","Epoch: [1][300/2061] Elapsed 1m 21s (remain 7m 58s) Loss: 0.4890(0.6024) Grad: 21236.9570  LR: 0.00001993  \n","Epoch: [1][400/2061] Elapsed 1m 48s (remain 7m 30s) Loss: 0.5843(0.5915) Grad: 20693.3320  LR: 0.00001988  \n","Epoch: [1][500/2061] Elapsed 2m 15s (remain 7m 2s) Loss: 0.5182(0.5842) Grad: 6865.7783  LR: 0.00001982  \n","Epoch: [1][600/2061] Elapsed 2m 42s (remain 6m 35s) Loss: 0.6326(0.5816) Grad: 8483.4219  LR: 0.00001974  \n","Epoch: [1][700/2061] Elapsed 3m 9s (remain 6m 7s) Loss: 0.5125(0.5781) Grad: 12633.0801  LR: 0.00001965  \n","Epoch: [1][800/2061] Elapsed 3m 36s (remain 5m 40s) Loss: 0.6755(0.5732) Grad: 40195.8242  LR: 0.00001954  \n","Epoch: [1][900/2061] Elapsed 4m 3s (remain 5m 13s) Loss: 0.5480(0.5704) Grad: 12199.3604  LR: 0.00001942  \n","Epoch: [1][1000/2061] Elapsed 4m 30s (remain 4m 46s) Loss: 0.5227(0.5698) Grad: 24358.0547  LR: 0.00001928  \n","Epoch: [1][1100/2061] Elapsed 4m 57s (remain 4m 19s) Loss: 0.5365(0.5681) Grad: 6265.1558  LR: 0.00001913  \n","Epoch: [1][1200/2061] Elapsed 5m 24s (remain 3m 52s) Loss: 0.5396(0.5672) Grad: 18021.8652  LR: 0.00001897  \n","Epoch: [1][1300/2061] Elapsed 5m 50s (remain 3m 25s) Loss: 0.4577(0.5658) Grad: 8685.7109  LR: 0.00001880  \n","Epoch: [1][1400/2061] Elapsed 6m 17s (remain 2m 57s) Loss: 0.5865(0.5646) Grad: 15451.1387  LR: 0.00001861  \n","Epoch: [1][1500/2061] Elapsed 6m 44s (remain 2m 30s) Loss: 0.7155(0.5630) Grad: 14699.4980  LR: 0.00001841  \n","Epoch: [1][1600/2061] Elapsed 7m 11s (remain 2m 3s) Loss: 0.5565(0.5617) Grad: 10519.5586  LR: 0.00001820  \n","Epoch: [1][1700/2061] Elapsed 7m 38s (remain 1m 37s) Loss: 0.4627(0.5606) Grad: 8830.5254  LR: 0.00001797  \n","Epoch: [1][1800/2061] Elapsed 8m 5s (remain 1m 10s) Loss: 0.5565(0.5594) Grad: 14831.3086  LR: 0.00001774  \n","Epoch: [1][1900/2061] Elapsed 8m 32s (remain 0m 43s) Loss: 0.5794(0.5582) Grad: 8524.4980  LR: 0.00001749  \n","Epoch: [1][2000/2061] Elapsed 8m 59s (remain 0m 16s) Loss: 0.5359(0.5573) Grad: 4732.9980  LR: 0.00001723  \n","Epoch: [1][2060/2061] Elapsed 9m 15s (remain 0m 0s) Loss: 0.5846(0.5574) Grad: 16134.0605  LR: 0.00001707  \n","EVAL: [0/218] Elapsed 0m 0s (remain 2m 34s) Loss: 0.5556(0.5556) \n","EVAL: [100/218] Elapsed 0m 17s (remain 0m 19s) Loss: 0.6506(0.5373) \n","EVAL: [200/218] Elapsed 0m 33s (remain 0m 2s) Loss: 0.6319(0.5456) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5574  avg_val_loss: 0.5492  time: 592s\n","Epoch 1 - Score: 0.8124\n","Epoch 1 - Save Best Score: 0.8124 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [217/218] Elapsed 0m 36s (remain 0m 0s) Loss: 0.6030(0.5492) \n","Epoch: [2][0/2061] Elapsed 0m 0s (remain 31m 10s) Loss: 0.5098(0.5098) Grad: 11922.9795  LR: 0.00001707  \n","Epoch: [2][100/2061] Elapsed 0m 28s (remain 9m 13s) Loss: 0.4990(0.5223) Grad: 16160.7666  LR: 0.00001680  \n","Epoch: [2][200/2061] Elapsed 0m 55s (remain 8m 36s) Loss: 0.4255(0.5201) Grad: 9140.6992  LR: 0.00001651  \n","Epoch: [2][300/2061] Elapsed 1m 22s (remain 8m 4s) Loss: 0.4138(0.5219) Grad: 15854.8506  LR: 0.00001622  \n","Epoch: [2][400/2061] Elapsed 1m 50s (remain 7m 36s) Loss: 0.5499(0.5200) Grad: 12736.0830  LR: 0.00001591  \n","Epoch: [2][500/2061] Elapsed 2m 17s (remain 7m 7s) Loss: 0.4768(0.5200) Grad: 15360.8164  LR: 0.00001560  \n","Epoch: [2][600/2061] Elapsed 2m 44s (remain 6m 38s) Loss: 0.5937(0.5194) Grad: 20179.7949  LR: 0.00001528  \n","Epoch: [2][700/2061] Elapsed 3m 11s (remain 6m 10s) Loss: 0.5471(0.5205) Grad: 24620.0430  LR: 0.00001496  \n","Epoch: [2][800/2061] Elapsed 3m 37s (remain 5m 42s) Loss: 0.6139(0.5209) Grad: 64202.0938  LR: 0.00001462  \n","Epoch: [2][900/2061] Elapsed 4m 4s (remain 5m 15s) Loss: 0.5362(0.5194) Grad: 21151.0977  LR: 0.00001428  \n","Epoch: [2][1000/2061] Elapsed 4m 31s (remain 4m 47s) Loss: 0.4331(0.5188) Grad: 10058.3301  LR: 0.00001393  \n","Epoch: [2][1100/2061] Elapsed 4m 58s (remain 4m 20s) Loss: 0.5158(0.5196) Grad: 14113.8447  LR: 0.00001358  \n","Epoch: [2][1200/2061] Elapsed 5m 25s (remain 3m 53s) Loss: 0.5097(0.5200) Grad: 130493.6406  LR: 0.00001322  \n","Epoch: [2][1300/2061] Elapsed 5m 52s (remain 3m 26s) Loss: 0.5536(0.5198) Grad: 11249.6396  LR: 0.00001286  \n","Epoch: [2][1400/2061] Elapsed 6m 19s (remain 2m 58s) Loss: 0.5177(0.5196) Grad: 14889.3828  LR: 0.00001249  \n","Epoch: [2][1500/2061] Elapsed 6m 46s (remain 2m 31s) Loss: 0.4540(0.5190) Grad: 8060.3506  LR: 0.00001212  \n","Epoch: [2][1600/2061] Elapsed 7m 13s (remain 2m 4s) Loss: 0.5096(0.5189) Grad: 12240.1074  LR: 0.00001175  \n","Epoch: [2][1700/2061] Elapsed 7m 40s (remain 1m 37s) Loss: 0.4715(0.5184) Grad: 40932.3242  LR: 0.00001137  \n","Epoch: [2][1800/2061] Elapsed 8m 8s (remain 1m 10s) Loss: 0.5577(0.5179) Grad: 5677.8530  LR: 0.00001099  \n","Epoch: [2][1900/2061] Elapsed 8m 35s (remain 0m 43s) Loss: 0.5139(0.5182) Grad: 18082.7539  LR: 0.00001061  \n","Epoch: [2][2000/2061] Elapsed 9m 2s (remain 0m 16s) Loss: 0.5599(0.5184) Grad: 29106.1172  LR: 0.00001023  \n","Epoch: [2][2060/2061] Elapsed 9m 18s (remain 0m 0s) Loss: 0.5596(0.5185) Grad: 8052.1924  LR: 0.00001000  \n","EVAL: [0/218] Elapsed 0m 0s (remain 2m 36s) Loss: 0.5355(0.5355) \n","EVAL: [100/218] Elapsed 0m 17s (remain 0m 19s) Loss: 0.6592(0.5365) \n","EVAL: [200/218] Elapsed 0m 33s (remain 0m 2s) Loss: 0.7501(0.5540) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.5185  avg_val_loss: 0.5565  time: 595s\n","Epoch 2 - Score: 0.8182\n","Epoch 2 - Save Best Score: 0.8182 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [217/218] Elapsed 0m 36s (remain 0m 0s) Loss: 0.6156(0.5565) \n","Epoch: [3][0/2061] Elapsed 0m 0s (remain 30m 36s) Loss: 0.4394(0.4394) Grad: 12256.9688  LR: 0.00001000  \n","Epoch: [3][100/2061] Elapsed 0m 28s (remain 9m 11s) Loss: 0.4910(0.4988) Grad: 10839.2637  LR: 0.00000962  \n","Epoch: [3][200/2061] Elapsed 0m 55s (remain 8m 36s) Loss: 0.4489(0.5025) Grad: 18978.9941  LR: 0.00000924  \n","Epoch: [3][300/2061] Elapsed 1m 22s (remain 8m 4s) Loss: 0.3948(0.5068) Grad: 20833.7676  LR: 0.00000886  \n","Epoch: [3][400/2061] Elapsed 1m 49s (remain 7m 34s) Loss: 0.4788(0.5037) Grad: 13676.2256  LR: 0.00000848  \n","Epoch: [3][500/2061] Elapsed 2m 16s (remain 7m 6s) Loss: 0.6555(0.5034) Grad: 166808.9375  LR: 0.00000811  \n","Epoch: [3][600/2061] Elapsed 2m 43s (remain 6m 37s) Loss: 0.4774(0.5033) Grad: 18533.3809  LR: 0.00000773  \n","Epoch: [3][700/2061] Elapsed 3m 10s (remain 6m 9s) Loss: 0.5564(0.5030) Grad: 17280.1523  LR: 0.00000736  \n","Epoch: [3][800/2061] Elapsed 3m 37s (remain 5m 42s) Loss: 0.6091(0.5032) Grad: 103937.9922  LR: 0.00000700  \n","Epoch: [3][900/2061] Elapsed 4m 4s (remain 5m 15s) Loss: 0.6189(0.5037) Grad: 22373.5117  LR: 0.00000664  \n","Epoch: [3][1000/2061] Elapsed 4m 31s (remain 4m 47s) Loss: 0.5618(0.5025) Grad: 69612.1875  LR: 0.00000628  \n","Epoch: [3][1100/2061] Elapsed 4m 58s (remain 4m 20s) Loss: 0.3792(0.5025) Grad: 30812.3750  LR: 0.00000593  \n","Epoch: [3][1200/2061] Elapsed 5m 25s (remain 3m 53s) Loss: 0.4671(0.5018) Grad: 20860.8789  LR: 0.00000559  \n","Epoch: [3][1300/2061] Elapsed 5m 52s (remain 3m 25s) Loss: 0.4804(0.5021) Grad: 14687.5400  LR: 0.00000525  \n","Epoch: [3][1400/2061] Elapsed 6m 19s (remain 2m 58s) Loss: 0.4072(0.5021) Grad: 70920.6172  LR: 0.00000492  \n","Epoch: [3][1500/2061] Elapsed 6m 46s (remain 2m 31s) Loss: 0.5145(0.5018) Grad: 131429.3438  LR: 0.00000459  \n","Epoch: [3][1600/2061] Elapsed 7m 13s (remain 2m 4s) Loss: 0.5167(0.5018) Grad: 12402.7725  LR: 0.00000427  \n","Epoch: [3][1700/2061] Elapsed 7m 40s (remain 1m 37s) Loss: 0.4758(0.5016) Grad: 19003.2910  LR: 0.00000397  \n","Epoch: [3][1800/2061] Elapsed 8m 7s (remain 1m 10s) Loss: 0.5770(0.5018) Grad: 27702.9980  LR: 0.00000367  \n","Epoch: [3][1900/2061] Elapsed 8m 35s (remain 0m 43s) Loss: 0.5078(0.5022) Grad: 28723.0176  LR: 0.00000338  \n","Epoch: [3][2000/2061] Elapsed 9m 2s (remain 0m 16s) Loss: 0.4491(0.5016) Grad: 15430.6523  LR: 0.00000310  \n","Epoch: [3][2060/2061] Elapsed 9m 18s (remain 0m 0s) Loss: 0.5292(0.5016) Grad: 50179.4258  LR: 0.00000293  \n","EVAL: [0/218] Elapsed 0m 0s (remain 2m 39s) Loss: 0.5377(0.5377) \n","EVAL: [100/218] Elapsed 0m 17s (remain 0m 19s) Loss: 0.6518(0.5461) \n","EVAL: [200/218] Elapsed 0m 33s (remain 0m 2s) Loss: 0.7069(0.5594) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.5016  avg_val_loss: 0.5611  time: 595s\n","Epoch 3 - Score: 0.8254\n","Epoch 3 - Save Best Score: 0.8254 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [217/218] Elapsed 0m 36s (remain 0m 0s) Loss: 0.5873(0.5611) \n","Epoch: [4][0/2061] Elapsed 0m 0s (remain 30m 46s) Loss: 0.3520(0.3520) Grad: 35705.8867  LR: 0.00000293  \n","Epoch: [4][100/2061] Elapsed 0m 28s (remain 9m 8s) Loss: 0.4341(0.4914) Grad: 21792.8750  LR: 0.00000267  \n","Epoch: [4][200/2061] Elapsed 0m 55s (remain 8m 34s) Loss: 0.5559(0.4928) Grad: 87501.5156  LR: 0.00000241  \n","Epoch: [4][300/2061] Elapsed 1m 22s (remain 8m 3s) Loss: 0.4268(0.4972) Grad: 45112.3750  LR: 0.00000217  \n","Epoch: [4][400/2061] Elapsed 1m 49s (remain 7m 34s) Loss: 0.4949(0.4978) Grad: 97857.0703  LR: 0.00000194  \n","Epoch: [4][500/2061] Elapsed 2m 16s (remain 7m 6s) Loss: 0.5956(0.4976) Grad: 55041.0859  LR: 0.00000172  \n","Epoch: [4][600/2061] Elapsed 2m 44s (remain 6m 38s) Loss: 0.3972(0.4942) Grad: 29519.7793  LR: 0.00000151  \n","Epoch: [4][700/2061] Elapsed 3m 11s (remain 6m 10s) Loss: 0.5475(0.4944) Grad: 44667.7617  LR: 0.00000132  \n","Epoch: [4][800/2061] Elapsed 3m 38s (remain 5m 43s) Loss: 0.4632(0.4945) Grad: 523303.8750  LR: 0.00000113  \n","Epoch: [4][900/2061] Elapsed 4m 5s (remain 5m 15s) Loss: 0.5143(0.4939) Grad: 37925.6797  LR: 0.00000096  \n","Epoch: [4][1000/2061] Elapsed 4m 32s (remain 4m 48s) Loss: 0.4883(0.4944) Grad: 39849.9180  LR: 0.00000081  \n","Epoch: [4][1100/2061] Elapsed 4m 59s (remain 4m 21s) Loss: 0.4827(0.4945) Grad: 168915.6719  LR: 0.00000066  \n","Epoch: [4][1200/2061] Elapsed 5m 26s (remain 3m 54s) Loss: 0.3684(0.4940) Grad: 21716.4355  LR: 0.00000053  \n","Epoch: [4][1300/2061] Elapsed 5m 54s (remain 3m 26s) Loss: 0.5132(0.4943) Grad: 46130.5156  LR: 0.00000042  \n","Epoch: [4][1400/2061] Elapsed 6m 21s (remain 2m 59s) Loss: 0.4617(0.4949) Grad: 50479.7734  LR: 0.00000032  \n","Epoch: [4][1500/2061] Elapsed 6m 48s (remain 2m 32s) Loss: 0.4428(0.4946) Grad: 37817.2969  LR: 0.00000023  \n","Epoch: [4][1600/2061] Elapsed 7m 15s (remain 2m 5s) Loss: 0.5436(0.4951) Grad: 31928.4863  LR: 0.00000015  \n","Epoch: [4][1700/2061] Elapsed 7m 42s (remain 1m 37s) Loss: 0.6236(0.4954) Grad: 25248.7598  LR: 0.00000009  \n","Epoch: [4][1800/2061] Elapsed 8m 9s (remain 1m 10s) Loss: 0.5495(0.4955) Grad: 93847.8750  LR: 0.00000005  \n","Epoch: [4][1900/2061] Elapsed 8m 36s (remain 0m 43s) Loss: 0.4967(0.4952) Grad: 47581.5000  LR: 0.00000002  \n","Epoch: [4][2000/2061] Elapsed 9m 3s (remain 0m 16s) Loss: 0.4351(0.4953) Grad: 42836.0938  LR: 0.00000000  \n","Epoch: [4][2060/2061] Elapsed 9m 19s (remain 0m 0s) Loss: 0.5965(0.4954) Grad: 75841.1719  LR: 0.00000000  \n","EVAL: [0/218] Elapsed 0m 0s (remain 2m 42s) Loss: 0.5480(0.5480) \n","EVAL: [100/218] Elapsed 0m 17s (remain 0m 19s) Loss: 0.6526(0.5489) \n","EVAL: [200/218] Elapsed 0m 33s (remain 0m 2s) Loss: 0.7175(0.5625) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4954  avg_val_loss: 0.5638  time: 596s\n","Epoch 4 - Score: 0.8246\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [217/218] Elapsed 0m 36s (remain 0m 0s) Loss: 0.5875(0.5638) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 8 result ==========\n","Score: 0.8254\n","========== fold: 9 training ==========\n","Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2032] Elapsed 0m 0s (remain 27m 57s) Loss: 0.6706(0.6706) Grad: inf  LR: 0.00002000  \n","Epoch: [1][100/2032] Elapsed 0m 27s (remain 8m 51s) Loss: 0.5632(0.6469) Grad: 45837.4336  LR: 0.00001999  \n","Epoch: [1][200/2032] Elapsed 0m 54s (remain 8m 19s) Loss: 0.6237(0.6159) Grad: 37316.8633  LR: 0.00001997  \n","Epoch: [1][300/2032] Elapsed 1m 21s (remain 7m 51s) Loss: 0.5494(0.6014) Grad: 54788.5039  LR: 0.00001993  \n","Epoch: [1][400/2032] Elapsed 1m 49s (remain 7m 23s) Loss: 0.6583(0.5894) Grad: 64812.0742  LR: 0.00001988  \n","Epoch: [1][500/2032] Elapsed 2m 16s (remain 6m 55s) Loss: 0.5964(0.5831) Grad: 19305.2207  LR: 0.00001981  \n","Epoch: [1][600/2032] Elapsed 2m 43s (remain 6m 28s) Loss: 0.5135(0.5796) Grad: 29908.4414  LR: 0.00001973  \n","Epoch: [1][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.5372(0.5760) Grad: 9860.5928  LR: 0.00001964  \n","Epoch: [1][800/2032] Elapsed 3m 37s (remain 5m 33s) Loss: 0.5379(0.5723) Grad: 23392.3984  LR: 0.00001952  \n","Epoch: [1][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.5004(0.5697) Grad: 24614.5176  LR: 0.00001940  \n","Epoch: [1][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.4686(0.5678) Grad: 30998.3262  LR: 0.00001926  \n","Epoch: [1][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.6104(0.5664) Grad: 12821.5098  LR: 0.00001911  \n","Epoch: [1][1200/2032] Elapsed 5m 26s (remain 3m 45s) Loss: 0.5326(0.5644) Grad: 23252.5547  LR: 0.00001894  \n","Epoch: [1][1300/2032] Elapsed 5m 53s (remain 3m 18s) Loss: 0.6468(0.5630) Grad: 50068.2461  LR: 0.00001876  \n","Epoch: [1][1400/2032] Elapsed 6m 20s (remain 2m 51s) Loss: 0.5808(0.5613) Grad: 27484.5059  LR: 0.00001857  \n","Epoch: [1][1500/2032] Elapsed 6m 47s (remain 2m 24s) Loss: 0.5230(0.5604) Grad: 11414.6426  LR: 0.00001836  \n","Epoch: [1][1600/2032] Elapsed 7m 14s (remain 1m 57s) Loss: 0.4256(0.5589) Grad: 16503.5508  LR: 0.00001815  \n","Epoch: [1][1700/2032] Elapsed 7m 41s (remain 1m 29s) Loss: 0.5534(0.5574) Grad: 16054.1689  LR: 0.00001792  \n","Epoch: [1][1800/2032] Elapsed 8m 8s (remain 1m 2s) Loss: 0.5653(0.5571) Grad: 24521.0332  LR: 0.00001768  \n","Epoch: [1][1900/2032] Elapsed 8m 35s (remain 0m 35s) Loss: 0.4623(0.5564) Grad: 18112.1113  LR: 0.00001742  \n","Epoch: [1][2000/2032] Elapsed 9m 2s (remain 0m 8s) Loss: 0.5197(0.5554) Grad: 67702.2344  LR: 0.00001716  \n","Epoch: [1][2031/2032] Elapsed 9m 11s (remain 0m 0s) Loss: 0.4619(0.5553) Grad: 34721.2109  LR: 0.00001707  \n","EVAL: [0/247] Elapsed 0m 0s (remain 2m 56s) Loss: 0.4833(0.4833) \n","EVAL: [100/247] Elapsed 0m 16s (remain 0m 24s) Loss: 0.6015(0.5808) \n","EVAL: [200/247] Elapsed 0m 33s (remain 0m 7s) Loss: 0.7837(0.5767) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.5553  avg_val_loss: 0.5666  time: 592s\n","Epoch 1 - Score: 0.8137\n","Epoch 1 - Save Best Score: 0.8137 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [246/247] Elapsed 0m 40s (remain 0m 0s) Loss: 0.3504(0.5666) \n","Epoch: [2][0/2032] Elapsed 0m 0s (remain 29m 56s) Loss: 0.5401(0.5401) Grad: 43911.3672  LR: 0.00001707  \n","Epoch: [2][100/2032] Elapsed 0m 28s (remain 8m 58s) Loss: 0.4265(0.5248) Grad: 18280.1191  LR: 0.00001679  \n","Epoch: [2][200/2032] Elapsed 0m 55s (remain 8m 23s) Loss: 0.5421(0.5222) Grad: 119284.5625  LR: 0.00001650  \n","Epoch: [2][300/2032] Elapsed 1m 22s (remain 7m 52s) Loss: 0.6029(0.5226) Grad: 58465.0977  LR: 0.00001621  \n","Epoch: [2][400/2032] Elapsed 1m 49s (remain 7m 24s) Loss: 0.5906(0.5217) Grad: 41097.3750  LR: 0.00001590  \n","Epoch: [2][500/2032] Elapsed 2m 16s (remain 6m 56s) Loss: 0.6539(0.5203) Grad: 60368.3398  LR: 0.00001558  \n","Epoch: [2][600/2032] Elapsed 2m 43s (remain 6m 28s) Loss: 0.6143(0.5182) Grad: 75073.6484  LR: 0.00001526  \n","Epoch: [2][700/2032] Elapsed 3m 10s (remain 6m 0s) Loss: 0.5241(0.5171) Grad: 63306.5781  LR: 0.00001492  \n","Epoch: [2][800/2032] Elapsed 3m 37s (remain 5m 33s) Loss: 0.7694(0.5167) Grad: 180913.5938  LR: 0.00001458  \n","Epoch: [2][900/2032] Elapsed 4m 4s (remain 5m 6s) Loss: 0.5803(0.5167) Grad: 20034.7578  LR: 0.00001424  \n","Epoch: [2][1000/2032] Elapsed 4m 31s (remain 4m 39s) Loss: 0.5484(0.5162) Grad: 25213.6387  LR: 0.00001388  \n","Epoch: [2][1100/2032] Elapsed 4m 58s (remain 4m 12s) Loss: 0.5374(0.5164) Grad: 20381.5820  LR: 0.00001353  \n","Epoch: [2][1200/2032] Elapsed 5m 25s (remain 3m 45s) Loss: 0.4916(0.5165) Grad: 31981.6406  LR: 0.00001316  \n","Epoch: [2][1300/2032] Elapsed 5m 52s (remain 3m 18s) Loss: 0.5737(0.5166) Grad: 61661.5156  LR: 0.00001279  \n","Epoch: [2][1400/2032] Elapsed 6m 19s (remain 2m 51s) Loss: 0.3966(0.5162) Grad: 13130.9492  LR: 0.00001242  \n","Epoch: [2][1500/2032] Elapsed 6m 46s (remain 2m 23s) Loss: 0.4951(0.5163) Grad: 25693.0879  LR: 0.00001204  \n","Epoch: [2][1600/2032] Elapsed 7m 13s (remain 1m 56s) Loss: 0.6094(0.5164) Grad: 294665.3438  LR: 0.00001166  \n","Epoch: [2][1700/2032] Elapsed 7m 41s (remain 1m 29s) Loss: 0.4785(0.5169) Grad: 21883.7578  LR: 0.00001128  \n","Epoch: [2][1800/2032] Elapsed 8m 8s (remain 1m 2s) Loss: 0.4763(0.5164) Grad: 13230.8389  LR: 0.00001090  \n","Epoch: [2][1900/2032] Elapsed 8m 35s (remain 0m 35s) Loss: 0.5929(0.5167) Grad: 17974.3770  LR: 0.00001051  \n","Epoch: [2][2000/2032] Elapsed 9m 2s (remain 0m 8s) Loss: 0.6120(0.5166) Grad: 108552.1094  LR: 0.00001013  \n","Epoch: [2][2031/2032] Elapsed 9m 10s (remain 0m 0s) Loss: 0.5954(0.5167) Grad: 42860.3750  LR: 0.00001001  \n","EVAL: [0/247] Elapsed 0m 0s (remain 2m 57s) Loss: 0.4013(0.4013) \n","EVAL: [100/247] Elapsed 0m 16s (remain 0m 24s) Loss: 0.6354(0.5530) \n","EVAL: [200/247] Elapsed 0m 33s (remain 0m 7s) Loss: 0.8893(0.5574) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.5167  avg_val_loss: 0.5494  time: 592s\n","Epoch 2 - Score: 0.8359\n","Epoch 2 - Save Best Score: 0.8359 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [246/247] Elapsed 0m 40s (remain 0m 0s) Loss: 0.3759(0.5494) \n","Epoch: [3][0/2032] Elapsed 0m 0s (remain 30m 54s) Loss: 0.5137(0.5137) Grad: 51942.2969  LR: 0.00001000  \n","Epoch: [3][100/2032] Elapsed 0m 28s (remain 9m 3s) Loss: 0.3006(0.4923) Grad: 120898.7188  LR: 0.00000962  \n","Epoch: [3][200/2032] Elapsed 0m 55s (remain 8m 26s) Loss: 0.5434(0.4983) Grad: 52388.7148  LR: 0.00000923  \n","Epoch: [3][300/2032] Elapsed 1m 22s (remain 7m 55s) Loss: 0.4860(0.4975) Grad: 19080.9336  LR: 0.00000885  \n","Epoch: [3][400/2032] Elapsed 1m 49s (remain 7m 26s) Loss: 0.5198(0.4992) Grad: 333764.5938  LR: 0.00000846  \n","Epoch: [3][500/2032] Elapsed 2m 16s (remain 6m 58s) Loss: 0.4824(0.5001) Grad: 142732.7031  LR: 0.00000808  \n","Epoch: [3][600/2032] Elapsed 2m 43s (remain 6m 29s) Loss: 0.4932(0.5002) Grad: 30889.3633  LR: 0.00000770  \n","Epoch: [3][700/2032] Elapsed 3m 10s (remain 6m 1s) Loss: 0.5075(0.4994) Grad: 54373.6211  LR: 0.00000733  \n","Epoch: [3][800/2032] Elapsed 3m 37s (remain 5m 34s) Loss: 0.6098(0.4994) Grad: 248802.9375  LR: 0.00000696  \n","Epoch: [3][900/2032] Elapsed 4m 5s (remain 5m 7s) Loss: 0.4930(0.4994) Grad: 41012.5664  LR: 0.00000659  \n","Epoch: [3][1000/2032] Elapsed 4m 32s (remain 4m 40s) Loss: 0.5124(0.5005) Grad: 31277.0938  LR: 0.00000623  \n","Epoch: [3][1100/2032] Elapsed 4m 59s (remain 4m 13s) Loss: 0.4560(0.5006) Grad: 29141.1504  LR: 0.00000588  \n","Epoch: [3][1200/2032] Elapsed 5m 27s (remain 3m 46s) Loss: 0.4309(0.5006) Grad: 46146.6758  LR: 0.00000553  \n","Epoch: [3][1300/2032] Elapsed 5m 54s (remain 3m 19s) Loss: 0.6063(0.5004) Grad: 427302.7188  LR: 0.00000519  \n","Epoch: [3][1400/2032] Elapsed 6m 21s (remain 2m 51s) Loss: 0.6089(0.5000) Grad: 39056.4414  LR: 0.00000485  \n","Epoch: [3][1500/2032] Elapsed 6m 48s (remain 2m 24s) Loss: 0.4374(0.4996) Grad: 37201.4961  LR: 0.00000453  \n","Epoch: [3][1600/2032] Elapsed 7m 15s (remain 1m 57s) Loss: 0.6306(0.4998) Grad: 96720.4219  LR: 0.00000421  \n","Epoch: [3][1700/2032] Elapsed 7m 43s (remain 1m 30s) Loss: 0.5357(0.5008) Grad: 57532.4570  LR: 0.00000390  \n","Epoch: [3][1800/2032] Elapsed 8m 10s (remain 1m 2s) Loss: 0.4461(0.5006) Grad: 83847.2031  LR: 0.00000359  \n","Epoch: [3][1900/2032] Elapsed 8m 37s (remain 0m 35s) Loss: 0.5390(0.5009) Grad: 28152.2988  LR: 0.00000330  \n","Epoch: [3][2000/2032] Elapsed 9m 4s (remain 0m 8s) Loss: 0.5540(0.5013) Grad: 203513.6406  LR: 0.00000302  \n","Epoch: [3][2031/2032] Elapsed 9m 13s (remain 0m 0s) Loss: 0.5160(0.5014) Grad: 86759.3828  LR: 0.00000294  \n","EVAL: [0/247] Elapsed 0m 0s (remain 3m 1s) Loss: 0.4711(0.4711) \n","EVAL: [100/247] Elapsed 0m 17s (remain 0m 24s) Loss: 0.6438(0.5566) \n","EVAL: [200/247] Elapsed 0m 33s (remain 0m 7s) Loss: 0.9374(0.5629) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.5014  avg_val_loss: 0.5538  time: 594s\n","Epoch 3 - Score: 0.8405\n","Epoch 3 - Save Best Score: 0.8405 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [246/247] Elapsed 0m 40s (remain 0m 0s) Loss: 0.3874(0.5538) \n","Epoch: [4][0/2032] Elapsed 0m 0s (remain 29m 57s) Loss: 0.5905(0.5905) Grad: 43927.2695  LR: 0.00000293  \n","Epoch: [4][100/2032] Elapsed 0m 28s (remain 9m 4s) Loss: 0.5754(0.4942) Grad: 254243.3906  LR: 0.00000266  \n","Epoch: [4][200/2032] Elapsed 0m 55s (remain 8m 29s) Loss: 0.4130(0.4964) Grad: 378305.1562  LR: 0.00000241  \n","Epoch: [4][300/2032] Elapsed 1m 23s (remain 7m 57s) Loss: 0.4923(0.4960) Grad: 429990.0625  LR: 0.00000216  \n","Epoch: [4][400/2032] Elapsed 1m 50s (remain 7m 28s) Loss: 0.5284(0.4964) Grad: 43074.4102  LR: 0.00000193  \n","Epoch: [4][500/2032] Elapsed 2m 17s (remain 6m 59s) Loss: 0.4056(0.4961) Grad: 20948.0996  LR: 0.00000171  \n","Epoch: [4][600/2032] Elapsed 2m 44s (remain 6m 31s) Loss: 0.4002(0.4961) Grad: 81039.5781  LR: 0.00000150  \n","Epoch: [4][700/2032] Elapsed 3m 11s (remain 6m 3s) Loss: 0.4234(0.4967) Grad: 138858.7500  LR: 0.00000130  \n","Epoch: [4][800/2032] Elapsed 3m 38s (remain 5m 36s) Loss: 0.5356(0.4968) Grad: 61632.1758  LR: 0.00000112  \n","Epoch: [4][900/2032] Elapsed 4m 6s (remain 5m 8s) Loss: 0.4566(0.4954) Grad: 126937.5078  LR: 0.00000094  \n","Epoch: [4][1000/2032] Elapsed 4m 33s (remain 4m 41s) Loss: 0.4355(0.4957) Grad: 202769.3125  LR: 0.00000079  \n","Epoch: [4][1100/2032] Elapsed 5m 0s (remain 4m 13s) Loss: 0.4645(0.4955) Grad: 110140.6406  LR: 0.00000064  \n","Epoch: [4][1200/2032] Elapsed 5m 27s (remain 3m 46s) Loss: 0.4696(0.4948) Grad: 108062.3594  LR: 0.00000051  \n","Epoch: [4][1300/2032] Elapsed 5m 54s (remain 3m 19s) Loss: 0.4662(0.4951) Grad: 68504.3203  LR: 0.00000040  \n","Epoch: [4][1400/2032] Elapsed 6m 22s (remain 2m 52s) Loss: 0.5019(0.4951) Grad: 96755.4922  LR: 0.00000030  \n","Epoch: [4][1500/2032] Elapsed 6m 49s (remain 2m 24s) Loss: 0.5798(0.4951) Grad: 98255.6094  LR: 0.00000021  \n","Epoch: [4][1600/2032] Elapsed 7m 16s (remain 1m 57s) Loss: 0.4307(0.4954) Grad: 37214.5469  LR: 0.00000014  \n","Epoch: [4][1700/2032] Elapsed 7m 43s (remain 1m 30s) Loss: 0.5583(0.4957) Grad: 85233.5312  LR: 0.00000008  \n","Epoch: [4][1800/2032] Elapsed 8m 10s (remain 1m 2s) Loss: 0.5350(0.4960) Grad: 639707.2500  LR: 0.00000004  \n","Epoch: [4][1900/2032] Elapsed 8m 38s (remain 0m 35s) Loss: 0.4490(0.4958) Grad: 83360.4297  LR: 0.00000001  \n","Epoch: [4][2000/2032] Elapsed 9m 5s (remain 0m 8s) Loss: 0.5395(0.4957) Grad: 212825.4375  LR: 0.00000000  \n","Epoch: [4][2031/2032] Elapsed 9m 13s (remain 0m 0s) Loss: 0.4816(0.4958) Grad: 303002.8750  LR: 0.00000000  \n","EVAL: [0/247] Elapsed 0m 0s (remain 3m 1s) Loss: 0.5016(0.5016) \n","EVAL: [100/247] Elapsed 0m 17s (remain 0m 24s) Loss: 0.6661(0.5643) \n","EVAL: [200/247] Elapsed 0m 33s (remain 0m 7s) Loss: 0.9789(0.5716) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4 - avg_train_loss: 0.4958  avg_val_loss: 0.5619  time: 595s\n","Epoch 4 - Score: 0.8370\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [246/247] Elapsed 0m 40s (remain 0m 0s) Loss: 0.3805(0.5619) \n"]},{"output_type":"stream","name":"stderr","text":["========== fold: 9 result ==========\n","Score: 0.8405\n","========== CV ==========\n","Score: 0.8287\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7488cbea96045c28d4d68d581aee6b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold0] avg_val_loss</td><td>▂▁▇█</td></tr><tr><td>[fold0] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold0] loss</td><td>█▅▅▆▄▄▇▃▄▅▆▃▃▅▃▄▃▆▁▂▄▅▅▅▄▃▅▂▅▃▂▄▃▅▄▄▄▃▅▃</td></tr><tr><td>[fold0] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold0] score</td><td>▁▄▇█</td></tr><tr><td>[fold1] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold1] avg_val_loss</td><td>▁▃▇█</td></tr><tr><td>[fold1] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold1] loss</td><td>▅▆▃█▅▆▄▅▄▆▃▄▅▄▂▅▅▃▄▂▃▄▃▇▂▅▅▂▄▃▃▄▁▃▃▅▃▃▁▄</td></tr><tr><td>[fold1] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold1] score</td><td>▁▃█▆</td></tr><tr><td>[fold2] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold2] avg_val_loss</td><td>▇▁▄█</td></tr><tr><td>[fold2] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold2] loss</td><td>▇▄▆▅▅█▅▆▄▅▅▅▃▅▅▅▆▂▁▅▂▂▄▂▃▂▂▄▃▄▅▇▄▅▂▅▄▃▃▃</td></tr><tr><td>[fold2] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold2] score</td><td>▁▇██</td></tr><tr><td>[fold3] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold3] avg_val_loss</td><td>▂▁▇█</td></tr><tr><td>[fold3] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold3] loss</td><td>▃▅▄▅▄▇▅▄▄▅▄▅▄█▄▅▃▃▄▂▆▃▃▅▆▅▅▃▄▄▅▄▁▄▂▄▅▂▄▃</td></tr><tr><td>[fold3] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold3] score</td><td>▁█▅▃</td></tr><tr><td>[fold4] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold4] avg_val_loss</td><td>▁▃▄█</td></tr><tr><td>[fold4] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold4] loss</td><td>█▅▃▅▃▃▆▃▅▄█▆▄▆▅▆▃▃▂▄▃▄▄▅▅▃▄▁▅▃▅▃▂▃▃▅▄▆▄▅</td></tr><tr><td>[fold4] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold4] score</td><td>▁▃█▆</td></tr><tr><td>[fold5] avg_train_loss</td><td>█▃▂▁</td></tr><tr><td>[fold5] avg_val_loss</td><td>▁▂██</td></tr><tr><td>[fold5] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold5] loss</td><td>▇▆▄▇▆▄▃▆▃▅▃▄▆▄▆▃▆▆▄▃█▇▁▇▄▅▄▅▄▄▃▇▃▂▅▅▆█▃▄</td></tr><tr><td>[fold5] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold5] score</td><td>▁█▃▄</td></tr><tr><td>[fold6] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold6] avg_val_loss</td><td>▂▁▅█</td></tr><tr><td>[fold6] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold6] loss</td><td>▆▅▅▂▄▇▄▃▆▄▅▅▅▃▄▃▇▄▄█▄▆▅▄▃▃▅▃▄▃▅▄▃▄▄▂▁▆▆▃</td></tr><tr><td>[fold6] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold6] score</td><td>▁█▅▃</td></tr><tr><td>[fold7] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold7] avg_val_loss</td><td>▁▂▅█</td></tr><tr><td>[fold7] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold7] loss</td><td>▇▇▆▆▇▆█▇█▇▃▄█▅▆▃▅▅▇▃▄▄▃▆▅▅▄▅▅▃▃▁▆▅▄▆▅▃▃▆</td></tr><tr><td>[fold7] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold7] score</td><td>▁▇█▇</td></tr><tr><td>[fold8] avg_train_loss</td><td>█▄▂▁</td></tr><tr><td>[fold8] avg_val_loss</td><td>▁▅▇█</td></tr><tr><td>[fold8] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold8] loss</td><td>▅▆█▆▃▅▅▃▆▅▆▁▇▄▁▄▅▆▆▅▄▅▄▄▄▃▂▄▅▃▅▃▃▄▄▃▅▄▄▆</td></tr><tr><td>[fold8] lr</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold8] score</td><td>▁▄██</td></tr><tr><td>[fold9] avg_train_loss</td><td>█▃▂▁</td></tr><tr><td>[fold9] avg_val_loss</td><td>█▁▃▆</td></tr><tr><td>[fold9] epoch</td><td>▁▃▆█</td></tr><tr><td>[fold9] loss</td><td>▆▃█▃▄▃▃▂▅▆▁▅▃▄▂▃▃▁▃▅▄▂▄▄▄▄▄▄▅▄▂▂▃▃▆▆▂▅▃▃</td></tr><tr><td>[fold9] lr</td><td>███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>[fold9] score</td><td>▁▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>[fold0] avg_train_loss</td><td>0.49398</td></tr><tr><td>[fold0] avg_val_loss</td><td>0.57681</td></tr><tr><td>[fold0] epoch</td><td>4</td></tr><tr><td>[fold0] loss</td><td>0.44792</td></tr><tr><td>[fold0] lr</td><td>0.0</td></tr><tr><td>[fold0] score</td><td>0.81401</td></tr><tr><td>[fold1] avg_train_loss</td><td>0.49474</td></tr><tr><td>[fold1] avg_val_loss</td><td>0.56581</td></tr><tr><td>[fold1] epoch</td><td>4</td></tr><tr><td>[fold1] loss</td><td>0.59054</td></tr><tr><td>[fold1] lr</td><td>0.0</td></tr><tr><td>[fold1] score</td><td>0.83766</td></tr><tr><td>[fold2] avg_train_loss</td><td>0.49463</td></tr><tr><td>[fold2] avg_val_loss</td><td>0.56118</td></tr><tr><td>[fold2] epoch</td><td>4</td></tr><tr><td>[fold2] loss</td><td>0.51852</td></tr><tr><td>[fold2] lr</td><td>0.0</td></tr><tr><td>[fold2] score</td><td>0.82776</td></tr><tr><td>[fold3] avg_train_loss</td><td>0.49558</td></tr><tr><td>[fold3] avg_val_loss</td><td>0.57822</td></tr><tr><td>[fold3] epoch</td><td>4</td></tr><tr><td>[fold3] loss</td><td>0.51974</td></tr><tr><td>[fold3] lr</td><td>0.0</td></tr><tr><td>[fold3] score</td><td>0.81867</td></tr><tr><td>[fold4] avg_train_loss</td><td>0.49516</td></tr><tr><td>[fold4] avg_val_loss</td><td>0.56736</td></tr><tr><td>[fold4] epoch</td><td>4</td></tr><tr><td>[fold4] loss</td><td>0.5542</td></tr><tr><td>[fold4] lr</td><td>0.0</td></tr><tr><td>[fold4] score</td><td>0.82916</td></tr><tr><td>[fold5] avg_train_loss</td><td>0.4957</td></tr><tr><td>[fold5] avg_val_loss</td><td>0.575</td></tr><tr><td>[fold5] epoch</td><td>4</td></tr><tr><td>[fold5] loss</td><td>0.62413</td></tr><tr><td>[fold5] lr</td><td>0.0</td></tr><tr><td>[fold5] score</td><td>0.81427</td></tr><tr><td>[fold6] avg_train_loss</td><td>0.49703</td></tr><tr><td>[fold6] avg_val_loss</td><td>0.56422</td></tr><tr><td>[fold6] epoch</td><td>4</td></tr><tr><td>[fold6] loss</td><td>0.58049</td></tr><tr><td>[fold6] lr</td><td>0.0</td></tr><tr><td>[fold6] score</td><td>0.8307</td></tr><tr><td>[fold7] avg_train_loss</td><td>0.49539</td></tr><tr><td>[fold7] avg_val_loss</td><td>0.55609</td></tr><tr><td>[fold7] epoch</td><td>4</td></tr><tr><td>[fold7] loss</td><td>0.49158</td></tr><tr><td>[fold7] lr</td><td>0.0</td></tr><tr><td>[fold7] score</td><td>0.83062</td></tr><tr><td>[fold8] avg_train_loss</td><td>0.49536</td></tr><tr><td>[fold8] avg_val_loss</td><td>0.56379</td></tr><tr><td>[fold8] epoch</td><td>4</td></tr><tr><td>[fold8] loss</td><td>0.59654</td></tr><tr><td>[fold8] lr</td><td>0.0</td></tr><tr><td>[fold8] score</td><td>0.82458</td></tr><tr><td>[fold9] avg_train_loss</td><td>0.49582</td></tr><tr><td>[fold9] avg_val_loss</td><td>0.56186</td></tr><tr><td>[fold9] epoch</td><td>4</td></tr><tr><td>[fold9] loss</td><td>0.48156</td></tr><tr><td>[fold9] lr</td><td>0.0</td></tr><tr><td>[fold9] score</td><td>0.837</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">exp003.002.001</strong>: <a href=\"https://wandb.ai/riow1983/us-patent-phrase-to-phrase-matching/runs/7ufuddux\" target=\"_blank\">https://wandb.ai/riow1983/us-patent-phrase-to-phrase-matching/runs/7ufuddux</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>../input/nb005t-deberta-v3-large/wandb/run-20220620_074556-7ufuddux/logs</code>"]},"metadata":{}}],"source":["if __name__ == '__main__':\n","    \n","    def get_result(oof_df):\n","        labels = oof_df['score'].values\n","        preds = oof_df['pred'].values\n","        score = get_score(labels, preds)\n","        LOGGER.info(f'Score: {score:<.4f}')\n","    \n","    if CFG.train:\n","        oof_df = pd.DataFrame()\n","        for fold in range(CFG.n_fold):\n","            if fold in CFG.trn_fold:\n","                _oof_df = train_loop(train, fold)\n","                oof_df = pd.concat([oof_df, _oof_df])\n","                LOGGER.info(f\"========== fold: {fold} result ==========\")\n","                get_result(_oof_df)\n","        oof_df = oof_df.reset_index(drop=True)\n","        LOGGER.info(f\"========== CV ==========\")\n","        get_result(oof_df)\n","        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n","        \n","    if CFG.wandb:\n","        wandb.finish()\n","\n","\n","\n","\n","    # Push to LINE\n","    import requests\n","\n","    def send_line_notification(message):\n","        import json\n","        f = open(\"../../line.json\", \"r\")\n","        json_data = json.load(f)\n","        line_token = json_data[\"kagglePush\"]\n","        endpoint = 'https://notify-api.line.me/api/notify'\n","        message = \"\\n{}\".format(message)\n","        payload = {'message': message}\n","        headers = {'Authorization': 'Bearer {}'.format(line_token)}\n","        requests.post(endpoint, data=payload, headers=headers)\n","\n","    if CFG.wandb:\n","        send_line_notification(f\"Training of {CFG.wandbproject+'/'+CFG.wandbgroup+'/'+CFG.wandbname} has been done. See {run.url}\")\n","    else:\n","        send_line_notification(f\"Training of {CFG.wandbproject+'/'+CFG.wandbgroup+'/'+CFG.wandbname} has been done.\")"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"nb005t-deberta-v3-large.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3ac66b6c07744a33bff4d56865805e89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c1639fe3dbb455aa5414d6b65a54e92","IPY_MODEL_866d48fa5f504e0f8ac59a3c168d5973","IPY_MODEL_be9cf9130db24e1fb7fa76740f0bccac"],"layout":"IPY_MODEL_bee0b391360d42909042aec83fe3fe92"}},"4c1639fe3dbb455aa5414d6b65a54e92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c41bc33e0e234f7dbdca4c6dca44bdb6","placeholder":"​","style":"IPY_MODEL_b03579da87b94698a417602ed4432c80","value":"100%"}},"866d48fa5f504e0f8ac59a3c168d5973":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_91954344fa794c38811d8bd3b8535f4a","max":136,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e94793aa60df41d7a4130c55eae0d5b4","value":136}},"be9cf9130db24e1fb7fa76740f0bccac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8653837c3b9e4435ab4332bf9d0f9093","placeholder":"​","style":"IPY_MODEL_739979a0b03f49bc9f93c59dceb5545d","value":" 136/136 [00:00&lt;00:00, 2537.84it/s]"}},"bee0b391360d42909042aec83fe3fe92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41bc33e0e234f7dbdca4c6dca44bdb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b03579da87b94698a417602ed4432c80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91954344fa794c38811d8bd3b8535f4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e94793aa60df41d7a4130c55eae0d5b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8653837c3b9e4435ab4332bf9d0f9093":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"739979a0b03f49bc9f93c59dceb5545d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"039f80a40aa44932a4c6e4b727ffa1a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f284cdbdd63243c4b4dac15f105a0130","IPY_MODEL_b119ed613c8b45a4bb3984cb22898794","IPY_MODEL_20dfec03c6e44a259baef51623a90bf4"],"layout":"IPY_MODEL_c3dc992e38a7400580f88c701695ffa7"}},"f284cdbdd63243c4b4dac15f105a0130":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a1a97345a73499388e45b1f6334b14f","placeholder":"​","style":"IPY_MODEL_fdddd16566c442b2b9f056517edaae18","value":"100%"}},"b119ed613c8b45a4bb3984cb22898794":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc000b714db643399b5d1e5332ea6a87","max":36473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ad792bc199c4be68cdd897b06cab243","value":36473}},"20dfec03c6e44a259baef51623a90bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e78b209fe3584824a8f841d0ca511e8a","placeholder":"​","style":"IPY_MODEL_7504df3a6f5542f9a251d12b71c55390","value":" 36473/36473 [00:03&lt;00:00, 11837.65it/s]"}},"c3dc992e38a7400580f88c701695ffa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a1a97345a73499388e45b1f6334b14f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdddd16566c442b2b9f056517edaae18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc000b714db643399b5d1e5332ea6a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad792bc199c4be68cdd897b06cab243":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e78b209fe3584824a8f841d0ca511e8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7504df3a6f5542f9a251d12b71c55390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47bc78679d234a7bbc4d5cfc0647938e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1b290e61709441c8990cc4abb91ac70","IPY_MODEL_c284caafc5484f9fbdfd2b362c307571","IPY_MODEL_c6cfae6a927541529bd0bfa911de2174"],"layout":"IPY_MODEL_0f1c6e2cbe9d41aaaf64f3ecd682e964"}},"e1b290e61709441c8990cc4abb91ac70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87d44827c36243d88b03704779b2df84","placeholder":"​","style":"IPY_MODEL_559b152dafb049ce88c566eedfa69f89","value":"100%"}},"c284caafc5484f9fbdfd2b362c307571":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d815330b1d840628ab6b13ce41dbfc5","max":36473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_457496984fe643b8986d6fb2277ff292","value":36473}},"c6cfae6a927541529bd0bfa911de2174":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81f89a025fd64a6398050b5b5f2cd2bb","placeholder":"​","style":"IPY_MODEL_75cb13840a764c06951251837acbcead","value":" 36473/36473 [00:03&lt;00:00, 11170.59it/s]"}},"0f1c6e2cbe9d41aaaf64f3ecd682e964":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87d44827c36243d88b03704779b2df84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"559b152dafb049ce88c566eedfa69f89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d815330b1d840628ab6b13ce41dbfc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"457496984fe643b8986d6fb2277ff292":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81f89a025fd64a6398050b5b5f2cd2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75cb13840a764c06951251837acbcead":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7488cbea96045c28d4d68d581aee6b5":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_94ce92fd1f6a4f16a9b64423ecc17d6e","IPY_MODEL_1f10820a76d04171b965c6d1c53d4d05"],"layout":"IPY_MODEL_c2ef0454e21146dab8befdd64a2caf82"}},"94ce92fd1f6a4f16a9b64423ecc17d6e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ca49ba0ae114c50897adab014d039e0","placeholder":"​","style":"IPY_MODEL_5eb5e8a75e3044d785fc27ee4c1399f1","value":"0.126 MB of 0.126 MB uploaded (0.000 MB deduped)\r"}},"1f10820a76d04171b965c6d1c53d4d05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_de96819e92bd4fbe83e83e5a753afd8b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7aa7dbbadb2a45a6b913651a22864353","value":1}},"c2ef0454e21146dab8befdd64a2caf82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca49ba0ae114c50897adab014d039e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eb5e8a75e3044d785fc27ee4c1399f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de96819e92bd4fbe83e83e5a753afd8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aa7dbbadb2a45a6b913651a22864353":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}